{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Arguments\n",
    "args_text = '--base-model sentence-transformers/all-MiniLM-L6-v2 '+\\\n",
    "            '--dataset imdb --n-word 5000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 1 '+ \\\n",
    "            '--n-cluster 50 ' + \\\n",
    "            '--stage-1-ckpt trained_model/imdb_model_all-MiniLM-L6-v2_stage1_50t_5000w_199e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    parser.add_argument('--epochs-1', default=100, type=int,\n",
    "                        help='Number of training epochs for Stage 1')\n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--dataset', default='news', type=str,\n",
    "                        choices=['news', 'twitter', 'wiki', 'nips', 'stackoverflow', 'reuters', 'r52', 'imdb', 'agnews', 'yahoo'],\n",
    "                        help='Name of the dataset')\n",
    "    parser.add_argument('--n-cluster', default=50, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "    \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    "    \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str, default='./',\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "def data_load(dataset_name):\n",
    "    should_measure_hungarian = False\n",
    "    if dataset_name == 'news':\n",
    "        textData = newsData()\n",
    "        should_measure_hungarian = True\n",
    "    elif dataset_name == 'imdb':\n",
    "        textData = IMDBData()\n",
    "    elif dataset_name == 'agnews':\n",
    "        textData = AGNewsData()\n",
    "    elif dataset_name == 'yahoo':\n",
    "        textData = YahooData()\n",
    "    elif dataset_name == 'twitter':\n",
    "        textData = twitterData('/home/data/topicmodel/twitter_covid19.tsv')\n",
    "    elif dataset_name == 'wiki':\n",
    "        textData = wikiData('/home/data/topicmodel/smplAbstracts/')\n",
    "    elif dataset_name == 'nips':\n",
    "        textData = nipsData('/home/data/topicmodel/papers.csv')\n",
    "    elif dataset_name == 'stackoverflow':\n",
    "        textData = stackoverflowData('/home/data/topicmodel/stack_overflow.csv')\n",
    "    elif dataset_name == 'reuters':\n",
    "        textData = reutersData('/home/data/topicmodel/reuters-21578.txt')\n",
    "    elif dataset_name == 'r52':\n",
    "        textData = r52Data('/home/data/topicmodel/r52/')\n",
    "        should_measure_hungarian = True\n",
    "    return textData, should_measure_hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:27<00:00, 911.06it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=textData.data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "basesim_path = f\"./save/{args.dataset}_{bert_name_short}_basesim_matrix_full.pkl\"\n",
    "if os.path.isfile(basesim_path) == False:\n",
    "    model = SentenceTransformer(bert_name.split('/')[-1], device='cuda')\n",
    "    base_result_list = []\n",
    "    for text in tqdm_notebook(trainds.nonempty_text):\n",
    "        base_result_list.append(model.encode(text))\n",
    "        \n",
    "    base_result_embedding = np.stack(base_result_list)\n",
    "    basereduced_norm = F.normalize(torch.tensor(base_result_embedding), dim=-1)\n",
    "    basesim_matrix = torch.mm(basereduced_norm, basereduced_norm.t())\n",
    "    ind = np.diag_indices(basesim_matrix.shape[0])\n",
    "    basesim_matrix[ind[0], ind[1]] = torch.ones(basesim_matrix.shape[0]) * -1\n",
    "    torch.save(basesim_matrix, basesim_path)\n",
    "else:\n",
    "    basesim_matrix = torch.load(basesim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Discovery neighborhood pairs and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hidden, alpha=1.0):\n",
    "    device = hidden.device\n",
    "    hidden_dim = hidden.shape[-1]\n",
    "    rand_w = torch.Tensor(np.eye(hidden_dim, dtype=np.float64)).to(device)\n",
    "    loss_dist_match = get_swd_loss(hidden, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t) ** 2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=384)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "    model.cuda(gpu_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = args.bsz =16#= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    losses = AverageMeter()\n",
    "    closses = AverageMeter() \n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "    finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "    optimizer = RangerLars(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    global_step = 0\n",
    "    memory_queue = F.softmax(torch.randn(512, n_cluster).cuda(gpu_ids[0]), dim=1)\n",
    "    for epoch in range(epochs_1):\n",
    "        model.train()\n",
    "        #ema_model.train()\n",
    "        tbar = tqdm_notebook(trainloader)\n",
    "        for batch_idx, batch in enumerate(tbar):       \n",
    "            org_input, pos_input, _, _ = batch\n",
    "            org_input_ids = org_input['input_ids'].cuda(gpu_ids[0])\n",
    "            org_attention_mask = org_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            pos_input_ids = pos_input['input_ids'].cuda(gpu_ids[0])\n",
    "            pos_attention_mask = pos_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            all_input_ids = torch.cat((org_input_ids, pos_input_ids), dim=0)\n",
    "            all_attention_masks = torch.cat((org_attention_mask, pos_attention_mask), dim=0)\n",
    "            all_topics, _ = model(all_input_ids, all_attention_masks, return_topic=True)\n",
    "\n",
    "            orig_topic, pos_topic = torch.split(all_topics, len(all_topics) // 2)\n",
    "            pos_sim = torch.sum(orig_topic * pos_topic, dim=-1)\n",
    "\n",
    "            # consistency loss\n",
    "            consistency_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution matching loss\n",
    "            memory_queue = torch.cat((memory_queue.detach(), all_topics), dim=0)[all_topics.size(0):]\n",
    "            distmatch_loss = dist_match_loss(memory_queue, dirichlet_alpha_1)\n",
    "            loss = args.coeff_1_sim * consistency_loss + \\\n",
    "                   args.coeff_1_dist * distmatch_loss\n",
    "\n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(consistency_loss.item(), bsz)\n",
    "            dlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            tbar.set_description(\"Epoch-{} / consistency: {:.5f} - dist: {:.5f}\".format(epoch, \n",
    "                                                                                        closses.avg, \n",
    "                                                                                        dlosses.avg), refresh=True)\n",
    "            tbar.refresh()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            global_step += 1\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model_stage1_name = f'./trained_model/{args.dataset}_model_{bert_name_short}_stage1_{args.n_topic}t_{args.n_word}w_{epoch}e.ckpt'\n",
    "    torch.save(model.state_dict(), model_stage1_name)\n",
    "else:\n",
    "    model_stage1_name = args.stage_1_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: extract vocab set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del ema_model\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in torch.load(model_stage1_name).items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state_dict[k[7:]] = v  # Remove 'module.' prefix\n",
    "    else:\n",
    "        new_state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=384)\n",
    "model.cuda(gpu_ids[0])\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003000497817993164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 782,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c928b5c472402aa772c829aa42597e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "result_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm_notebook(memoryloader)):        \n",
    "        org_input, _, _, _ = batch\n",
    "        org_input_ids = org_input['input_ids'].to(gpu_ids[0])\n",
    "        org_attention_mask = org_input['attention_mask'].to(gpu_ids[0])\n",
    "        topic, embed = model(org_input_ids, org_attention_mask, return_topic = True)\n",
    "        result_list.append(topic)\n",
    "result_embedding = torch.cat(result_list)\n",
    "_, result_topic = torch.max(result_embedding, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': trainds.preprocess_ctm(trainds.nonempty_text), \n",
    "     'cluster_label': result_topic.cpu().numpy()}\n",
    "cluster_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_class = cluster_df.groupby(['cluster_label'], as_index=False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70735\n"
     ]
    }
   ],
   "source": [
    "supertxt = \"\"\n",
    "for idx, row in docs_per_class.iterrows():\n",
    "    supertxt = supertxt + row['text'] + \" \"\n",
    "wwwwwwwwwwwwww = supertxt.split()\n",
    "ssssssssssssss = set(wwwwwwwwwwwwww)\n",
    "print(len(ssssssssssssss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "# count_vectorizer = CountVectorizer()\n",
    "ctfidf_vectorizer = CTFIDFVectorizer()\n",
    "count = count_vectorizer.fit_transform(docs_per_class.text)\n",
    "ctfidf = ctfidf_vectorizer.fit_transform(count, n_samples=len(cluster_df)).toarray()\n",
    "words = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68211"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transport to gensim\n",
    "(gensim_corpus, gensim_dict) = vect2gensim(count_vectorizer, count)\n",
    "vocab_list = set(gensim_dict.token2id.keys())\n",
    "stopwords = set(line.strip() for line in open('stopwords_en.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "resolution_score = (ctfidf - np.min(ctfidf, axis=1, keepdims=True)) / (np.max(ctfidf, axis=1, keepdims=True) - np.min(ctfidf, axis=1, keepdims=True))\n",
    "\n",
    "n_word = args.n_word\n",
    "# n_topic_word = n_word / len(docs_per_class.cluster_label.index)\n",
    "n_topic_word = n_word\n",
    "n_topic_word = 15\n",
    "\n",
    "topic_word_dict = {}\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['show', 'book', 'actor', 'dont', 'love', 'work', 'director', 'watch', 'funny', 'made', 'story', 'life', 'character', 'acting', 'people'],\n",
      "1: ['game', 'match', 'sport', 'baseball', 'team', 'wwe', 'player', 'championship', 'sox', 'win', 'fan', 'cena', 'wrestling', 'league', 'football'],\n",
      "2: ['russian', 'chinese', 'china', 'soviet', 'political', 'russia', 'swedish', 'documentary', 'american', 'muslim', 'people', 'country', 'communist', 'actor', 'story'],\n",
      "3: ['vampire', 'werewolf', 'monster', 'zombie', 'creature', 'horror', 'hunter', 'shark', 'effect', 'tarzan', 'bad', 'bug', 'cat', 'blood', 'gore'],\n",
      "4: ['gun', 'violence', 'action', 'kill', 'sniper', 'killer', 'terrorist', 'guy', 'gunga', 'watch', 'people', 'din', 'triad', 'lead', 'plot'],\n",
      "5: ['video', 'acting', 'bad', 'performance', 'great', 'disappointed', 'found', 'cast', 'documentary', 'dialogue', 'soo', 'worst', 'beautiful', 'complete', 'depardieu'],\n",
      "6: ['laugh', 'bad', 'funny', 'worst', 'comedy', 'waste', 'terrible', 'wasnt', 'watch', 'humor', 'acting', 'dont', 'joke', 'laughed', 'dvd'],\n",
      "7: ['gay', 'sex', 'sexual', 'lesbian', 'aid', 'porn', 'scene', 'acting', 'bad', 'sexy', 'woman', 'nudity', 'boy', 'life', 'real'],\n",
      "8: ['killer', 'slasher', 'serial', 'death', 'funeral', 'kill', 'killing', 'waste', 'flick', 'cut', 'thriller', 'murder', 'minute', 'bad', 'plot'],\n",
      "9: ['action', 'bad', 'guy', 'scene', 'people', 'thing', 'man', 'made', 'dont', 'end', 'plot', 'point', 'make', 'back', 'war'],\n",
      "10: ['christmas', 'santa', 'holiday', 'scrooge', 'claus', 'book', 'grinch', 'version', 'family', 'scott', 'drew', 'read', 'kid', 'carol', 'affleck'],\n",
      "11: ['soldier', 'war', 'military', 'luke', 'macarthur', 'army', 'jedi', 'vader', 'battle', 'star', 'navy', 'rebel', 'western', 'action', 'training'],\n",
      "12: ['romantic', 'comedy', 'romance', 'love', 'funny', 'meg', 'robbins', 'woman', 'sweet', 'ryan', 'einstein', 'watch', 'beautiful', 'great', 'tim'],\n",
      "13: ['horror', 'scary', 'scarecrow', 'gore', 'nightmare', 'thriller', 'watch', 'halloween', 'worst', 'effect', 'creepy', 'watching', 'bad', 'fan', 'genre'],\n",
      "14: ['war', 'soldier', 'vietnam', 'american', 'macarthur', 'world', 'wwii', 'documentary', 'western', 'country', 'propaganda', 'political', 'iraq', 'series', 'men'],\n",
      "15: ['eddie', 'car', 'cowboy', 'murphy', 'mexican', 'funny', 'delirious', 'horse', 'midnight', 'bad', 'ratso', 'bud', 'hoffman', 'laugh', 'mexico'],\n",
      "16: ['christian', 'jesus', 'church', 'christ', 'god', 'religion', 'religious', 'mormon', 'joseph', 'bible', 'faith', 'christianity', 'smith', 'lds', 'catholic'],\n",
      "17: ['comedy', 'funny', 'laugh', 'woman', 'comedic', 'enjoy', 'joke', 'humour', 'humor', 'acting', 'worst', 'watch', 'guy', 'female', 'snl'],\n",
      "18: ['dinosaur', 'alien', 'cat', 'animal', 'pokemon', 'dragon', 'bug', 'cartoon', 'earth', 'rex', 'moon', 'whale', 'space', 'human', 'planet'],\n",
      "19: ['woman', 'love', 'life', 'sex', 'wife', 'girl', 'family', 'father', 'man', 'young', 'mother', 'role', 'performance', 'husband', 'comedy'],\n",
      "20: ['episode', 'season', 'show', 'series', 'abc', 'sabrina', 'watch', 'back', 'twilight', 'watching', 'lost', 'great', 'zone', 'hope', 'funny'],\n",
      "21: ['wife', 'husband', 'wedding', 'hardy', 'married', 'plump', 'runt', 'marriage', 'divorce', 'performance', 'comedy', 'couple', 'funny', 'woman', 'love'],\n",
      "22: ['book', 'read', 'jane', 'version', 'adaptation', 'nancy', 'eyre', 'reading', 'greek', 'based', 'emma', 'drew', 'disappointed', 'austen', 'true'],\n",
      "23: ['murder', 'killer', 'crime', 'police', 'prison', 'criminal', 'noir', 'cop', 'detective', 'man', 'welles', 'role', 'performance', 'play', 'gangster'],\n",
      "24: ['family', 'show', 'series', 'brother', 'sister', 'episode', 'season', 'loved', 'full', 'house', 'father', 'great', 'life', 'twin', 'dad'],\n",
      "25: ['trek', 'episode', 'spock', 'star', 'titanic', 'ship', 'biko', 'enterprise', 'kirk', 'series', 'south', 'africa', 'voyager', 'captain', 'seagal'],\n",
      "26: ['zombie', 'fido', 'zombi', 'dead', 'gore', 'fulci', 'timmy', 'flick', 'sci', 'revolt', 'flesh', 'undead', 'living', 'louque', 'alien'],\n",
      "27: ['hitler', 'japanese', 'german', 'nazi', 'japan', 'germany', 'war', 'jew', 'american', 'world', 'history', 'documentary', 'carlyle', 'jewish', 'evil'],\n",
      "28: ['bollywood', 'indian', 'music', 'song', 'india', 'dance', 'sinatra', 'dancing', 'hindi', 'musical', 'prince', 'soundtrack', 'great', 'gandhi', 'watch'],\n",
      "29: ['disney', 'animation', 'cinderella', 'animated', 'cartoon', 'lion', 'king', 'timon', 'original', 'kid', 'sequel', 'pumbaa', 'song', 'atlantis', 'voice'],\n",
      "30: ['victoria', 'mother', 'daughter', 'emma', 'queen', 'mary', 'albert', 'young', 'love', 'princess', 'actress', 'austen', 'prince', 'performance', 'family'],\n",
      "31: ['baby', 'mother', 'father', 'child', 'lily', 'streep', 'stanwyck', 'son', 'face', 'worth', 'pregnant', 'mama', 'lindy', 'give', 'adult'],\n",
      "32: ['shakespeare', 'writer', 'actor', 'script', 'director', 'writing', 'acting', 'hamlet', 'cast', 'bad', 'branagh', 'work', 'directing', 'production', 'great'],\n",
      "33: ['french', 'italian', 'show', 'france', 'spanish', 'paris', 'cinema', 'puerto', 'actor', 'great', 'spain', 'portuguese', 'love', 'story', 'life'],\n",
      "34: ['bad', 'watch', 'worst', 'beatles', 'waste', 'soap', 'dont', 'acting', 'funny', 'didnt', 'plot', 'recommend', 'wasnt', 'boring', 'ending'],\n",
      "35: ['musical', 'song', 'dance', 'number', 'music', 'singing', 'kelly', 'dancing', 'sinatra', 'broadway', 'stage', 'astaire', 'dancer', 'sing', 'great'],\n",
      "36: ['horror', 'scary', 'gore', 'ghost', 'house', 'creepy', 'slasher', 'blood', 'killer', 'night', 'scare', 'monster', 'effect', 'fan', 'director'],\n",
      "37: ['student', 'school', 'teacher', 'college', 'high', 'bad', 'waste', 'acting', 'comment', 'grade', 'teen', 'worst', 'money', 'script', 'watched'],\n",
      "38: ['scary', 'ghost', 'horror', 'scared', 'halloween', 'scare', 'freddy', 'nightmare', 'creepy', 'haunted', 'special', 'scariest', 'freddys', 'frightening', 'effect'],\n",
      "39: ['gangster', 'lynch', 'cop', 'noir', 'drug', 'david', 'role', 'fan', 'action', 'performance', 'criminal', 'great', 'actor', 'police', 'work'],\n",
      "40: ['kid', 'child', 'adult', 'age', 'show', 'childrens', 'funny', 'animation', 'disney', 'parent', 'fun', 'worm', 'watch', 'miike', 'year'],\n",
      "41: ['child', 'kid', 'adult', 'parent', 'family', 'fox', 'bear', 'watched', 'year', 'watching', 'watch', 'heart', 'childrens', 'back', 'boy'],\n",
      "42: ['music', 'bad', 'rock', 'soundtrack', 'great', 'acting', 'actor', 'watch', 'dont', 'worst', 'video', 'good', 'ramones', 'waste', 'tap'],\n",
      "43: ['song', 'music', 'soundtrack', 'great', 'funny', 'watch', 'girl', 'bad', 'acting', 'pretty', 'love', 'dont', 'worst', 'favorite', 'fun'],\n",
      "44: ['porn', 'sex', 'rape', 'erotic', 'bad', 'porno', 'nudity', 'watching', 'naked', 'funny', 'waste', 'watch', 'acting', 'girl', 'scene'],\n",
      "45: ['band', 'rock', 'music', 'metal', 'ramones', 'song', 'dandy', 'anton', 'warhol', 'roll', 'musician', 'concert', 'newcombe', 'fan', 'singer'],\n",
      "46: ['murder', 'columbo', 'killer', 'detective', 'crime', 'mystery', 'series', 'powell', 'murderer', 'excellent', 'episode', 'edmund', 'capote', 'serial', 'philo'],\n",
      "47: ['son', 'father', 'family', 'gandhi', 'child', 'daughter', 'dad', 'hiralal', 'mother', 'wonderful', 'young', 'reiser', 'parent', 'year', 'performance'],\n",
      "48: ['batman', 'superman', 'black', 'white', 'african', 'joker', 'series', 'animated', 'lois', 'comic', 'batwoman', 'race', 'racist', 'racism', 'clark'],\n",
      "49: ['martial', 'kung', 'fight', 'anime', 'art', 'ninja', 'gundam', 'fighting', 'action', 'jet', 'jackie', 'japanese', 'chan', 'sammo', 'series'],\n"
     ]
    }
   ],
   "source": [
    "for key in topic_word_dict:\n",
    "    print(f\"{key}: {topic_word_dict[key]},\")\n",
    "topic_words_list = list(topic_word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40596\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "\n",
    "class WhiteSpacePreprocessing():\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
    "                      \"Use WhiteSpacePreprocessingStopwords.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "    \n",
    "def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):  \n",
    "    num_k = class_num\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "  \n",
    "    for c1 in range(0, num_k):\n",
    "        for c2 in range(0, num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "  \n",
    "    match = linear_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "  \n",
    "    return res\n",
    "\n",
    "def get_document_topic(topic_words, preprocessed_documents_lemmatized):\n",
    "    topic_words_flatten = list(itertools.chain.from_iterable(topic_words))\n",
    "    if '' in topic_words_flatten:\n",
    "        topic_words_flatten.remove('')\n",
    "    topic_words_flatten = list(set(topic_words_flatten))\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary = topic_words_flatten)\n",
    "    vectorizer = vectorizer.fit(preprocessed_documents_lemmatized)\n",
    "    count_mat = vectorizer.transform(preprocessed_documents_lemmatized).toarray()\n",
    "    \n",
    "    count_mat_normalized = count_mat + 1e-4\n",
    "    count_mat_normalized = count_mat_normalized / count_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat = vectorizer.transform([' '.join(i) for i in topic_words]).toarray()\n",
    "    topic_mat_normalized = topic_mat + 1e-4\n",
    "    topic_mat_normalized = topic_mat_normalized / topic_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat_inverse = topic_mat_normalized @ topic_mat_normalized.transpose()\n",
    "    topic_mat_inverse = np.linalg.inv(topic_mat_inverse)\n",
    "    topic_mat_inverse = topic_mat_normalized.transpose() @ topic_mat_inverse\n",
    "    document_topic = count_mat_normalized @ topic_mat_inverse\n",
    "    return document_topic\n",
    "\n",
    "class TopicModelDataPreparationNoNumber(TopicModelDataPreparation):\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, wordlist=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b', vocabulary=wordlist)\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(train_contextualized_embeddings, train_bow_embeddings, self.id2token, encoded_labels)\n",
    "    \n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "qt = TopicModelDataPreparationNoNumber(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sp = WhiteSpacePreprocessing(textData.data, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_documents_lemmatized = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()]) for doc in preprocessed_documents]\n",
    "\n",
    "document_topic = get_document_topic(topic_words_list, preprocessed_documents_lemmatized)\n",
    "train_target_filtered = textData.targets.squeeze()[retained_indices]\n",
    "flat_predict = torch.tensor(np.argmax(document_topic, axis=1))\n",
    "flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "num_samples = flat_predict.shape[0]\n",
    "match = _hungarian_match(flat_predict, flat_target, num_samples, 20)    \n",
    "reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "for pred_i, target_i in match:\n",
    "    reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009404526169858558"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(reordered_preds, flat_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "[-4.36574, -2.90311, -2.18412, -2.78158, -3.6183, -5.1184, -7.69746, -4.11328, -2.64054, -2.38845, -4.31582, -3.03664, -2.71763, -3.93272, -2.58013, -5.02507, -2.29871, -6.67594, -2.82068, -1.72432, -3.32589, -2.23587, -4.49461, -2.39847, -1.87177, -4.52966, -8.94201, -2.554, -4.47081, -4.69599, -2.3376, -3.39419, -3.28968, -2.20681, -11.41623, -4.73776, -2.31907, -5.09992, -6.86984, -1.7435, -3.85016, -1.82977, -5.87089, -6.26561, -4.31719, -3.52146, -3.96526, -3.64719, -5.67721, -3.40347]\n",
      "-3.9644106000000012\n",
      "[-0.06631, 0.08325, 0.04481, 0.03595, -0.05396, -0.09748, -0.04491, 0.04557, -0.01932, -0.05291, -0.03615, 0.02592, -0.00608, -0.05, 0.01554, -0.07139, 0.1879, -0.00488, 0.02186, 0.07947, -0.00511, -0.0035, -0.06041, 0.06361, 0.03511, -0.01945, -0.1527, 0.03617, -0.00075, 0.01858, 0.06669, -0.05027, -0.0001, 0.03259, -0.14481, 0.03172, 0.04443, -0.04385, 0.00595, 0.01206, -0.02995, -0.00197, -0.08207, -0.02717, 0.00538, 0.02355, 0.01196, -0.02517, -0.03089, 0.01687]\n",
      "-0.0047324\n",
      "[-0.0857, 0.50727, 0.22677, 0.45118, 0.05717, -0.29323, -0.22077, 0.26109, 0.22995, 0.03687, 0.12584, 0.25196, 0.31337, 0.09894, 0.19938, 0.06285, 0.69809, 0.13819, 0.33296, 0.35675, 0.08166, 0.03281, 0.0144, 0.43142, 0.11793, 0.10535, -0.21332, 0.22915, 0.08548, 0.27089, 0.37771, 0.09757, 0.06893, 0.178, -0.62753, 0.20228, 0.42501, -0.13317, -0.00832, 0.20376, 0.10504, 0.04801, -0.14055, 0.00363, 0.14156, 0.32911, 0.2563, 0.11283, 0.03481, 0.37176]\n",
      "0.13902879999999998\n",
      "[-2.35923, -0.24202, 0.42764, -0.34178, -2.1501, -2.76808, -1.90451, -0.10086, -1.47109, -1.86846, -1.8213, -0.51313, -1.05745, -2.15959, -0.25982, -2.3909, 2.09103, -1.21335, -0.58275, 0.75796, -0.91437, -1.21749, -2.30676, 0.33189, 0.32984, -1.87748, -4.76228, -0.25236, -0.93353, -0.70947, 0.33259, -2.28761, -0.77411, 0.28502, -4.12601, -0.52198, 0.16885, -1.75009, -0.91302, -0.13225, -1.66126, -0.58083, -2.76774, -1.74875, -0.85475, -0.9799, -1.10421, -1.78858, -2.08315, -0.82555]\n",
      "-1.1270626\n",
      "[0.4347, 0.53276, 0.32907, 0.37028, 0.40191, 0.42342, 0.48169, 0.41676, 0.38956, 0.34768, 0.41803, 0.37919, 0.31656, 0.42884, 0.29665, 0.3972, 0.52934, 0.47291, 0.38936, 0.34428, 0.35568, 0.40642, 0.37456, 0.39305, 0.3005, 0.48183, 0.54404, 0.37206, 0.44162, 0.42692, 0.33599, 0.40101, 0.38458, 0.32706, 0.54746, 0.47655, 0.3507, 0.42831, 0.45483, 0.2988, 0.39367, 0.28979, 0.44751, 0.47693, 0.39701, 0.45673, 0.43866, 0.42623, 0.49562, 0.39983]\n",
      "0.40848280000000003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic_N': 50,\n",
       " 'umass_wiki': -3.9644106000000012,\n",
       " 'npmi_wiki': -0.0047324,\n",
       " 'uci_wiki': -1.1270626,\n",
       " 'CV_wiki': 0.40848280000000003,\n",
       " 'cp_wiki': 0.13902879999999998,\n",
       " 'sim_w2v': 0.19386990993152348,\n",
       " 'diversity': 0.6173333333333333,\n",
       " 'filename': 'results/240518_144816.txt'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "results = get_topic_qualities(topic_words_list, args.palmetto_dir, reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                              filename=f'results/{now}.txt')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "\n",
    "n_word = args.n_word\n",
    "n_topic_word = n_word\n",
    "\n",
    "words_to_idx = {k: v for v, k in enumerate(words)}\n",
    "topic_word_dict = {}\n",
    "topic_score_dict = {}\n",
    "total_score_cat = []\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    total_score_cat.append(total_score)\n",
    "    # topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    # topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]\n",
    "    topic_score_dict[docs_per_class.cluster_label.iloc[label]] = [total_score[words_to_idx[top_word]] for top_word in topic_word_list[:int(n_topic_word)]]\n",
    "total_score_cat = np.stack(total_score_cat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "topic_word_set = list(itertools.chain.from_iterable(pd.DataFrame.from_dict(topic_word_dict).values))\n",
    "word_candidates = remove_dup(topic_word_set)[:n_word]\n",
    "n_word = len(word_candidates)\n",
    "n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('our_word_candidates_10000.pkl', 'wb') as f:\n",
    "    pickle.dump(word_candidates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_candidates = {}\n",
    "for candidate in word_candidates:\n",
    "    weight_candidates[candidate] = [total_score_cat[label, words_to_idx[candidate]] for label in range(n_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cand_to_idx = {k: v for v, k in enumerate(list(weight_candidates.keys()))}\n",
    "weight_cand_matrix = np.array(list(weight_candidates.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, basesim_matrix, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        sim_weight, sim_indices = basesim_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2TestDataset(Dataset):\n",
    "    def __init__(self, encoder, ds, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embedding_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:11<00:00, 2155.99it/s]\n",
      "100%|██████████| 25000/25000 [02:04<00:00, 200.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, basesim_matrix, word_candidates, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_cands = torch.tensor(weight_cand_matrix.max(axis=1)).cuda(gpu_ids[0]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:27<00:00, 924.75it/s] \n",
      "100%|██████████| 25000/25000 [00:11<00:00, 2242.43it/s]\n",
      "100%|██████████| 25000/25000 [02:06<00:00, 198.39it/s]\n"
     ]
    }
   ],
   "source": [
    "testds = BertDataset(bert=bert_name, text_list=textData.test_data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds2 = Stage2TestDataset(model.encoder, testds, word_candidates, lemmatize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.94840 - dist: 0.22322 - cons: -0.03311\n",
      "Epoch-1 / recon: 7.91175 - dist: 0.21235 - cons: -0.03618\n",
      "Epoch-2 / recon: 7.89567 - dist: 0.20702 - cons: -0.03745\n",
      "Epoch-3 / recon: 7.88558 - dist: 0.20239 - cons: -0.03825\n",
      "Epoch-4 / recon: 7.87823 - dist: 0.19907 - cons: -0.03885\n",
      "Epoch-5 / recon: 7.87241 - dist: 0.19661 - cons: -0.03936\n",
      "Epoch-6 / recon: 7.86760 - dist: 0.19450 - cons: -0.03982\n",
      "Epoch-7 / recon: 7.86358 - dist: 0.19273 - cons: -0.04028\n",
      "Epoch-8 / recon: 7.85996 - dist: 0.19114 - cons: -0.04059\n",
      "Epoch-9 / recon: 7.85680 - dist: 0.18969 - cons: -0.04083\n",
      "Epoch-10 / recon: 7.85396 - dist: 0.18840 - cons: -0.04107\n",
      "Epoch-11 / recon: 7.85141 - dist: 0.18725 - cons: -0.04127\n",
      "Epoch-12 / recon: 7.84906 - dist: 0.18620 - cons: -0.04146\n",
      "Epoch-13 / recon: 7.84695 - dist: 0.18518 - cons: -0.04163\n",
      "Epoch-14 / recon: 7.84494 - dist: 0.18442 - cons: -0.04182\n",
      "Epoch-15 / recon: 7.84312 - dist: 0.18369 - cons: -0.04196\n",
      "Epoch-16 / recon: 7.84142 - dist: 0.18303 - cons: -0.04208\n",
      "Epoch-17 / recon: 7.83984 - dist: 0.18240 - cons: -0.04221\n",
      "Epoch-18 / recon: 7.83834 - dist: 0.18185 - cons: -0.04233\n",
      "Epoch-19 / recon: 7.83696 - dist: 0.18127 - cons: -0.04241\n",
      "Epoch-20 / recon: 7.83568 - dist: 0.18081 - cons: -0.04250\n",
      "Epoch-21 / recon: 7.83443 - dist: 0.18039 - cons: -0.04257\n",
      "Epoch-22 / recon: 7.83324 - dist: 0.18000 - cons: -0.04265\n",
      "Epoch-23 / recon: 7.83214 - dist: 0.17959 - cons: -0.04272\n",
      "Epoch-24 / recon: 7.83106 - dist: 0.17921 - cons: -0.04278\n",
      "Epoch-25 / recon: 7.83001 - dist: 0.17887 - cons: -0.04282\n",
      "Epoch-26 / recon: 7.82899 - dist: 0.17853 - cons: -0.04287\n",
      "Epoch-27 / recon: 7.82802 - dist: 0.17827 - cons: -0.04292\n",
      "Epoch-28 / recon: 7.82708 - dist: 0.17799 - cons: -0.04297\n",
      "Epoch-29 / recon: 7.82617 - dist: 0.17778 - cons: -0.04303\n",
      "Epoch-30 / recon: 7.82527 - dist: 0.17754 - cons: -0.04308\n",
      "Epoch-31 / recon: 7.82440 - dist: 0.17730 - cons: -0.04312\n",
      "Epoch-32 / recon: 7.82355 - dist: 0.17707 - cons: -0.04318\n",
      "Epoch-33 / recon: 7.82274 - dist: 0.17686 - cons: -0.04324\n",
      "Epoch-34 / recon: 7.82197 - dist: 0.17668 - cons: -0.04329\n",
      "Epoch-35 / recon: 7.82120 - dist: 0.17651 - cons: -0.04333\n",
      "Epoch-36 / recon: 7.82043 - dist: 0.17632 - cons: -0.04337\n",
      "Epoch-37 / recon: 7.81971 - dist: 0.17613 - cons: -0.04340\n",
      "Epoch-38 / recon: 7.81901 - dist: 0.17595 - cons: -0.04345\n",
      "Epoch-39 / recon: 7.81831 - dist: 0.17579 - cons: -0.04350\n",
      "Epoch-40 / recon: 7.81764 - dist: 0.17565 - cons: -0.04353\n",
      "Epoch-41 / recon: 7.81699 - dist: 0.17551 - cons: -0.04357\n",
      "Epoch-42 / recon: 7.81636 - dist: 0.17537 - cons: -0.04361\n",
      "Epoch-43 / recon: 7.81574 - dist: 0.17524 - cons: -0.04364\n",
      "Epoch-44 / recon: 7.81512 - dist: 0.17514 - cons: -0.04369\n",
      "Epoch-45 / recon: 7.81453 - dist: 0.17503 - cons: -0.04372\n",
      "Epoch-46 / recon: 7.81394 - dist: 0.17490 - cons: -0.04375\n",
      "Epoch-47 / recon: 7.81336 - dist: 0.17476 - cons: -0.04379\n",
      "Epoch-48 / recon: 7.81281 - dist: 0.17466 - cons: -0.04381\n",
      "Epoch-49 / recon: 7.81227 - dist: 0.17453 - cons: -0.04384\n",
      "------- Evaluation results -------\n",
      "topic-0 ['harilal', 'book', 'description', 'softly', 'changed', 'choreography', 'gujarati', 'dance', 'annie', 'gershwin', 'dislike', 'combined', 'gandhi', 'woody', 'liebmann']\n",
      "topic-1 ['kid', 'absolutley', 'child', 'family', 'great', 'would', 'time', 'good', 'show', 'watched', 'year', 'like', 'smallville', 'parent', 'see']\n",
      "topic-2 ['song', 'band', 'music', 'musical', 'gandhi', 'time', 'streisand', 'kris', 'mahatma', 'berkeley', 'rock', 'gujarati', 'singer', 'singing', 'dance']\n",
      "topic-3 ['brock', 'student', 'pokemon', 'wuhl', 'like', 'animal', 'school', 'time', 'cartoon', 'mayo', 'teacher', 'mina', 'love', 'show', 'anime']\n",
      "topic-4 ['zombie', 'horror', 'sniper', 'vampire', 'like', 'good', 'also', 'well', 'story', 'kung', 'batman', 'martial', 'even', 'time', 'character']\n",
      "topic-5 ['homeward', 'dreamworks', 'timon', 'book', 'novel', 'softly', 'pumbaa', 'disney', 'eyre', 'rochester', 'adaptation', 'austen', 'dalmations', 'jerry', 'simbas']\n",
      "topic-6 ['ajay', 'zealous', 'fuqua', 'miscast', 'brock', 'refund', 'townspeople', 'avoid', 'corey', 'fight', 'buseys', 'money', 'dreadful', 'ab', 'lam']\n",
      "topic-7 ['zombie', 'horror', 'sniper', 'vampire', 'soldier', 'like', 'character', 'alien', 'man', 'work', 'make', 'ninja', 'werewolf', 'get', 'scene']\n",
      "topic-8 ['good', 'romantic', 'character', 'comedy', 'shikhar', 'great', 'performance', 'see', 'funny', 'love', 'well', 'white', 'time', 'black', 'acting']\n",
      "topic-9 ['season', 'murder', 'disney', 'episode', 'murderer', 'cop', 'crime', 'baloo', 'animation', 'show', 'drift', 'detective', 'suspect', 'receipt', 'lupino']\n",
      "topic-10 ['depraved', 'capote', 'franklin', 'winfield', 'brook', 'animation', 'forsythe', 'scooby', 'clutter', 'merrill', 'surely', 'disney', 'mvp', 'tatsuhito', 'philo']\n",
      "topic-11 ['mvp', 'vampire', 'werewolf', 'unavailable', 'zombie', 'lesbian', 'serum', 'woman', 'sexploitation', 'mordrid', 'romance', 'erotica', 'divorce', 'sexy', 'panty']\n",
      "topic-12 ['supranatural', 'french', 'story', 'episode', 'italian', 'good', 'show', 'see', 'great', 'acting', 'character', 'watch', 'like', 'spain', 'time']\n",
      "topic-13 ['aishwarya', 'softcore', 'rape', 'pornographic', 'abigail', 'kissed', 'twilight', 'blossomed', 'rapist', 'athena', 'erotic', 'compassionately', 'sex', 'farcical', 'zone']\n",
      "topic-14 ['stanly', 'scooby', 'elmer', 'gagged', 'chuckling', 'smallville', 'tucci', 'doo', 'scrappy', 'bromwell', 'snl', 'springer', 'coulier', 'ringmaster', 'zorak']\n",
      "topic-15 ['horror', 'vampire', 'noire', 'attach', 'scare', 'closet', 'gothic', 'putrid', 'porno', 'nation', 'nuit', 'laughable', 'filipino', 'sex', 'verisimilitude']\n",
      "topic-16 ['bolo', 'yeung', 'zombie', 'horror', 'sammo', 'monster', 'make', 'character', 'vampire', 'scene', 'kung', 'end', 'fi', 'martial', 'sci']\n",
      "topic-17 ['baptist', 'war', 'platoon', 'game', 'propaganda', 'mormon', 'swedish', 'screened', 'farrel', 'soldier', 'outstanding', 'russia', 'germany', 'military', 'german']\n",
      "topic-18 ['book', 'glistening', 'hillsborough', 'good', 'novel', 'story', 'great', 'read', 'circling', 'acting', 'beatons', 'time', 'hagar', 'watch', 'much']\n",
      "topic-19 ['zombie', 'horror', 'scene', 'sammo', 'time', 'take', 'like', 'martial', 'action', 'character', 'animal', 'story', 'good', 'gun', 'also']\n",
      "topic-20 ['horror', 'vampire', 'zombie', 'time', 'scene', 'alien', 'action', 'bug', 'really', 'character', 'like', 'good', 'director', 'story', 'get']\n",
      "topic-21 ['season', 'sci', 'fi', 'alien', 'sitcom', 'episode', 'show', 'hunter', 'trek', 'canceled', 'martian', 'hooked', 'series', 'ship', 'abc']\n",
      "topic-22 ['zombie', 'sammo', 'horror', 'character', 'vampire', 'like', 'killer', 'man', 'fi', 'time', 'sci', 'good', 'scene', 'story', 'monster']\n",
      "topic-23 ['bonhoeffer', 'gay', 'supranatural', 'hillsborough', 'ciano', 'bromwell', 'blimp', 'colombo', 'mexican', 'wuhl', 'standardize', 'infantilize', 'gandhi', 'student', 'hf']\n",
      "topic-24 ['pollan', 'horror', 'kiyoshi', 'blaisdell', 'hagar', 'scary', 'scared', 'cinderella', 'make', 'like', 'nuit', 'prince', 'mother', 'dragstrip', 'ghost']\n",
      "topic-25 ['horror', 'scary', 'like', 'good', 'hopelessly', 'watch', 'christmas', 'bad', 'eads', 'really', 'make', 'see', 'acting', 'awful', 'get']\n",
      "topic-26 ['farrel', 'vietnam', 'vet', 'colin', 'bozz', 'afghanistan', 'whove', 'veteran', 'yugoslavia', 'tigerland', 'war', 'recruit', 'civilian', 'venezuela', 'jesus']\n",
      "topic-27 ['cena', 'match', 'wwe', 'undertaker', 'mvp', 'benoit', 'ppv', 'booker', 'batista', 'vipul', 'wrestlemania', 'intercontinental', 'taker', 'kane', 'tag']\n",
      "topic-28 ['brock', 'pokemon', 'white', 'war', 'african', 'martial', 'mormon', 'episode', 'fight', 'avalanche', 'sammo', 'season', 'beatles', 'black', 'show']\n",
      "topic-29 ['bolo', 'yeung', 'vampire', 'zombie', 'horror', 'soldier', 'time', 'man', 'character', 'action', 'like', 'good', 'scene', 'also', 'even']\n",
      "topic-30 ['kung', 'fu', 'anime', 'sammo', 'zombie', 'werners', 'action', 'japanese', 'like', 'war', 'see', 'fight', 'martial', 'people', 'ninja']\n",
      "topic-31 ['like', 'dont', 'good', 'time', 'great', 'character', 'see', 'really', 'review', 'seen', 'made', 'even', 'much', 'musical', 'story']\n",
      "topic-32 ['woman', 'sexual', 'comedy', 'gay', 'love', 'like', 'romantic', 'character', 'sex', 'really', 'funny', 'leon', 'people', 'story', 'little']\n",
      "topic-33 ['cement', 'feeder', 'horror', 'beatons', 'kung', 'martial', 'fu', 'good', 'sliminess', 'killer', 'vampire', 'probobly', 'arbuckle', 'overprinting', 'ul']\n",
      "topic-34 ['christian', 'religious', 'haunting', 'horror', 'scare', 'jesus', 'bible', 'closet', 'christ', 'ringu', 'religion', 'comdey', 'church', 'ghost', 'evangelical']\n",
      "topic-35 ['mulroney', 'shrunk', 'child', 'dermot', 'med', 'childrens', 'molested', 'adult', 'mvp', 'fluffer', 'milligan', 'kid', 'kidmans', 'akshay', 'amitabh']\n",
      "topic-36 ['gay', 'sex', 'porn', 'scene', 'good', 'horror', 'urinates', 'funny', 'like', 'bad', 'really', 'sexual', 'watch', 'seen', 'see']\n",
      "topic-37 ['family', 'father', 'sister', 'son', 'life', 'daughter', 'brother', 'dad', 'mother', 'story', 'child', 'love', 'hagar', 'two', 'character']\n",
      "topic-38 ['horror', 'zombie', 'batman', 'soldier', 'dracula', 'man', 'scene', 'vampire', 'time', 'like', 'gun', 'action', 'alien', 'character', 'monster']\n",
      "topic-39 ['kris', 'shefali', 'kung', 'fu', 'thakur', 'band', 'martial', 'songwriter', 'guitarist', 'concert', 'wedlock', 'india', 'anime', 'gangster', 'midlers']\n",
      "topic-40 ['zombie', 'horror', 'vampire', 'character', 'like', 'man', 'creature', 'action', 'batman', 'scene', 'time', 'killer', 'bad', 'house', 'little']\n",
      "topic-41 ['closet', 'china', 'russian', 'haunted', 'russia', 'hitler', 'nell', 'geisha', 'japan', 'comdey', 'chinese', 'disney', 'globalization', 'worker', 'treaty']\n",
      "topic-42 ['shakespeare', 'acclaimed', 'orson', 'kidmans', 'schnaas', 'angkor', 'spurlock', 'hoffman', 'bloodbath', 'zombie', 'it', 'percent', 'tazer', 'freeway', 'undead']\n",
      "topic-43 ['zombie', 'horror', 'alien', 'like', 'scene', 'vampire', 'get', 'good', 'animal', 'look', 'story', 'even', 'bug', 'fi', 'character']\n",
      "topic-44 ['bad', 'like', 'really', 'even', 'funny', 'get', 'character', 'dont', 'scene', 'boring', 'laugh', 'carrere', 'love', 'make', 'time']\n",
      "topic-45 ['magtena', 'coleen', 'lightest', 'nooooo', 'good', 'unprejudiced', 'great', 'kung', 'scaffolding', 'fu', 'bad', 'rosalind', 'time', 'see', 'watch']\n",
      "topic-46 ['horror', 'zombie', 'vampire', 'fi', 'sci', 'like', 'kung', 'time', 'character', 'good', 'even', 'story', 'get', 'work', 'lot']\n",
      "topic-47 ['babysitting', 'descriptive', 'kimberly', 'chill', 'squirrel', 'kid', 'scariest', 'finnish', 'boggy', 'scared', 'romeros', 'plotwise', 'duvalls', 'swedish', 'scary']\n",
      "topic-48 ['zombie', 'horror', 'sniper', 'vampire', 'sammo', 'soldier', 'werewolf', 'monster', 'character', 'man', 'kung', 'cat', 'like', 'scene', 'good']\n",
      "topic-49 ['french', 'show', 'italian', 'france', 'princesse', 'character', 'paris', 'tam', 'like', 'italy', 'story', 'portuguese', 'czech', 'time', 'garbo']\n",
      "./\n",
      "[-5.21047, -1.44717, -5.28896, -4.10104, -2.0227, -6.92173, -7.72669, -2.09617, -4.41246, -5.19115, -7.9885, -7.84452, -4.67178, -6.54706, -16.47717, -8.17312, -4.1028, -6.14178, -5.42597, -1.77396, -1.35026, -2.11004, -2.58066, -13.1394, -8.29194, -3.7904, -6.99365, -5.08661, -5.27944, -1.58794, -4.21748, -3.60795, -1.74019, -4.60589, -4.73622, -9.51038, -3.30315, -2.13386, -1.98973, -5.85028, -1.76797, -3.87077, -10.69849, -2.15731, -2.91753, -5.00985, -1.73245, -9.04592, -2.53661, -3.59026]\n",
      "-4.9759566\n",
      "[-0.09287, 0.01824, -0.04836, -0.087, -9e-05, -0.08281, -0.18252, 0.03585, -0.04019, -0.08602, -0.16417, -0.14707, -0.05215, -0.12418, -0.10787, -0.19021, -0.08311, -0.08631, -0.06023, -0.00044, 0.044, 0.00901, -0.00707, -0.18545, -0.16886, -0.02174, -0.10521, 0.09675, -0.10148, -0.02265, -0.00786, -0.07251, 0.05814, -0.08438, -0.03162, -0.12794, 0.0219, 0.08583, 0.06148, -0.13876, 0.05356, -0.13914, -0.1904, 0.03855, -0.04018, -0.08412, 0.00204, -0.14751, -0.0055, -0.06324]\n",
      "-0.0571174\n",
      "[-0.42008, -0.04573, 0.03962, -0.11386, 0.23671, -0.24112, -0.54881, 0.3245, -0.08175, 0.0196, -0.59636, -0.38492, -0.17986, -0.44101, -0.94295, -0.48641, 0.011, -0.18707, -0.25344, 0.15292, 0.20556, 0.17587, 0.2119, -0.84928, -0.40795, -0.13074, -0.31755, 0.4323, -0.14254, 0.09137, 0.09948, -0.16924, 0.26115, -0.25185, 0.01048, -0.61642, 0.02692, 0.39977, 0.3736, -0.20646, 0.31899, -0.23095, -0.84291, 0.30495, -0.02721, -0.56247, 0.17537, -0.6547, 0.25417, 0.07264]\n",
      "-0.12269540000000001\n",
      "[-2.78239, 0.15595, -2.29619, -2.62794, -0.59279, -2.8679, -5.13169, 0.11319, -1.63443, -3.18107, -4.67516, -4.60227, -1.62777, -3.94261, -3.21432, -5.47806, -2.99462, -2.70586, -1.9674, -0.52191, 0.46952, -0.68333, -1.07885, -5.14031, -4.87192, -1.12325, -3.3913, 0.03057, -3.14336, -1.1479, -1.0436, -2.33156, 0.51827, -2.71146, -2.08634, -3.80259, -0.15126, 0.66075, 0.59922, -4.25763, 0.61513, -4.28201, -5.35259, 0.09937, -1.83724, -2.50931, -0.62965, -4.18465, -1.10536, -2.10309]\n",
      "-2.1710194\n",
      "[0.41325, 0.26848, 0.4653, 0.40045, 0.30769, 0.43482, 0.546, 0.3292, 0.42126, 0.45147, 0.53146, 0.57429, 0.4341, 0.53681, 0.55796, 0.56561, 0.48713, 0.46158, 0.33095, 0.31548, 0.29318, 0.32624, 0.37323, 0.57344, 0.49415, 0.37142, 0.44179, 0.65617, 0.41195, 0.33344, 0.40514, 0.36924, 0.31582, 0.41573, 0.50005, 0.48758, 0.34974, 0.37499, 0.34022, 0.47775, 0.30969, 0.44595, 0.58984, 0.31392, 0.35006, 0.41378, 0.33258, 0.51447, 0.39365, 0.35574]\n",
      "0.4232847999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [00:02, 719.34it/s]\n",
      "1563it [00:01, 985.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -4.9759566, 'npmi_wiki': -0.0571174, 'uci_wiki': -2.1710194, 'CV_wiki': 0.4232847999999998, 'cp_wiki': -0.12269540000000001, 'sim_w2v': 0.17356750496417564, 'diversity': 0.5426666666666666, 'filename': 'results/240518_150636.txt', 'acc': 0.58988, 'macro-F1': 0.5877974014410481, 'Purity': 0.55276, 'NMI': 0.0045109195716402616}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.94928 - dist: 0.22848 - cons: -0.03655\n",
      "Epoch-1 / recon: 7.91551 - dist: 0.21724 - cons: -0.03765\n",
      "Epoch-2 / recon: 7.90176 - dist: 0.21195 - cons: -0.03834\n",
      "Epoch-3 / recon: 7.89281 - dist: 0.20798 - cons: -0.03917\n",
      "Epoch-4 / recon: 7.88606 - dist: 0.20523 - cons: -0.03982\n",
      "Epoch-5 / recon: 7.88070 - dist: 0.20269 - cons: -0.04042\n",
      "Epoch-6 / recon: 7.87610 - dist: 0.20088 - cons: -0.04091\n",
      "Epoch-7 / recon: 7.87205 - dist: 0.19918 - cons: -0.04125\n",
      "Epoch-8 / recon: 7.86848 - dist: 0.19751 - cons: -0.04163\n",
      "Epoch-9 / recon: 7.86535 - dist: 0.19596 - cons: -0.04191\n",
      "Epoch-10 / recon: 7.86239 - dist: 0.19451 - cons: -0.04216\n",
      "Epoch-11 / recon: 7.85968 - dist: 0.19319 - cons: -0.04238\n",
      "Epoch-12 / recon: 7.85712 - dist: 0.19200 - cons: -0.04258\n",
      "Epoch-13 / recon: 7.85473 - dist: 0.19094 - cons: -0.04276\n",
      "Epoch-14 / recon: 7.85243 - dist: 0.19000 - cons: -0.04295\n",
      "Epoch-15 / recon: 7.85020 - dist: 0.18911 - cons: -0.04307\n",
      "Epoch-16 / recon: 7.84807 - dist: 0.18831 - cons: -0.04319\n",
      "Epoch-17 / recon: 7.84603 - dist: 0.18751 - cons: -0.04331\n",
      "Epoch-18 / recon: 7.84406 - dist: 0.18681 - cons: -0.04342\n",
      "Epoch-19 / recon: 7.84221 - dist: 0.18618 - cons: -0.04353\n",
      "Epoch-20 / recon: 7.84042 - dist: 0.18553 - cons: -0.04364\n",
      "Epoch-21 / recon: 7.83874 - dist: 0.18498 - cons: -0.04374\n",
      "Epoch-22 / recon: 7.83711 - dist: 0.18441 - cons: -0.04382\n",
      "Epoch-23 / recon: 7.83556 - dist: 0.18391 - cons: -0.04390\n",
      "Epoch-24 / recon: 7.83408 - dist: 0.18342 - cons: -0.04398\n",
      "Epoch-25 / recon: 7.83262 - dist: 0.18298 - cons: -0.04408\n",
      "Epoch-26 / recon: 7.83125 - dist: 0.18251 - cons: -0.04417\n",
      "Epoch-27 / recon: 7.82991 - dist: 0.18207 - cons: -0.04425\n",
      "Epoch-28 / recon: 7.82863 - dist: 0.18166 - cons: -0.04432\n",
      "Epoch-29 / recon: 7.82740 - dist: 0.18130 - cons: -0.04439\n",
      "Epoch-30 / recon: 7.82622 - dist: 0.18093 - cons: -0.04445\n",
      "Epoch-31 / recon: 7.82508 - dist: 0.18061 - cons: -0.04451\n",
      "Epoch-32 / recon: 7.82396 - dist: 0.18026 - cons: -0.04458\n",
      "Epoch-33 / recon: 7.82289 - dist: 0.17995 - cons: -0.04463\n",
      "Epoch-34 / recon: 7.82183 - dist: 0.17966 - cons: -0.04468\n",
      "Epoch-35 / recon: 7.82083 - dist: 0.17937 - cons: -0.04474\n",
      "Epoch-36 / recon: 7.81985 - dist: 0.17910 - cons: -0.04480\n",
      "Epoch-37 / recon: 7.81890 - dist: 0.17883 - cons: -0.04485\n",
      "Epoch-38 / recon: 7.81797 - dist: 0.17857 - cons: -0.04491\n",
      "Epoch-39 / recon: 7.81708 - dist: 0.17831 - cons: -0.04496\n",
      "Epoch-40 / recon: 7.81622 - dist: 0.17810 - cons: -0.04501\n",
      "Epoch-41 / recon: 7.81538 - dist: 0.17787 - cons: -0.04506\n",
      "Epoch-42 / recon: 7.81455 - dist: 0.17766 - cons: -0.04512\n",
      "Epoch-43 / recon: 7.81374 - dist: 0.17746 - cons: -0.04518\n",
      "Epoch-44 / recon: 7.81295 - dist: 0.17728 - cons: -0.04522\n",
      "Epoch-45 / recon: 7.81219 - dist: 0.17707 - cons: -0.04527\n",
      "Epoch-46 / recon: 7.81145 - dist: 0.17690 - cons: -0.04533\n",
      "Epoch-47 / recon: 7.81071 - dist: 0.17671 - cons: -0.04537\n",
      "Epoch-48 / recon: 7.81001 - dist: 0.17655 - cons: -0.04542\n",
      "Epoch-49 / recon: 7.80930 - dist: 0.17639 - cons: -0.04546\n",
      "------- Evaluation results -------\n",
      "topic-0 ['zombie', 'trek', 'war', 'hitler', 'soldier', 'nazi', 'band', 'alien', 'game', 'german', 'bloodbath', 'frontier', 'military', 'space', 'vet']\n",
      "topic-1 ['zorak', 'elmer', 'bug', 'bunny', 'pokemon', 'rabbit', 'brock', 'santa', 'vulcan', 'cartoon', 'trek', 'looney', 'batman', 'dinosaur', 'reindeer']\n",
      "topic-2 ['duvalls', 'bonhoeffer', 'good', 'trek', 'like', 'great', 'black', 'mexican', 'story', 'time', 'car', 'see', 'white', 'african', 'cartoon']\n",
      "topic-3 ['neo', 'murphy', 'eddie', 'skinhead', 'racism', 'white', 'african', 'racist', 'dinosaur', 'race', 'baptist', 'automatically', 'black', 'faithful', 'regret']\n",
      "topic-4 ['nooooo', 'magtena', 'unprejudiced', 'hopelessly', 'good', 'tarintino', 'scary', 'bad', 'absolutley', 'like', 'effect', 'keeped', 'time', 'best', 'dumb']\n",
      "topic-5 ['zombie', 'war', 'soldier', 'trek', 'hitler', 'alien', 'character', 'time', 'western', 'game', 'world', 'man', 'propaganda', 'life', 'band']\n",
      "topic-6 ['book', 'novel', 'good', 'shakespeare', 'read', 'story', 'character', 'mintons', 'italian', 'time', 'great', 'much', 'like', 'french', 'see']\n",
      "topic-7 ['supranatural', 'gay', 'rna', 'disney', 'diamantino', 'buffet', 'harilal', 'french', 'ducktales', 'japanese', 'war', 'gandhi', 'efx', 'people', 'story']\n",
      "topic-8 ['ambushed', 'mitchum', 'sniper', 'soldier', 'trek', 'war', 'zombie', 'wwe', 'wwii', 'western', 'episode', 'military', 'match', 'nazi', 'sgt']\n",
      "topic-9 ['riped', 'sliminess', 'pollan', 'taker', 'overprinting', 'schnaas', 'zodiac', 'slug', 'zombie', 'btk', 'bloodbath', 'unfitting', 'weber', 'killer', 'it']\n",
      "topic-10 ['blimp', 'farrel', 'jap', 'recruit', 'lengle', 'hamish', 'obi', 'beatons', 'iraqi', 'colin', 'softly', 'biko', 'eyre', 'navy', 'tigerland']\n",
      "topic-11 ['horror', 'putrid', 'carrere', 'scary', 'cap', 'closet', 'loosing', 'retarded', 'slasher', 'chainsaw', 'awful', 'gore', 'murdering', 'scare', 'fright']\n",
      "topic-12 ['janeway', 'sg', 'episode', 'trek', 'season', 'ambushed', 'stargate', 'voyager', 'guitarist', 'zombie', 'smallville', 'series', 'twilight', 'zone', 'metal']\n",
      "topic-13 ['sex', 'gay', 'porn', 'rape', 'scene', 'like', 'sexual', 'bad', 'rapist', 'good', 'dont', 'erotic', 'nudity', 'people', 'plot']\n",
      "topic-14 ['serbian', 'urmilla', 'deter', 'amitabh', 'govinda', 'gershwin', 'ringmaster', 'desi', 'springer', 'abigail', 'critiquing', 'justifying', 'matondkar', 'knockout', 'analyzing']\n",
      "topic-15 ['show', 'nuit', 'horror', 'zorak', 'noire', 'episode', 'smallville', 'like', 'make', 'time', 'well', 'scary', 'eads', 'good', 'people']\n",
      "topic-16 ['comedy', 'woman', 'romantic', 'love', 'character', 'funny', 'show', 'like', 'time', 'would', 'two', 'gay', 'really', 'wife', 'married']\n",
      "topic-17 ['supranatural', 'episode', 'italian', 'buffet', 'french', 'story', 'time', 'good', 'show', 'acting', 'like', 'great', 'character', 'seen', 'script']\n",
      "topic-18 ['book', 'keeped', 'music', 'bogdonavich', 'nadja', 'great', 'good', 'deewar', 'like', 'time', 'soundtrack', 'read', 'gujarati', 'made', 'see']\n",
      "topic-19 ['werewolf', 'vampire', 'dracula', 'jovi', 'earthling', 'bon', 'pallette', 'orson', 'hunter', 'jenna', 'theresa', 'maddy', 'cat', 'mammy', 'kitty']\n",
      "topic-20 ['horror', 'like', 'really', 'bad', 'watch', 'get', 'even', 'make', 'time', 'thing', 'story', 'scooby', 'scene', 'good', 'dont']\n",
      "topic-21 ['shaolin', 'diagnosis', 'scaffolding', 'flynn', 'yeung', 'bolo', 'mafia', 'rosalind', 'columbo', 'clutter', 'kung', 'philo', 'dixon', 'flynns', 'murder']\n",
      "topic-22 ['war', 'zombie', 'trek', 'soldier', 'action', 'fi', 'starfleet', 'sci', 'hitler', 'alien', 'story', 'western', 'army', 'game', 'like']\n",
      "topic-23 ['anime', 'amitabh', 'animation', 'ajay', 'kung', 'carrere', 'fu', 'animated', 'shiny', 'batman', 'gamers', 'sholay', 'action', 'ninja', 'character']\n",
      "topic-24 ['aquafresh', 'duvalls', 'explainable', 'buseys', 'glistening', 'lightest', 'expunged', 'coleen', 'bypassed', 'spurlock', 'ditto', 'wei', 'warmly', 'receipt', 'screened']\n",
      "topic-25 ['trek', 'zombie', 'war', 'soldier', 'fi', 'sci', 'alien', 'space', 'scene', 'time', 'military', 'vietnam', 'like', 'people', 'look']\n",
      "topic-26 ['guitarist', 'palestinian', 'globalization', 'kung', 'fu', 'definitive', 'harilal', 'eldest', 'midlers', 'band', 'hagar', 'santa', 'hitler', 'khanna', 'israeli']\n",
      "topic-27 ['trek', 'war', 'zombie', 'soldier', 'well', 'vietnam', 'hitler', 'alien', 'like', 'series', 'german', 'frontier', 'macarthur', 'world', 'time']\n",
      "topic-28 ['fullscreen', 'good', 'comedy', 'funny', 'well', 'romantic', 'character', 'love', 'great', 'really', 'like', 'performance', 'mvp', 'watch', 'end']\n",
      "topic-29 ['child', 'shrunk', 'med', 'serb', 'adopt', 'worm', 'childrens', 'broomstick', 'bedknobs', 'kid', 'disney', 'adopted', 'babysitting', 'animation', 'croat']\n",
      "topic-30 ['sinatra', 'versatility', 'astaire', 'kelly', 'sergeant', 'audition', 'aweigh', 'caron', 'blaine', 'simmons', 'gene', 'singin', 'clarence', 'musical', 'gershwin']\n",
      "topic-31 ['like', 'really', 'good', 'dont', 'great', 'bad', 'time', 'see', 'rajini', 'seen', 'character', 'made', 'music', 'people', 'get']\n",
      "topic-32 ['horror', 'good', 'like', 'dentist', 'character', 'scene', 'great', 'see', 'murder', 'even', 'really', 'elvira', 'wife', 'man', 'well']\n",
      "topic-33 ['hagar', 'daughter', 'mother', 'husband', 'mulroney', 'son', 'harilal', 'family', 'woman', 'story', 'life', 'dermot', 'shrunk', 'father', 'wife']\n",
      "topic-34 ['zombie', 'trek', 'war', 'vietnam', 'sniper', 'stargate', 'soldier', 'vulcan', 'game', 'tng', 'german', 'military', 'people', 'alien', 'world']\n",
      "topic-35 ['alien', 'sword', 'upshot', 'platoon', 'war', 'soldier', 'science', 'robot', 'ship', 'gun', 'fi', 'sci', 'creature', 'radiation', 'weapon']\n",
      "topic-36 ['war', 'zombie', 'trek', 'soldier', 'fi', 'sci', 'german', 'documentary', 'people', 'series', 'episode', 'band', 'story', 'like', 'world']\n",
      "topic-37 ['gandhi', 'amitabh', 'mahatma', 'croat', 'time', 'character', 'scene', 'song', 'govinda', 'like', 'year', 'chhaya', 'story', 'music', 'two']\n",
      "topic-38 ['family', 'marija', 'father', 'brother', 'son', 'show', 'gillmore', 'sister', 'story', 'dad', 'daughter', 'time', 'good', 'would', 'dutton']\n",
      "topic-39 ['gay', 'sex', 'fluffer', 'homosexual', 'lesbian', 'novel', 'rift', 'harrer', 'homosexuality', 'mvp', 'blurred', 'sexual', 'amitabh', 'greek', 'adaptation']\n",
      "topic-40 ['danish', 'upshot', 'russia', 'russian', 'sweden', 'soviet', 'bypassed', 'croat', 'joshua', 'swedish', 'political', 'amateurism', 'country', 'union', 'rarity']\n",
      "topic-41 ['zorak', 'scarecrow', 'typing', 'recomend', 'elm', 'haunted', 'anime', 'scared', 'revolting', 'scooby', 'freddys', 'eads', 'ghost', 'haunting', 'ducktales']\n",
      "topic-42 ['slasher', 'horror', 'murdering', 'creepy', 'russian', 'killer', 'vampire', 'filth', 'supernatural', 'afro', 'zombie', 'newscaster', 'rubric', 'church', 'sermon']\n",
      "topic-43 ['kid', 'child', 'show', 'great', 'adult', 'good', 'time', 'really', 'watched', 'like', 'smallville', 'family', 'watch', 'year', 'love']\n",
      "topic-44 ['brock', 'gershwin', 'caron', 'joker', 'singin', 'ballet', 'batgirl', 'astaire', 'ta', 'elmer', 'doolittle', 'season', 'baseball', 'guitarist', 'sabrina']\n",
      "topic-45 ['zombie', 'trek', 'war', 'game', 'beatles', 'soldier', 'vietnam', 'military', 'sniper', 'wwii', 'german', 'scene', 'propaganda', 'band', 'rock']\n",
      "topic-46 ['kidmans', 'christmas', 'kiddy', 'softcore', 'zombie', 'stuningly', 'dumbest', 'angkor', 'marley', 'jouissance', 'asleep', 'santa', 'tagliner', 'jesus', 'gymkata']\n",
      "topic-47 ['bypassed', 'buseys', 'memento', 'soundtrack', 'ajay', 'music', 'lyric', 'song', 'ishtar', 'laughable', 'refund', 'descriptive', 'amateurism', 'hindi', 'hopelessly']\n",
      "topic-48 ['good', 'crime', 'killer', 'cement', 'gangster', 'murder', 'like', 'story', 'well', 'scene', 'role', 'great', 'action', 'best', 'criminal']\n",
      "topic-49 ['descriptive', 'chill', 'comdey', 'mvp', 'freddys', 'freddy', 'sammo', 'krueger', 'elm', 'hung', 'scared', 'switch', 'ghost', 'doe', 'ringu']\n",
      "./\n",
      "[-3.17349, -6.25837, -2.09841, -3.13144, -2.2414, -1.91433, -2.63843, -9.57889, -4.27699, -6.90437, -9.70277, -4.4443, -5.45286, -3.38546, -9.58141, -4.69204, -1.37358, -6.57841, -3.68817, -9.3315, -2.41669, -11.89276, -2.76759, -10.31227, -4.55567, -2.08485, -6.86003, -2.28684, -1.84759, -5.12887, -4.69442, -4.08892, -1.90038, -2.94875, -3.08602, -3.86451, -1.95904, -3.08532, -3.8869, -6.44482, -4.25233, -10.12672, -5.75751, -1.3655, -7.63685, -3.31894, -6.50282, -6.34904, -2.31894, -10.79932]\n",
      "-4.8997366\n",
      "[-0.05219, -0.13483, -0.0354, -0.1111, -0.03221, -0.00766, -0.0081, -0.17057, -0.11667, -0.09177, -0.13256, -0.10119, -0.06409, 0.04447, -0.13864, -0.06637, 0.04148, -0.07409, -0.0754, -0.17255, 0.01448, -0.18532, -0.05874, -0.11548, -0.03178, -0.03945, -0.2186, -0.0239, -0.02071, -0.10759, -0.10699, -0.08622, 0.01443, 0.00942, -0.04602, -0.08576, -0.00872, -0.08093, -0.02449, -0.09426, -0.08531, -0.10665, -0.15115, 0.02031, -0.23054, -0.05156, -0.08912, -0.088, -0.01021, -0.16212]\n",
      "-0.07500839999999998\n",
      "[0.03473, -0.09882, -0.16229, -0.01845, -0.4279, 0.10131, 0.0228, -0.60068, -0.11941, -0.73566, -0.62446, -0.31837, -0.00301, 0.29719, -0.61916, -0.08191, 0.22532, -0.28901, -0.36803, -0.45841, 0.0934, -0.68856, 0.02984, -0.33489, -0.62857, 0.06489, -0.50328, 0.09763, 0.04015, -0.43191, -0.02744, -0.31577, 0.10908, 0.144, 0.06008, 0.01498, 0.09033, -0.03913, 0.0582, -0.28679, -0.13752, -0.65999, -0.34542, 0.02841, -0.52299, 0.0078, -0.62566, -0.45149, 0.14116, -0.7476]\n",
      "-0.20022559999999998\n",
      "[-1.92008, -4.74441, -1.11273, -3.62193, -1.05957, -0.49276, -0.76778, -4.82426, -3.61629, -2.66735, -3.73463, -3.17754, -2.84523, -0.06715, -3.83069, -2.22691, 0.41277, -2.32924, -2.24839, -5.37594, -0.37379, -5.42538, -2.02219, -3.90416, -0.87815, -1.50512, -6.28978, -1.12334, -1.15897, -3.26409, -3.76044, -2.65562, -0.00459, -1.05228, -1.89039, -3.05604, -0.66427, -2.53771, -1.37922, -3.47248, -3.10182, -3.21045, -4.60261, 0.15434, -6.55734, -1.87744, -2.57814, -2.86823, -0.90831, -4.76579]\n",
      "-2.539678199999999\n",
      "[0.37405, 0.55577, 0.32371, 0.40507, 0.30808, 0.30096, 0.39245, 0.52554, 0.44105, 0.45349, 0.47528, 0.4499, 0.49943, 0.42182, 0.50848, 0.39717, 0.28201, 0.4718, 0.3668, 0.60511, 0.28675, 0.62396, 0.36938, 0.56554, 0.29445, 0.31654, 0.5223, 0.30068, 0.33419, 0.47496, 0.53663, 0.40369, 0.27445, 0.41112, 0.39031, 0.42947, 0.31809, 0.3633, 0.40318, 0.55588, 0.45304, 0.49093, 0.52477, 0.26231, 0.60175, 0.37366, 0.41068, 0.45433, 0.31756, 0.56178]\n",
      "0.42367299999999986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [00:02, 703.09it/s]\n",
      "1563it [00:01, 1028.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -4.8997366, 'npmi_wiki': -0.07500839999999998, 'uci_wiki': -2.539678199999999, 'CV_wiki': 0.42367299999999986, 'cp_wiki': -0.20022559999999998, 'sim_w2v': 0.166481458692187, 'diversity': 0.5666666666666667, 'filename': 'results/240518_152039.txt', 'acc': 0.59408, 'macro-F1': 0.5937392766064873, 'Purity': 0.55672, 'NMI': 0.004768001586599952}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.95061 - dist: 0.22580 - cons: -0.03492\n",
      "Epoch-1 / recon: 7.91609 - dist: 0.21640 - cons: -0.03648\n",
      "Epoch-2 / recon: 7.90075 - dist: 0.21137 - cons: -0.03725\n",
      "Epoch-3 / recon: 7.89140 - dist: 0.20695 - cons: -0.03794\n",
      "Epoch-4 / recon: 7.88485 - dist: 0.20368 - cons: -0.03859\n",
      "Epoch-5 / recon: 7.87970 - dist: 0.20104 - cons: -0.03898\n",
      "Epoch-6 / recon: 7.87540 - dist: 0.19892 - cons: -0.03929\n",
      "Epoch-7 / recon: 7.87170 - dist: 0.19705 - cons: -0.03955\n",
      "Epoch-8 / recon: 7.86852 - dist: 0.19528 - cons: -0.03975\n",
      "Epoch-9 / recon: 7.86567 - dist: 0.19388 - cons: -0.03991\n",
      "Epoch-10 / recon: 7.86300 - dist: 0.19244 - cons: -0.04015\n",
      "Epoch-11 / recon: 7.86048 - dist: 0.19115 - cons: -0.04032\n",
      "Epoch-12 / recon: 7.85805 - dist: 0.19001 - cons: -0.04047\n",
      "Epoch-13 / recon: 7.85579 - dist: 0.18889 - cons: -0.04065\n",
      "Epoch-14 / recon: 7.85364 - dist: 0.18796 - cons: -0.04080\n",
      "Epoch-15 / recon: 7.85162 - dist: 0.18708 - cons: -0.04093\n",
      "Epoch-16 / recon: 7.84975 - dist: 0.18636 - cons: -0.04105\n",
      "Epoch-17 / recon: 7.84797 - dist: 0.18569 - cons: -0.04118\n",
      "Epoch-18 / recon: 7.84630 - dist: 0.18510 - cons: -0.04127\n",
      "Epoch-19 / recon: 7.84471 - dist: 0.18451 - cons: -0.04138\n",
      "Epoch-20 / recon: 7.84320 - dist: 0.18395 - cons: -0.04148\n",
      "Epoch-21 / recon: 7.84179 - dist: 0.18346 - cons: -0.04158\n",
      "Epoch-22 / recon: 7.84045 - dist: 0.18299 - cons: -0.04167\n",
      "Epoch-23 / recon: 7.83914 - dist: 0.18261 - cons: -0.04176\n",
      "Epoch-24 / recon: 7.83789 - dist: 0.18216 - cons: -0.04185\n",
      "Epoch-25 / recon: 7.83667 - dist: 0.18176 - cons: -0.04193\n",
      "Epoch-26 / recon: 7.83550 - dist: 0.18141 - cons: -0.04199\n",
      "Epoch-27 / recon: 7.83437 - dist: 0.18108 - cons: -0.04205\n",
      "Epoch-28 / recon: 7.83331 - dist: 0.18076 - cons: -0.04211\n",
      "Epoch-29 / recon: 7.83230 - dist: 0.18049 - cons: -0.04219\n",
      "Epoch-30 / recon: 7.83131 - dist: 0.18024 - cons: -0.04226\n",
      "Epoch-31 / recon: 7.83037 - dist: 0.17996 - cons: -0.04232\n",
      "Epoch-32 / recon: 7.82944 - dist: 0.17972 - cons: -0.04236\n",
      "Epoch-33 / recon: 7.82856 - dist: 0.17949 - cons: -0.04242\n",
      "Epoch-34 / recon: 7.82771 - dist: 0.17927 - cons: -0.04246\n",
      "Epoch-35 / recon: 7.82688 - dist: 0.17905 - cons: -0.04250\n",
      "Epoch-36 / recon: 7.82607 - dist: 0.17885 - cons: -0.04255\n",
      "Epoch-37 / recon: 7.82529 - dist: 0.17868 - cons: -0.04259\n",
      "Epoch-38 / recon: 7.82452 - dist: 0.17850 - cons: -0.04264\n",
      "Epoch-39 / recon: 7.82378 - dist: 0.17834 - cons: -0.04268\n",
      "Epoch-40 / recon: 7.82306 - dist: 0.17818 - cons: -0.04273\n",
      "Epoch-41 / recon: 7.82236 - dist: 0.17800 - cons: -0.04276\n",
      "Epoch-42 / recon: 7.82168 - dist: 0.17784 - cons: -0.04279\n",
      "Epoch-43 / recon: 7.82101 - dist: 0.17769 - cons: -0.04282\n",
      "Epoch-44 / recon: 7.82038 - dist: 0.17755 - cons: -0.04286\n",
      "Epoch-45 / recon: 7.81975 - dist: 0.17740 - cons: -0.04289\n",
      "Epoch-46 / recon: 7.81914 - dist: 0.17728 - cons: -0.04292\n",
      "Epoch-47 / recon: 7.81854 - dist: 0.17715 - cons: -0.04295\n",
      "Epoch-48 / recon: 7.81796 - dist: 0.17700 - cons: -0.04299\n",
      "Epoch-49 / recon: 7.81739 - dist: 0.17690 - cons: -0.04302\n",
      "------- Evaluation results -------\n",
      "topic-0 ['horror', 'like', 'scene', 'even', 'bad', 'good', 'really', 'character', 'make', 'also', 'fan', 'director', 'well', 'get', 'time']\n",
      "topic-1 ['zombie', 'bloodbath', 'schnaas', 'riped', 'porno', 'eads', 'romeros', 'cheaply', 'alien', 'porn', 'mormon', 'buseys', 'lds', 'cambodia', 'snake']\n",
      "topic-2 ['war', 'sniper', 'man', 'wife', 'husband', 'story', 'bradford', 'serb', 'well', 'life', 'tony', 'time', 'propaganda', 'much', 'john']\n",
      "topic-3 ['episode', 'scooby', 'anime', 'animation', 'season', 'doo', 'like', 'show', 'series', 'cartoon', 'character', 'get', 'really', 'sci', 'sg']\n",
      "topic-4 ['grasshopper', 'gandhi', 'harilal', 'mahatma', 'nintendo', 'bug', 'colony', 'eldest', 'pixar', 'elmer', 'skater', 'reiser', 'bunny', 'leukemia', 'sabrina']\n",
      "topic-5 ['farrel', 'palestinian', 'recruit', 'rna', 'gere', 'colin', 'portuguese', 'tigerland', 'israel', 'norwegian', 'turkish', 'blimp', 'patriotic', 'danish', 'israeli']\n",
      "topic-6 ['russian', 'globalization', 'closet', 'kiyoshi', 'comdey', 'cullen', 'dentist', 'ghost', 'chinese', 'gandhi', 'danish', 'china', 'wei', 'norwegian', 'story']\n",
      "topic-7 ['war', 'soldier', 'wife', 'woman', 'life', 'character', 'man', 'vietnam', 'role', 'cameroon', 'great', 'two', 'made', 'world', 'good']\n",
      "topic-8 ['good', 'killer', 'crime', 'gangster', 'thriller', 'well', 'horror', 'scene', 'cement', 'story', 'criminal', 'murder', 'like', 'great', 'freeway']\n",
      "topic-9 ['anime', 'cartoon', 'airing', 'abigail', 'drowning', 'babysitting', 'disney', 'animation', 'spoof', 'alien', 'blew', 'personally', 'fi', 'finnish', 'sci']\n",
      "topic-10 ['man', 'soldier', 'time', 'serb', 'war', 'noir', 'good', 'character', 'veteran', 'wife', 'story', 'performance', 'like', 'also', 'best']\n",
      "topic-11 ['war', 'soldier', 'serb', 'man', 'story', 'get', 'wife', 'character', 'performance', 'go', 'woman', 'men', 'vietnam', 'time', 'military']\n",
      "topic-12 ['blurred', 'sexual', 'melon', 'sex', 'gay', 'starship', 'porno', 'male', 'flying', 'snl', 'homosexuality', 'detmers', 'wilson', 'heterosexual', 'chemistry']\n",
      "topic-13 ['comedy', 'funny', 'laugh', 'romantic', 'like', 'woman', 'show', 'really', 'love', 'character', 'good', 'think', 'dont', 'time', 'watch']\n",
      "topic-14 ['serb', 'war', 'trek', 'cadet', 'character', 'soldier', 'performance', 'wife', 'man', 'macarthur', 'well', 'scene', 'murder', 'time', 'vietnam']\n",
      "topic-15 ['time', 'great', 'story', 'japanese', 'see', 'communism', 'scene', 'good', 'china', 'like', 'well', 'russian', 'people', 'think', 'life']\n",
      "topic-16 ['family', 'algy', 'tenny', 'daughter', 'good', 'great', 'love', 'mother', 'story', 'like', 'book', 'well', 'father', 'character', 'child']\n",
      "topic-17 ['magtena', 'bad', 'good', 'hopelessly', 'time', 'really', 'seen', 'watch', 'nooooo', 'like', 'see', 'effect', 'worst', 'terrible', 'dont']\n",
      "topic-18 ['serbian', 'nooooo', 'china', 'ussr', 'mvp', 'croat', 'russian', 'russia', 'chinese', 'reinvents', 'african', 'grinch', 'unpredictable', 'globalization', 'desi']\n",
      "topic-19 ['murderer', 'unlucky', 'killer', 'murder', 'mvp', 'pallette', 'columbo', 'detective', 'solve', 'unavailable', 'ingenious', 'capote', 'depraved', 'clutter', 'serial']\n",
      "topic-20 ['italian', 'book', 'french', 'supranatural', 'episode', 'novel', 'good', 'story', 'read', 'buffet', 'character', 'show', 'see', 'adaptation', 'shakespeare']\n",
      "topic-21 ['fifteen', 'chimney', 'band', 'khanna', 'akshaye', 'anyways', 'portrays', 'scrooge', 'bass', 'nicely', 'christina', 'harilal', 'molested', 'trumpet', 'relation']\n",
      "topic-22 ['cena', 'ppv', 'wwe', 'booker', 'kung', 'mvp', 'intercontinental', 'wrestlemania', 'benoit', 'defeated', 'fu', 'undertaker', 'batista', 'taker', 'match']\n",
      "topic-23 ['nuit', 'eads', 'noire', 'scary', 'scared', 'descriptive', 'horror', 'pollan', 'spatula', 'creepy', 'chill', 'halloween', 'scare', 'atmosphere', 'bizarre']\n",
      "topic-24 ['war', 'man', 'wife', 'story', 'year', 'life', 'character', 'time', 'well', 'woman', 'puerto', 'macarthur', 'best', 'work', 'good']\n",
      "topic-25 ['dawkins', 'bible', 'santa', 'christian', 'biblical', 'lds', 'christmas', 'prophet', 'religious', 'flemming', 'sword', 'mormon', 'church', 'theology', 'joseph']\n",
      "topic-26 ['war', 'character', 'military', 'serb', 'great', 'soldier', 'man', 'wife', 'show', 'year', 'star', 'performance', 'woman', 'also', 'two']\n",
      "topic-27 ['war', 'soldier', 'role', 'vietnam', 'also', 'well', 'wife', 'like', 'time', 'woman', 'story', 'character', 'good', 'military', 'life']\n",
      "topic-28 ['raunchiest', 'funny', 'comedy', 'dowry', 'cushion', 'hesseman', 'hashed', 'romantic', 'lingered', 'laugh', 'wife', 'misused', 'stanly', 'aykroyd', 'love']\n",
      "topic-29 ['gay', 'black', 'white', 'sex', 'good', 'urinates', 'like', 'porn', 'people', 'scene', 'great', 'racism', 'really', 'funny', 'rape']\n",
      "topic-30 ['serb', 'soldier', 'war', 'woman', 'role', 'man', 'iraq', 'wife', 'life', 'vietnam', 'military', 'american', 'world', 'time', 'like']\n",
      "topic-31 ['like', 'character', 'really', 'time', 'bad', 'see', 'good', 'great', 'review', 'much', 'dont', 'even', 'get', 'story', 'made']\n",
      "topic-32 ['man', 'soldier', 'time', 'war', 'role', 'woman', 'also', 'wife', 'story', 'scene', 'vietnam', 'musical', 'husband', 'come', 'prisoner']\n",
      "topic-33 ['vietnam', 'serb', 'soldier', 'war', 'wife', 'military', 'character', 'best', 'time', 'director', 'performance', 'play', 'noir', 'story', 'get']\n",
      "topic-34 ['child', 'kid', 'mulroney', 'dermot', 'parent', 'show', 'family', 'baby', 'time', 'like', 'would', 'old', 'see', 'great', 'mordrid']\n",
      "topic-35 ['disney', 'christmas', 'musical', 'funniest', 'cinderella', 'coolio', 'yikes', 'shrek', 'duet', 'mahatma', 'bollywood', 'number', 'ballet', 'eddie', 'anastasia']\n",
      "topic-36 ['kid', 'good', 'child', 'great', 'show', 'like', 'tarintino', 'saw', 'disney', 'time', 'seen', 'school', 'really', 'adult', 'see']\n",
      "topic-37 ['war', 'like', 'wife', 'woman', 'story', 'great', 'character', 'get', 'man', 'life', 'married', 'role', 'time', 'also', 'make']\n",
      "topic-38 ['finnish', 'germany', 'serb', 'gershwin', 'viet', 'sweden', 'marlon', 'allied', 'horror', 'wwii', 'nazi', 'german', 'regime', 'kelly', 'kumari']\n",
      "topic-39 ['war', 'wife', 'character', 'man', 'best', 'soldier', 'time', 'vietnam', 'young', 'also', 'role', 'life', 'director', 'husband', 'much']\n",
      "topic-40 ['novel', 'festival', 'cheating', 'amitabh', 'drummer', 'adaptation', 'literature', 'soundtrack', 'musical', 'mvp', 'rift', 'singer', 'hindi', 'artistic', 'cap']\n",
      "topic-41 ['bolo', 'hadleyville', 'yeung', 'hitler', 'bamboo', 'sammo', 'hung', 'sakura', 'liu', 'nuremberg', 'verisimilitude', 'serb', 'chans', 'fu', 'tatsuhito']\n",
      "topic-42 ['war', 'soldier', 'sniper', 'also', 'story', 'man', 'show', 'life', 'time', 'well', 'pow', 'character', 'performance', 'role', 'woman']\n",
      "topic-43 ['janeway', 'trek', 'vulcan', 'roddenberry', 'klingons', 'galactica', 'spock', 'voyager', 'leukemia', 'smallville', 'pike', 'zorak', 'homicide', 'hf', 'starfleet']\n",
      "topic-44 ['haunted', 'horror', 'season', 'creepy', 'supernatural', 'ghost', 'scare', 'episode', 'castle', 'murder', 'evangelical', 'atmospheric', 'columbo', 'townspeople', 'vampire']\n",
      "topic-45 ['rape', 'erotica', 'lesbian', 'heterosexual', 'erotic', 'farcical', 'sexuality', 'rapist', 'softcore', 'nude', 'explicit', 'baby', 'depraved', 'homosexuality', 'milligan']\n",
      "topic-46 ['bogdonavich', 'cartoon', 'scooby', 'ducktales', 'music', 'rickshaw', 'show', 'great', 'disney', 'soundtrack', 'time', 'gadget', 'doo', 'gujarati', 'like']\n",
      "topic-47 ['kid', 'child', 'scare', 'childrens', 'scary', 'horror', 'vampire', 'adult', 'babysitting', 'scarecrow', 'ghost', 'parent', 'baptist', 'monster', 'creature']\n",
      "topic-48 ['trek', 'great', 'animal', 'good', 'dinosaur', 'story', 'time', 'like', 'sci', 'fi', 'bug', 'zombie', 'people', 'space', 'vampire']\n",
      "topic-49 ['glistening', 'good', 'see', 'acting', 'great', 'dont', 'really', 'time', 'think', 'story', 'warmly', 'watch', 'fantastic', 'liebmann', 'firelight']\n",
      "./\n",
      "[-0.97613, -7.22239, -2.23533, -2.68756, -9.73664, -8.53856, -3.97585, -2.64884, -2.64643, -7.33193, -2.53102, -2.03748, -7.92028, -1.69509, -2.30788, -1.76666, -1.98344, -4.45359, -7.13444, -7.20264, -4.37082, -9.13528, -3.36225, -10.09081, -2.39961, -3.61503, -1.38888, -1.42489, -4.11109, -2.00867, -1.46573, -2.67064, -2.10065, -2.47607, -4.74936, -6.44023, -2.81061, -0.85276, -5.98915, -1.82473, -3.80137, -14.31182, -1.81003, -10.69821, -4.56546, -4.5913, -3.62761, -4.18544, -3.05864, -4.23825]\n",
      "-4.304151399999999\n",
      "[0.03257, -0.12956, -0.02714, 0.03725, -0.22748, -0.18462, -0.16809, -0.0334, -0.00204, -0.13717, -0.03553, -0.00626, -0.1205, 0.046, -0.02975, 0.01291, -0.04242, -0.0635, -0.15624, -0.11921, -0.01298, -0.14353, 0.02704, -0.15404, -0.01234, -0.02122, 0.00417, 0.01944, -0.06964, 0.0304, 0.02532, -0.03657, 0.02656, -0.05752, -0.045, -0.11124, -0.02654, 0.02383, -0.17686, -0.01149, -0.11984, -0.159, 0.01552, -0.11211, -0.05529, -0.04848, -0.04995, -0.03301, -0.03897, -0.07475]\n",
      "-0.05504539999999999\n",
      "[0.09332, -0.58942, 0.02046, 0.23194, -0.6328, -0.45868, -0.38293, -0.00827, 0.20436, -0.31261, 0.05044, 0.04919, -0.46661, 0.18482, 0.05012, 0.10893, -0.00738, -0.51136, -0.43234, -0.22835, 0.03443, -0.62511, 0.22775, -0.46405, 0.06926, 0.20923, 0.06026, 0.11866, -0.41196, 0.09563, 0.14581, -0.06515, 0.13691, -0.02151, -0.06281, -0.24136, -0.12647, 0.09459, -0.215, 0.08242, -0.15437, -0.74804, 0.13404, -0.43083, 0.12632, 0.07421, -0.13872, 0.15785, 0.11049, -0.39486]\n",
      "-0.10519100000000002\n",
      "[0.33054, -3.71993, -0.9164, -0.27087, -6.77226, -5.40722, -4.84729, -1.34445, -0.78482, -4.3654, -1.19554, -0.39149, -3.98406, 0.17197, -1.02514, 0.04415, -1.88072, -1.9372, -4.60947, -4.10547, -1.01418, -4.12064, -1.30894, -4.82365, -0.5875, -1.89535, -0.09728, 0.16267, -2.3311, 0.20472, 0.04851, -1.52872, 0.23324, -1.60881, -1.74908, -3.4867, -0.92213, 0.21449, -5.4521, -0.65158, -3.70068, -4.65941, 0.00235, -4.02104, -2.53463, -2.45159, -2.02502, -2.05307, -1.36884, -2.24084]\n",
      "-2.0555594000000004\n",
      "[0.27032, 0.49613, 0.29485, 0.39654, 0.67799, 0.58547, 0.4956, 0.345, 0.33048, 0.5468, 0.30227, 0.29941, 0.52153, 0.33229, 0.30089, 0.27251, 0.45384, 0.37103, 0.51909, 0.53156, 0.39158, 0.49868, 0.62648, 0.61982, 0.28405, 0.47915, 0.28821, 0.28796, 0.40525, 0.28866, 0.30051, 0.30149, 0.29079, 0.31361, 0.37914, 0.45702, 0.33352, 0.27162, 0.55166, 0.28338, 0.40268, 0.60343, 0.28779, 0.68006, 0.46027, 0.54057, 0.41621, 0.45136, 0.3193, 0.34513]\n",
      "0.4100596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [00:02, 729.85it/s]\n",
      "1563it [00:01, 965.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -4.304151399999999, 'npmi_wiki': -0.05504539999999999, 'uci_wiki': -2.0555594000000004, 'CV_wiki': 0.4100596, 'cp_wiki': -0.10519100000000002, 'sim_w2v': 0.1715318778059141, 'diversity': 0.5253333333333333, 'filename': 'results/240518_153348.txt', 'acc': 0.58952, 'macro-F1': 0.5891482587150343, 'Purity': 0.55188, 'NMI': 0.0037977234338741075}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.95245 - dist: 0.22639 - cons: -0.03058\n",
      "Epoch-1 / recon: 7.91634 - dist: 0.21580 - cons: -0.03252\n",
      "Epoch-2 / recon: 7.90010 - dist: 0.20919 - cons: -0.03433\n",
      "Epoch-3 / recon: 7.88920 - dist: 0.20489 - cons: -0.03568\n",
      "Epoch-4 / recon: 7.88111 - dist: 0.20134 - cons: -0.03658\n",
      "Epoch-5 / recon: 7.87455 - dist: 0.19865 - cons: -0.03723\n",
      "Epoch-6 / recon: 7.86893 - dist: 0.19663 - cons: -0.03776\n",
      "Epoch-7 / recon: 7.86419 - dist: 0.19480 - cons: -0.03813\n",
      "Epoch-8 / recon: 7.86002 - dist: 0.19320 - cons: -0.03848\n",
      "Epoch-9 / recon: 7.85637 - dist: 0.19183 - cons: -0.03877\n",
      "Epoch-10 / recon: 7.85314 - dist: 0.19058 - cons: -0.03901\n",
      "Epoch-11 / recon: 7.85024 - dist: 0.18936 - cons: -0.03921\n",
      "Epoch-12 / recon: 7.84760 - dist: 0.18826 - cons: -0.03942\n",
      "Epoch-13 / recon: 7.84520 - dist: 0.18730 - cons: -0.03959\n",
      "Epoch-14 / recon: 7.84296 - dist: 0.18636 - cons: -0.03976\n",
      "Epoch-15 / recon: 7.84089 - dist: 0.18559 - cons: -0.03991\n",
      "Epoch-16 / recon: 7.83894 - dist: 0.18477 - cons: -0.04004\n",
      "Epoch-17 / recon: 7.83712 - dist: 0.18403 - cons: -0.04017\n",
      "Epoch-18 / recon: 7.83541 - dist: 0.18346 - cons: -0.04029\n",
      "Epoch-19 / recon: 7.83381 - dist: 0.18282 - cons: -0.04039\n",
      "Epoch-20 / recon: 7.83234 - dist: 0.18232 - cons: -0.04048\n",
      "Epoch-21 / recon: 7.83097 - dist: 0.18181 - cons: -0.04058\n",
      "Epoch-22 / recon: 7.82969 - dist: 0.18135 - cons: -0.04066\n",
      "Epoch-23 / recon: 7.82846 - dist: 0.18095 - cons: -0.04075\n",
      "Epoch-24 / recon: 7.82731 - dist: 0.18054 - cons: -0.04083\n",
      "Epoch-25 / recon: 7.82622 - dist: 0.18017 - cons: -0.04091\n",
      "Epoch-26 / recon: 7.82519 - dist: 0.17979 - cons: -0.04099\n",
      "Epoch-27 / recon: 7.82420 - dist: 0.17939 - cons: -0.04107\n",
      "Epoch-28 / recon: 7.82328 - dist: 0.17902 - cons: -0.04115\n",
      "Epoch-29 / recon: 7.82237 - dist: 0.17868 - cons: -0.04123\n",
      "Epoch-30 / recon: 7.82152 - dist: 0.17837 - cons: -0.04130\n",
      "Epoch-31 / recon: 7.82069 - dist: 0.17805 - cons: -0.04136\n",
      "Epoch-32 / recon: 7.81992 - dist: 0.17776 - cons: -0.04144\n",
      "Epoch-33 / recon: 7.81914 - dist: 0.17748 - cons: -0.04149\n",
      "Epoch-34 / recon: 7.81840 - dist: 0.17723 - cons: -0.04155\n",
      "Epoch-35 / recon: 7.81770 - dist: 0.17696 - cons: -0.04160\n",
      "Epoch-36 / recon: 7.81702 - dist: 0.17670 - cons: -0.04165\n",
      "Epoch-37 / recon: 7.81637 - dist: 0.17647 - cons: -0.04169\n",
      "Epoch-38 / recon: 7.81576 - dist: 0.17624 - cons: -0.04175\n",
      "Epoch-39 / recon: 7.81515 - dist: 0.17599 - cons: -0.04180\n",
      "Epoch-40 / recon: 7.81457 - dist: 0.17578 - cons: -0.04185\n",
      "Epoch-41 / recon: 7.81401 - dist: 0.17558 - cons: -0.04190\n",
      "Epoch-42 / recon: 7.81347 - dist: 0.17539 - cons: -0.04196\n",
      "Epoch-43 / recon: 7.81293 - dist: 0.17518 - cons: -0.04202\n",
      "Epoch-44 / recon: 7.81242 - dist: 0.17498 - cons: -0.04208\n",
      "Epoch-45 / recon: 7.81195 - dist: 0.17479 - cons: -0.04212\n",
      "Epoch-46 / recon: 7.81147 - dist: 0.17463 - cons: -0.04217\n",
      "Epoch-47 / recon: 7.81099 - dist: 0.17446 - cons: -0.04221\n",
      "Epoch-48 / recon: 7.81054 - dist: 0.17429 - cons: -0.04225\n",
      "Epoch-49 / recon: 7.81011 - dist: 0.17415 - cons: -0.04229\n",
      "------- Evaluation results -------\n",
      "topic-0 ['killer', 'wife', 'horror', 'soldier', 'make', 'murder', 'vietnam', 'scene', 'crime', 'also', 'man', 'performance', 'father', 'time', 'war']\n",
      "topic-1 ['serb', 'war', 'killer', 'horror', 'vampire', 'murder', 'good', 'character', 'dracula', 'work', 'man', 'soldier', 'time', 'well', 'great']\n",
      "topic-2 ['serb', 'war', 'murder', 'crime', 'horror', 'killer', 'vampire', 'prisoner', 'character', 'also', 'man', 'good', 'military', 'soldier', 'scene']\n",
      "topic-3 ['student', 'disney', 'school', 'wuhl', 'kid', 'show', 'teacher', 'like', 'time', 'seen', 'good', 'ciano', 'animation', 'watch', 'college']\n",
      "topic-4 ['lds', 'mormon', 'screwed', 'gram', 'church', 'spiritual', 'belief', 'intend', 'fooled', 'entertained', 'chemistry', 'shelton', 'wall', 'drummer', 'inept']\n",
      "topic-5 ['harilal', 'ciano', 'werners', 'tatsuhito', 'japanese', 'war', 'china', 'palestinian', 'gandhi', 'documentary', 'maruschka', 'bead', 'mardi', 'gras', 'american']\n",
      "topic-6 ['jesus', 'religion', 'atheist', 'christianity', 'scripture', 'catholic', 'christ', 'rapture', 'mormon', 'baptist', 'church', 'christian', 'sermon', 'pilate', 'trumpet']\n",
      "topic-7 ['magtena', 'nooooo', 'good', 'bad', 'great', 'see', 'time', 'watch', 'dont', 'really', 'best', 'disney', 'seen', 'fun', 've']\n",
      "topic-8 ['tremain', 'book', 'much', 'time', 'like', 'good', 'scene', 'love', 'story', 'best', 'character', 'daughter', 'amitabh', 'really', 'work']\n",
      "topic-9 ['murder', 'horror', 'killer', 'vampire', 'war', 'soldier', 'man', 'time', 'military', 'character', 'help', 'woman', 'also', 'scene', 'crime']\n",
      "topic-10 ['serb', 'yugoslavia', 'horror', 'vampire', 'war', 'serbian', 'scene', 'time', 'murder', 'man', 'killer', 'character', 'story', 'like', 'life']\n",
      "topic-11 ['french', 'jesss', 'elson', 'latrina', 'natella', 'france', 'bromwell', 'princesse', 'tam', 'verneuil', 'keisha', 'zorak', 'series', 'episode', 'show']\n",
      "topic-12 ['murder', 'serb', 'war', 'killer', 'horror', 'soldier', 'wife', 'scene', 'well', 'detective', 'vampire', 'vietnam', 'murdered', 'good', 'man']\n",
      "topic-13 ['stormare', 'kiddy', 'rapist', 'porn', 'baseketball', 'mockumentary', 'horror', 'gay', 'putrid', 'halloween', 'pornographic', 'stuningly', 'erotica', 'lds', 'simulated']\n",
      "topic-14 ['serb', 'horror', 'murder', 'killer', 'war', 'man', 'criminal', 'well', 'story', 'thing', 'scene', 'time', 'work', 'prison', 'soldier']\n",
      "topic-15 ['batman', 'war', 'joker', 'ta', 'propaganda', 'batwoman', 'nazi', 'ohearn', 'eddie', 'murphy', 'delirious', 'superman', 'conroy', 'collora', 'skinhead']\n",
      "topic-16 ['horror', 'like', 'gore', 'bad', 'really', 'good', 'even', 'people', 'make', 'monster', 'killer', 'acting', 'made', 'time', 'effect']\n",
      "topic-17 ['coolio', 'hopelessly', 'amateurism', 'spurlock', 'yashraj', 'carrere', 'zealous', 'mulroney', 'absolutley', 'bypassed', 'insufferable', 'dermot', 'brutally', 'brock', 'horrified']\n",
      "topic-18 ['bolo', 'yeung', 'whodunnit', 'kung', 'fu', 'venom', 'scaffolding', 'lau', 'chang', 'shaw', 'leung', 'sammo', 'shaolin', 'bloodbath', 'li']\n",
      "topic-19 ['child', 'family', 'son', 'kid', 'great', 'story', 'marija', 'father', 'like', 'gillmore', 'show', 'brother', 'parent', 'time', 'baby']\n",
      "topic-20 ['bug', 'trek', 'infant', 'dreamworks', 'pixar', 'grasshopper', 'disney', 'bunny', 'kirk', 'elmer', 'voyager', 'animation', 'vulcan', 'cartoon', 'klingons']\n",
      "topic-21 ['good', 'great', 'story', 'think', 'script', 'episode', 'character', 'really', 'acting', 'actor', 'diamantino', 'seen', 'watch', 'time', 'people']\n",
      "topic-22 ['book', 'italian', 'russian', 'danish', 'story', 'serbian', 'french', 'chinese', 'swedish', 'china', 'sweden', 'supranatural', 'good', 'japanese', 'italy']\n",
      "topic-23 ['romantic', 'character', 'good', 'well', 'love', 'mvp', 'romance', 'performance', 'great', 'acting', 'funny', 'comedy', 'fullscreen', 'dowry', 'story']\n",
      "topic-24 ['threesome', 'tatsuhito', 'ambushed', 'reset', 'kiras', 'stefaniuk', 'bug', 'sex', 'feeder', 'clavier', 'strickler', 'elmer', 'killer', 'gun', 'it']\n",
      "topic-25 ['serb', 'murder', 'horror', 'crime', 'wife', 'killer', 'soldier', 'take', 'man', 'story', 'well', 'character', 'vampire', 'detective', 'prison']\n",
      "topic-26 ['gay', 'woman', 'sex', 'hagar', 'husband', 'love', 'mother', 'wife', 'friend', 'daughter', 'character', 'sister', 'family', 'girl', 'comedy']\n",
      "topic-27 ['kid', 'scary', 'horror', 'good', 'scared', 'make', 'great', 'child', 'see', 'really', 'like', 'get', 'creepy', 'year', 'scare']\n",
      "topic-28 ['child', 'kid', 'shrunk', 'oldest', 'heartwarming', 'son', 'eldest', 'harilal', 'mvp', 'father', 'babysitting', 'marrying', 'kimberly', 'med', 'finnish']\n",
      "topic-29 ['croat', 'spear', 'loni', 'keystone', 'strangely', 'ronin', 'chaplin', 'lighting', 'car', 'cannonball', 'campus', 'bitterly', 'gritty', 'habit', 'bruckheimer']\n",
      "topic-30 ['book', 'novel', 'read', 'adaptation', 'mintons', 'shakespeare', 'dillinger', 'taboo', 'good', 'much', 'great', 'time', 'story', 'see', 'minton']\n",
      "topic-31 ['abc', 'petition', 'season', 'kelso', 'canceled', 'show', 'cancelled', 'bynes', 'counting', 'online', 'episode', 'airing', 'reeling', 'hooked', 'homicide']\n",
      "topic-32 ['virology', 'justness', 'daftness', 'innovatively', 'cement', 'unfitting', 'braille', 'freeway', 'software', 'ul', 'telecommunication', 'whomever', 'afflicted', 'moroni', 'ulagam']\n",
      "topic-33 ['rahman', 'cat', 'gershwin', 'haunted', 'undead', 'mouse', 'dinosaur', 'musically', 'jerry', 'musician', 'zombie', 'bug', 'jo', 'pinjar', 'lion']\n",
      "topic-34 ['character', 'like', 'really', 'see', 'time', 'much', 'review', 'good', 'story', 'actor', 'think', 'dont', 'make', 'even', 'great']\n",
      "topic-35 ['astaire', 'mink', 'duet', 'singin', 'footlight', 'choreographer', 'berkeley', 'broadway', 'blaine', 'flashdance', 'sings', 'musically', 'sinatra', 'musical', 'audition']\n",
      "topic-36 ['killer', 'horror', 'murder', 'life', 'crime', 'get', 'war', 'sniper', 'man', 'performance', 'also', 'scene', 'character', 'detective', 'story']\n",
      "topic-37 ['serb', 'yugoslavia', 'killer', 'horror', 'war', 'police', 'well', 'man', 'murder', 'criminal', 'killed', 'crime', 'wife', 'scene', 'character']\n",
      "topic-38 ['twilight', 'bloodbath', 'nuit', 'romeros', 'cinderella', 'book', 'comdey', 'lucifer', 'zone', 'vampire', 'noire', 'novel', 'scooby', 'outbreak', 'serling']\n",
      "topic-39 ['gameplay', 'baseball', 'evangelion', 'anime', 'sammo', 'ranma', 'game', 'dermot', 'benoit', 'pitcher', 'mvp', 'ab', 'hung', 'batista', 'mulroney']\n",
      "topic-40 ['harilal', 'santana', 'yokozuna', 'dibiase', 'helena', 'gandhi', 'wwe', 'lawler', 'jackies', 'cena', 'ppv', 'undertaker', 'midler', 'ohearn', 'luger']\n",
      "topic-41 ['killer', 'scene', 'horror', 'war', 'murder', 'first', 'character', 'man', 'well', 'also', 'good', 'get', 'make', 'old', 'time']\n",
      "topic-42 ['suare', 'comedy', 'farrel', 'funny', 'batman', 'romantic', 'scooby', 'show', 'superhero', 'hashed', 'batwoman', 'hee', 'like', 'good', 'laugh']\n",
      "topic-43 ['serb', 'vampire', 'war', 'killer', 'horror', 'german', 'man', 'scene', 'murder', 'good', 'yugoslavia', 'crime', 'find', 'get', 'tony']\n",
      "topic-44 ['rna', 'beatles', 'doodlebops', 'sniper', 'war', 'farrel', 'kosher', 'season', 'blimp', 'soldier', 'sabrina', 'harrer', 'episode', 'ambushed', 'sim']\n",
      "topic-45 ['delirious', 'hr', 'eddie', 'wilson', 'czech', 'afro', 'blurred', 'character', 'croat', 'batmobile', 'flying', 'time', 'story', 'sexual', 'carnosaur']\n",
      "topic-46 ['algy', 'tenny', 'drummond', 'gay', 'bonhoeffer', 'good', 'gillians', 'like', 'little', 'gadg', 'scene', 'great', 'really', 'lesbian', 'vampire']\n",
      "topic-47 ['music', 'great', 'soundtrack', 'see', 'good', 'song', 'bogdonavich', 'story', 'time', 'made', 'like', 'vinod', 'watch', 'really', 'gujarati']\n",
      "topic-48 ['war', 'horror', 'murder', 'sniper', 'time', 'killer', 'soldier', 'also', 'man', 'good', 'character', 'noir', 'like', 'crime', 'year']\n",
      "topic-49 ['bad', 'melon', 'like', 'sex', 'good', 'really', 'scene', 'dont', 'laugh', 'character', 'even', 'people', 'would', 'get', 'acting']\n",
      "./\n",
      "[-1.48087, -1.65073, -1.86402, -5.395, -3.14763, -10.17247, -3.15526, -5.62291, -2.5777, -1.53604, -1.81281, -9.18364, -1.95348, -8.64563, -1.65021, -7.28773, -4.34896, -7.3288, -9.11443, -3.02318, -6.77482, -6.19072, -5.08628, -5.37039, -13.12005, -2.13516, -1.7769, -1.85086, -3.84959, -10.84834, -3.97557, -6.1741, -9.67649, -5.86786, -1.72845, -4.3003, -1.78268, -1.63537, -7.47113, -11.39312, -9.22361, -0.94576, -5.48881, -2.135, -11.98429, -7.69834, -3.34576, -3.11352, -1.84887, -3.78434]\n",
      "-5.030559600000001\n",
      "[0.02609, 0.0131, 0.03176, -0.07746, -0.11117, -0.12085, 0.05895, -0.07831, -0.03769, 0.04555, 0.01146, -0.11033, 0.00221, -0.09308, 0.01938, -0.14498, -0.04121, -0.05236, -0.03998, -0.0305, -0.1294, -0.05607, -0.00181, -0.05894, -0.15024, 0.04497, 0.05852, 0.01369, -0.08362, -0.19231, -0.06616, -0.05097, -0.11423, -0.1536, 0.00216, -0.08688, 0.04282, 0.03141, -0.11786, -0.17777, -0.10787, 0.02051, -0.07457, 0.00553, -0.19021, -0.15546, -0.08672, -0.03905, 0.01993, -0.0255]\n",
      "-0.05218240000000002\n",
      "[0.15744, 0.14919, 0.22556, -0.2123, -0.33035, -0.54415, 0.55068, -0.42182, 0.06291, 0.23379, 0.1684, -0.56307, 0.13059, -0.44033, 0.20087, -0.39743, -0.15903, -0.56193, -0.10631, 0.06853, -0.12436, -0.27762, 0.0487, -0.10087, -0.75946, 0.29074, 0.38449, 0.15064, -0.42223, -0.83411, -0.04125, -0.33928, -0.7143, -0.17723, 0.01297, -0.08414, 0.27888, 0.19459, -0.3554, -0.4906, -0.3894, 0.13914, -0.09936, 0.16147, -0.68581, -0.45206, -0.31906, -0.06191, 0.18336, -0.05837]\n",
      "-0.13461199999999998\n",
      "[0.26061, -0.15687, 0.0836, -2.58431, -3.34941, -3.64935, -0.69921, -2.30569, -1.50915, 0.48404, -0.41677, -3.27141, -0.5032, -3.00176, -0.05355, -4.56826, -1.5501, -1.52802, -2.39437, -1.59886, -4.77247, -1.99363, -0.51387, -2.37549, -4.28426, 0.26671, 0.18069, -0.27085, -2.63278, -5.34151, -2.36195, -1.78758, -3.1977, -4.62148, -0.44373, -3.00359, 0.3503, 0.00465, -3.70152, -5.5754, -3.64056, 0.21327, -2.7726, -0.53736, -5.34631, -4.3145, -2.64112, -1.37871, 0.0642, -1.41036]\n",
      "-2.003031\n",
      "[0.29846, 0.3229, 0.32854, 0.46722, 0.39103, 0.51433, 0.5191, 0.38304, 0.31874, 0.31274, 0.33926, 0.45293, 0.31436, 0.51798, 0.30047, 0.50354, 0.37437, 0.38227, 0.62165, 0.39139, 0.59072, 0.46587, 0.38611, 0.44734, 0.53305, 0.34816, 0.3674, 0.30333, 0.37385, 0.56637, 0.41803, 0.36346, 0.45216, 0.43682, 0.27691, 0.47018, 0.32096, 0.32503, 0.49729, 0.64973, 0.62437, 0.28281, 0.43217, 0.32347, 0.53046, 0.45209, 0.40313, 0.32539, 0.30404, 0.37001]\n",
      "0.4139005999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [00:02, 704.05it/s]\n",
      "1563it [00:01, 1032.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -5.030559600000001, 'npmi_wiki': -0.05218240000000002, 'uci_wiki': -2.003031, 'CV_wiki': 0.4139005999999999, 'cp_wiki': -0.13461199999999998, 'sim_w2v': 0.18077759427683193, 'diversity': 0.5573333333333333, 'filename': 'results/240518_155135.txt', 'acc': 0.59208, 'macro-F1': 0.5894998760175307, 'Purity': 0.5518, 'NMI': 0.004335383614208191}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.94764 - dist: 0.22354 - cons: -0.03442\n",
      "Epoch-1 / recon: 7.91178 - dist: 0.21168 - cons: -0.03744\n",
      "Epoch-2 / recon: 7.89599 - dist: 0.20599 - cons: -0.03911\n",
      "Epoch-3 / recon: 7.88612 - dist: 0.20258 - cons: -0.04024\n",
      "Epoch-4 / recon: 7.87924 - dist: 0.20000 - cons: -0.04112\n",
      "Epoch-5 / recon: 7.87389 - dist: 0.19785 - cons: -0.04172\n",
      "Epoch-6 / recon: 7.86940 - dist: 0.19630 - cons: -0.04222\n",
      "Epoch-7 / recon: 7.86550 - dist: 0.19485 - cons: -0.04259\n",
      "Epoch-8 / recon: 7.86207 - dist: 0.19371 - cons: -0.04291\n",
      "Epoch-9 / recon: 7.85899 - dist: 0.19265 - cons: -0.04320\n",
      "Epoch-10 / recon: 7.85617 - dist: 0.19169 - cons: -0.04342\n",
      "Epoch-11 / recon: 7.85356 - dist: 0.19084 - cons: -0.04359\n",
      "Epoch-12 / recon: 7.85118 - dist: 0.19003 - cons: -0.04374\n",
      "Epoch-13 / recon: 7.84898 - dist: 0.18925 - cons: -0.04385\n",
      "Epoch-14 / recon: 7.84699 - dist: 0.18847 - cons: -0.04395\n",
      "Epoch-15 / recon: 7.84513 - dist: 0.18770 - cons: -0.04403\n",
      "Epoch-16 / recon: 7.84339 - dist: 0.18694 - cons: -0.04412\n",
      "Epoch-17 / recon: 7.84168 - dist: 0.18630 - cons: -0.04421\n",
      "Epoch-18 / recon: 7.84009 - dist: 0.18568 - cons: -0.04427\n",
      "Epoch-19 / recon: 7.83854 - dist: 0.18509 - cons: -0.04432\n",
      "Epoch-20 / recon: 7.83704 - dist: 0.18452 - cons: -0.04436\n",
      "Epoch-21 / recon: 7.83563 - dist: 0.18396 - cons: -0.04444\n",
      "Epoch-22 / recon: 7.83427 - dist: 0.18347 - cons: -0.04451\n",
      "Epoch-23 / recon: 7.83296 - dist: 0.18300 - cons: -0.04455\n",
      "Epoch-24 / recon: 7.83171 - dist: 0.18256 - cons: -0.04461\n",
      "Epoch-25 / recon: 7.83050 - dist: 0.18215 - cons: -0.04465\n",
      "Epoch-26 / recon: 7.82936 - dist: 0.18173 - cons: -0.04469\n",
      "Epoch-27 / recon: 7.82827 - dist: 0.18134 - cons: -0.04475\n",
      "Epoch-28 / recon: 7.82720 - dist: 0.18100 - cons: -0.04479\n",
      "Epoch-29 / recon: 7.82618 - dist: 0.18068 - cons: -0.04483\n",
      "Epoch-30 / recon: 7.82520 - dist: 0.18036 - cons: -0.04487\n",
      "Epoch-31 / recon: 7.82425 - dist: 0.18007 - cons: -0.04490\n",
      "Epoch-32 / recon: 7.82332 - dist: 0.17978 - cons: -0.04493\n",
      "Epoch-33 / recon: 7.82244 - dist: 0.17951 - cons: -0.04497\n",
      "Epoch-34 / recon: 7.82158 - dist: 0.17923 - cons: -0.04501\n",
      "Epoch-35 / recon: 7.82077 - dist: 0.17898 - cons: -0.04505\n",
      "Epoch-36 / recon: 7.82000 - dist: 0.17874 - cons: -0.04508\n",
      "Epoch-37 / recon: 7.81922 - dist: 0.17848 - cons: -0.04511\n",
      "Epoch-38 / recon: 7.81849 - dist: 0.17829 - cons: -0.04515\n",
      "Epoch-39 / recon: 7.81776 - dist: 0.17806 - cons: -0.04519\n",
      "Epoch-40 / recon: 7.81708 - dist: 0.17783 - cons: -0.04521\n",
      "Epoch-41 / recon: 7.81642 - dist: 0.17761 - cons: -0.04524\n",
      "Epoch-42 / recon: 7.81575 - dist: 0.17741 - cons: -0.04525\n",
      "Epoch-43 / recon: 7.81511 - dist: 0.17719 - cons: -0.04527\n",
      "Epoch-44 / recon: 7.81450 - dist: 0.17702 - cons: -0.04530\n",
      "Epoch-45 / recon: 7.81388 - dist: 0.17683 - cons: -0.04533\n",
      "Epoch-46 / recon: 7.81329 - dist: 0.17667 - cons: -0.04535\n",
      "Epoch-47 / recon: 7.81271 - dist: 0.17653 - cons: -0.04535\n",
      "Epoch-48 / recon: 7.81213 - dist: 0.17639 - cons: -0.04537\n",
      "Epoch-49 / recon: 7.81156 - dist: 0.17624 - cons: -0.04540\n",
      "------- Evaluation results -------\n",
      "topic-0 ['bad', 'like', 'hopelessly', 'horror', 'really', 'good', 'even', 'people', 'dont', 'wolfman', 'get', 'stormare', 'watch', 'make', 'awful']\n",
      "topic-1 ['zombie', 'vampire', 'batman', 'trek', 'robot', 'batwoman', 'sci', 'fi', 'bug', 'horror', 'scientist', 'spock', 'story', 'like', 'good']\n",
      "topic-2 ['ulagam', 'tollywood', 'gangster', 'good', 'crime', 'cop', 'performance', 'great', 'story', 'criminal', 'well', 'like', 'drug', 'action', 'character']\n",
      "topic-3 ['horror', 'like', 'good', 'music', 'really', 'character', 'gore', 'also', 'bad', 'dont', 'even', 'make', 'great', 'time', 'rock']\n",
      "topic-4 ['nazi', 'upshot', 'stamos', 'racist', 'skinhead', 'olsen', 'croat', 'adopt', 'racism', 'muslim', 'soviet', 'youngest', 'german', 'serb', 'gay']\n",
      "topic-5 ['haunted', 'horror', 'creepy', 'ghost', 'scary', 'scare', 'scared', 'atmospheric', 'deceased', 'gothic', 'haunting', 'castle', 'christian', 'chill', 'spooky']\n",
      "topic-6 ['zombie', 'trek', 'vampire', 'fi', 'sci', 'werewolf', 'batman', 'batwoman', 'horror', 'superman', 'like', 'animal', 'alien', 'cat', 'gore']\n",
      "topic-7 ['book', 'episode', 'good', 'supranatural', 'show', 'great', 'story', 'french', 'novel', 'season', 'read', 'series', 'italian', 'time', 'watch']\n",
      "topic-8 ['harilal', 'mahatma', 'gandhi', 'farrel', 'ciano', 'serb', 'akshaye', 'israel', 'biko', 'eldest', 'beatles', 'palestinian', 'mormon', 'venezuelan', 'bonhoeffer']\n",
      "topic-9 ['kiyoshi', 'blaisdell', 'dragstrip', 'comdey', 'murder', 'mother', 'horror', 'woman', 'maslin', 'lydon', 'killer', 'good', 'daughter', 'story', 'creepy']\n",
      "topic-10 ['sex', 'hee', 'fluffer', 'recomend', 'colt', 'analyzing', 'bullet', 'melon', 'bravo', 'ishtar', 'akshay', 'laughed', 'unfunny', 'sexual', 'unrecommended']\n",
      "topic-11 ['zombie', 'alien', 'batman', 'fi', 'sci', 'bug', 'like', 'superman', 'time', 'cartoon', 'animal', 'get', 'space', 'character', 'scene']\n",
      "topic-12 ['rna', 'it', 'blaisdell', 'ambushed', 'duvalls', 'zorak', 'comdey', 'organic', 'blimp', 'elm', 'hubba', 've', 'dragstrip', 'sniper', 'diagnosis']\n",
      "topic-13 ['chhaya', 'mahatma', 'inxs', 'gram', 'like', 'show', 'amitabh', 'song', 'eddie', 'gandhi', 'character', 'good', 'best', 'scene', 'time']\n",
      "topic-14 ['qestions', 'upto', 'manufacture', 'scary', 'good', 'really', 'horror', 'martial', 'like', 'time', 'great', 'kung', 'story', 'scared', 'watch']\n",
      "topic-15 ['croat', 'keystone', 'poitier', 'empower', 'abysmally', 'rosario', 'goldmine', 'strangely', 'bypassed', 'loni', 'memento', 'southern', 'arbuckle', 'chaplin', 'analyzing']\n",
      "topic-16 ['benoit', 'sammo', 'ppv', 'guerrero', 'cena', 'shaolin', 'franklin', 'mvp', 'defeated', 'batista', 'wwe', 'booker', 'undertaker', 'kung', 'gameplay']\n",
      "topic-17 ['mordrid', 'killer', 'gun', 'violence', 'character', 'like', 'well', 'scene', 'good', 'action', 'also', 'time', 'crime', 'killed', 'get']\n",
      "topic-18 ['zombie', 'trek', 'alien', 'like', 'see', 'character', 'time', 'voyager', 'batman', 'action', 'people', 'vampire', 'fi', 'sci', 'man']\n",
      "topic-19 ['zombie', 'batman', 'vampire', 'trek', 'alien', 'story', 'voyager', 'make', 'creature', 'robot', 'monster', 'character', 'space', 'like', 'stargate']\n",
      "topic-20 ['elson', 'episode', 'comedy', 'show', 'pollan', 'algy', 'jesss', 'family', 'harilal', 'series', 'brother', 'funny', 'like', 'smallville', 'tenny']\n",
      "topic-21 ['sinatra', 'astaire', 'kelly', 'dance', 'caron', 'midlers', 'gershwin', 'versatility', 'flashdance', 'jerry', 'leslie', 'midler', 'musical', 'dancing', 'gene']\n",
      "topic-22 ['trek', 'zombie', 'batman', 'vampire', 'werewolf', 'episode', 'alien', 'animal', 'monster', 'creature', 'spock', 'batwoman', 'bloodbath', 'superman', 'space']\n",
      "topic-23 ['mvp', 'unavailable', 'lengle', 'harilal', 'govinda', 'kenneth', 'compassionately', 'unreal', 'shikhar', 'lucifer', 'cinderella', 'softly', 'helena', 'mathis', 'shakespearean']\n",
      "topic-24 ['season', 'episode', 'shrug', 'dinosaur', 'series', 'trek', 'doo', 'bug', 'alien', 'scooby', 'animation', 'pilot', 'ducktales', 'stargate', 'rocked']\n",
      "topic-25 ['melon', 'aishwarya', 'amber', 'pornographic', 'sexploitation', 'eleniak', 'sexuality', 'rape', 'maruschka', 'erotic', 'gender', 'fluffer', 'typing', 'rebel', 'sexual']\n",
      "topic-26 ['keeped', 'unrecommended', 'lillard', 'lightest', 'coleen', 'buseys', 'eddie', 'stanly', 'circulated', 'absolutley', 'disney', 'bypassed', 'nooooo', 'yashraj', 'hve']\n",
      "topic-27 ['eleniak', 'pow', 'berenger', 'sniper', 'custer', 'mitchum', 'sgt', 'beckett', 'harilal', 'cadet', 'chewie', 'bradford', 'gillians', 'tierney', 'alvin']\n",
      "topic-28 ['disney', 'time', 'great', 'action', 'japanese', 'martial', 'scene', 'story', 'people', 'like', 'war', 'see', 'make', 'ducktales', 'danish']\n",
      "topic-29 ['kid', 'child', 'student', 'good', 'school', 'great', 'adult', 'time', 'wuhl', 'really', 'like', 'watch', 'little', 'teacher', 'seen']\n",
      "topic-30 ['music', 'great', 'bogdonavich', 'soundtrack', 'good', 'see', 'like', 'story', 'musical', 'dance', 'really', 'song', 'watch', 'acting', 'time']\n",
      "topic-31 ['aquafresh', 'qestions', 'rabins', 'good', 'upto', 'duvalls', 'glistening', 'really', 'dont', 'think', 'time', 'story', 'acting', 'actor', 'great']\n",
      "topic-32 ['trek', 'zombie', 'alien', 'time', 'fi', 'sci', 'character', 'space', 'scene', 'horror', 'like', 'story', 'really', 'animal', 'work']\n",
      "topic-33 ['ajay', 'bollywood', 'song', 'salvatores', 'bachchan', 'brock', 'soften', 'jesus', 'bypassed', 'amateurism', 'songwriter', 'miscast', 'dreadful', 'bankrupt', 'preacher']\n",
      "topic-34 ['zombie', 'scene', 'vampire', 'fi', 'sci', 'like', 'even', 'time', 'cat', 'man', 'much', 'well', 'character', 'people', 'look']\n",
      "topic-35 ['ul', 'virology', 'telecommunication', 'zombie', 'sliminess', 'schnaas', 'kiddy', 'killer', 'braille', 'software', 'bloodbath', 'slasher', 'riped', 'angkor', 'slug']\n",
      "topic-36 ['child', 'family', 'kid', 'doodlebops', 'show', 'would', 'parent', 'son', 'father', 'baby', 'story', 'mulroney', 'love', 'time', 'dermot']\n",
      "topic-37 ['zombie', 'like', 'trek', 'animal', 'dragon', 'make', 'time', 'character', 'romeros', 'even', 'bad', 'story', 'space', 'get', 'first']\n",
      "topic-38 ['woman', 'hagar', 'husband', 'love', 'war', 'mother', 'wife', 'shes', 'daughter', 'married', 'sex', 'family', 'look', 'scene', 'role']\n",
      "topic-39 ['algy', 'tenny', 'white', 'black', 'good', 'african', 'people', 'mvp', 'gay', 'car', 'bonhoeffer', 'scene', 'gillians', 'chibas', 'like']\n",
      "topic-40 ['book', 'french', 'novel', 'italian', 'hagar', 'character', 'adaptation', 'france', 'story', 'read', 'like', 'really', 'time', 'well', 'love']\n",
      "topic-41 ['horror', 'goldmine', 'jurassic', 'nuit', 'recomend', 'bravo', 'alien', 'rainy', 'hee', 'ha', 'analyzing', 'funniest', 'shes', 'malkovich', 'nooooo']\n",
      "topic-42 ['beatons', 'batman', 'hamish', 'dentist', 'superman', 'shakespeare', 'ohearn', 'edmunds', 'mozart', 'murder', 'crime', 'batwoman', 'collora', 'noir', 'slug']\n",
      "topic-43 ['brock', 'show', 'pokemon', 'funny', 'animation', 'cartoon', 'like', 'scooby', 'awsome', 'seen', 'see', 'animated', 'time', 'comedy', 'think']\n",
      "topic-44 ['zombie', 'batman', 'trek', 'vampire', 'story', 'space', 'like', 'fi', 'sci', 'monster', 'alien', 'character', 'animal', 'time', 'batwoman']\n",
      "topic-45 ['book', 'keystone', 'croat', 'novel', 'read', 'swordplay', 'loni', 'reading', 'adaptation', 'shrug', 'poitier', 'changed', 'unrecommended', 'bypassed', 'punch']\n",
      "topic-46 ['knife', 'nuit', 'began', 'band', 'halloween', 'noire', 'scarecrow', 'phone', 'pratfall', 'psycho', 'talk', 'metal', 'insect', 'scary', 'lisas']\n",
      "topic-47 ['guitar', 'hitler', 'geisha', 'japan', 'russia', 'sakura', 'china', 'treaty', 'russian', 'kershaw', 'carlyle', 'berkley', 'nudist', 'chinese', 'jannings']\n",
      "topic-48 ['zombie', 'batman', 'trek', 'animal', 'vampire', 'alien', 'superman', 'flesh', 'sci', 'fi', 'like', 'time', 'people', 'cat', 'good']\n",
      "topic-49 ['antz', 'homeward', 'fox', 'betty', 'disney', 'animation', 'bedknobs', 'broomstick', 'stallion', 'walt', 'dreamworks', 'animated', 'tolkien', 'pixar', 'worm']\n",
      "./\n",
      "[-3.24803, -3.04492, -2.74418, -1.80549, -5.48266, -5.85117, -3.63587, -3.78897, -13.14542, -4.63123, -6.08282, -2.11103, -11.70251, -4.63602, -2.84885, -6.9714, -6.44588, -2.74533, -2.78149, -2.26739, -6.86263, -5.77836, -3.90568, -8.44838, -6.06556, -11.67096, -4.30083, -10.34156, -2.71691, -4.40981, -2.62196, -3.77961, -1.68253, -10.1837, -1.59221, -9.55583, -4.60687, -3.51661, -2.07886, -5.95679, -1.57191, -7.93249, -8.29462, -5.3314, -3.13368, -5.35548, -9.14246, -7.91431, -2.21184, -7.38593]\n",
      "-5.2864086\n",
      "[-0.03403, -0.06686, -0.02713, -0.00186, -0.09463, -0.06379, -0.05881, -0.00841, -0.20202, -0.12341, -0.12035, -0.00471, -0.14444, -0.09132, -0.07196, -0.10728, -0.08444, -0.00296, -0.01857, 0.06918, -0.10766, -0.0527, -0.01015, -0.14542, -0.07013, -0.14451, -0.02463, -0.1731, -0.03649, -0.03889, 0.00169, -0.06778, 0.00523, -0.16251, -0.00297, -0.15808, -0.01211, -0.01047, -0.0201, -0.13519, 0.00776, -0.14151, -0.16077, -0.05875, -0.02693, -0.10239, -0.15198, -0.20978, -0.0389, -0.10633]\n",
      "-0.07226699999999998\n",
      "[-0.1796, 0.09656, 0.00308, 0.00227, -0.2018, 0.00166, 0.16607, -0.01474, -0.79665, -0.21308, -0.54944, 0.21046, -0.84823, -0.11484, -0.2775, -0.56332, -0.16295, 0.00951, 0.15766, 0.48728, -0.35471, -0.04469, 0.27011, -0.72055, 0.11291, -0.58581, -0.63461, -0.6545, 0.03009, -0.17553, -0.10499, -0.57416, 0.18709, -0.63072, 0.1334, -0.6817, 0.03615, 0.03702, 0.05119, -0.41877, 0.09992, -0.62924, -0.52327, -0.18419, 0.18948, -0.26306, -0.39809, -0.43551, 0.17823, -0.33439]\n",
      "-0.19621000000000002\n",
      "[-1.49842, -2.59444, -1.29247, -0.42739, -3.18912, -2.85099, -2.6503, -0.70105, -5.84051, -3.83756, -3.40107, -0.9517, -4.01001, -2.93575, -2.25575, -3.08564, -3.54615, -0.53534, -1.08256, 0.43649, -3.48212, -2.24637, -1.65596, -4.05408, -2.91737, -4.47509, -0.68138, -4.92175, -1.1929, -1.50707, -0.53055, -2.00529, -0.46021, -4.71273, -0.64272, -4.46688, -1.19322, -0.77347, -1.3173, -3.93406, -0.3374, -3.96602, -4.76294, -2.10619, -1.619, -3.13332, -4.50543, -6.19542, -1.66284, -3.72068]\n",
      "-2.5085898\n",
      "[0.32075, 0.44185, 0.40855, 0.27799, 0.51269, 0.51171, 0.46881, 0.38086, 0.63433, 0.42667, 0.43685, 0.37375, 0.53567, 0.4131, 0.34984, 0.46678, 0.63142, 0.34746, 0.3609, 0.37511, 0.48245, 0.46156, 0.49507, 0.51141, 0.46663, 0.58312, 0.28098, 0.55371, 0.32186, 0.42201, 0.37118, 0.38103, 0.3115, 0.57758, 0.30617, 0.5499, 0.39348, 0.3642, 0.37001, 0.48708, 0.31206, 0.46989, 0.4761, 0.4124, 0.39725, 0.42458, 0.5154, 0.54099, 0.37582, 0.55186]\n",
      "0.4368474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [00:02, 692.48it/s]\n",
      "1563it [00:01, 1010.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -5.2864086, 'npmi_wiki': -0.07226699999999998, 'uci_wiki': -2.5085898, 'CV_wiki': 0.4368474, 'cp_wiki': -0.19621000000000002, 'sim_w2v': 0.17091036006735336, 'diversity': 0.548, 'filename': 'results/240518_160627.txt', 'acc': 0.59392, 'macro-F1': 0.5928891536488884, 'Purity': 0.55084, 'NMI': 0.004484720275798487}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_classification, evaluate_clustering\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=384)\n",
    "    model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    testloader = DataLoader(testds2, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * ((1-org_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * ((1-pos_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic, pos_topic), dim=0), dirichlet_alpha_2)\n",
    "            \n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(15, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "    now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "                                  reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                                  filename=f'results/{now}.txt')\n",
    "    train_theta = []\n",
    "    test_theta = []\n",
    "    for batch_idx, batch in tqdm(enumerate(trainloader)):\n",
    "        org_input, _, org_bow, _ = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        train_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    train_theta = np.concatenate(train_theta, axis=0)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(testloader)): \n",
    "        org_input, org_bow = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        test_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    test_theta = np.concatenate(test_theta, axis=0)\n",
    "    \n",
    "    classification_res = evaluate_classification(train_theta, test_theta, textData.targets, textData.test_targets)\n",
    "    clustering_res = evaluate_clustering(test_theta, textData.test_targets)\n",
    "    \n",
    "    results.update(classification_res)\n",
    "    results.update(clustering_res)\n",
    "    \n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(textData.data) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_N  umass_wiki  npmi_wiki  uci_wiki   CV_wiki   cp_wiki   sim_w2v  \\\n",
      "0       50   -4.975957  -0.057117 -2.171019  0.423285 -0.122695  0.173568   \n",
      "1       50   -4.899737  -0.075008 -2.539678  0.423673 -0.200226  0.166481   \n",
      "2       50   -4.304151  -0.055045 -2.055559  0.410060 -0.105191  0.171532   \n",
      "3       50   -5.030560  -0.052182 -2.003031  0.413901 -0.134612  0.180778   \n",
      "4       50   -5.286409  -0.072267 -2.508590  0.436847 -0.196210  0.170910   \n",
      "\n",
      "   diversity                   filename      acc  macro-F1   Purity       NMI  \n",
      "0   0.542667  results/240518_150636.txt  0.58988  0.587797  0.55276  0.004511  \n",
      "1   0.566667  results/240518_152039.txt  0.59408  0.593739  0.55672  0.004768  \n",
      "2   0.525333  results/240518_153348.txt  0.58952  0.589148  0.55188  0.003798  \n",
      "3   0.557333  results/240518_155135.txt  0.59208  0.589500  0.55180  0.004335  \n",
      "4   0.548000  results/240518_160627.txt  0.59392  0.592889  0.55084  0.004485  \n",
      "mean\n",
      "topic_N       50.000000\n",
      "umass_wiki    -4.899363\n",
      "npmi_wiki     -0.062324\n",
      "uci_wiki      -2.255576\n",
      "CV_wiki        0.421553\n",
      "cp_wiki       -0.151787\n",
      "sim_w2v        0.172654\n",
      "diversity      0.548000\n",
      "acc            0.591896\n",
      "macro-F1       0.590615\n",
      "Purity         0.552800\n",
      "NMI            0.004379\n",
      "dtype: float64\n",
      "std\n",
      "topic_N       0.000000\n",
      "umass_wiki    0.363029\n",
      "npmi_wiki     0.010520\n",
      "uci_wiki      0.252817\n",
      "CV_wiki       0.010393\n",
      "cp_wiki       0.043681\n",
      "sim_w2v       0.005226\n",
      "diversity     0.015635\n",
      "acc           0.002157\n",
      "macro-F1      0.002563\n",
      "Purity        0.002294\n",
      "NMI           0.000360\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)\n",
    "print('mean')\n",
    "print(results_df.mean())\n",
    "print('std')\n",
    "print(results_df.std())\n",
    "\n",
    "if args.result_file is not None:\n",
    "    result_filename = f'results/{args.result_file}'\n",
    "else:\n",
    "    result_filename = f'results/{now}.tsv'\n",
    "\n",
    "results_df.to_csv(result_filename, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
