{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\utopic\\contrastive_topic_model\\data.py:223: DeprecationWarning: invalid escape sequence \\S\n",
      "  self.nonempty_text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in self.nonempty_text]\n",
      "f:\\utopic\\contrastive_topic_model\\data.py:226: DeprecationWarning: invalid escape sequence \\s\n",
      "  self.nonempty_text = [re.sub('\\s+', ' ', sent) for sent in self.nonempty_text]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setting Arguments\n",
    "args_text = '--base-model sentence-transformers/all-MiniLM-L6-v2 '+\\\n",
    "            '--dataset news --n-word 5000 --epochs-1 200 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 '+ \\\n",
    "            '--n-cluster 50 ' + \\\n",
    "            '--stage-1-ckpt trained_model/news_model_all-MiniLM-L6-v2_stage1_50t_5000w_199e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    parser.add_argument('--epochs-1', default=100, type=int,\n",
    "                        help='Number of training epochs for Stage 1')\n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--dataset', default='news', type=str,\n",
    "                        choices=['news', 'twitter', 'wiki', 'nips', 'stackoverflow', 'reuters', 'r52', 'imdb', 'agnews', 'yahoo'],\n",
    "                        help='Name of the dataset')\n",
    "    parser.add_argument('--n-cluster', default=50, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "    \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    "    \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str, default='./',\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "def data_load(dataset_name):\n",
    "    should_measure_hungarian = False\n",
    "    if dataset_name == 'news':\n",
    "        textData = newsData()\n",
    "        should_measure_hungarian = True\n",
    "    elif dataset_name == 'imdb':\n",
    "        textData = IMDBData()\n",
    "    elif dataset_name == 'agnews':\n",
    "        textData = AGNewsData()\n",
    "    elif dataset_name == 'yahoo':\n",
    "        textData = YahooData()\n",
    "    elif dataset_name == 'twitter':\n",
    "        textData = twitterData('/home/data/topicmodel/twitter_covid19.tsv')\n",
    "    elif dataset_name == 'wiki':\n",
    "        textData = wikiData('/home/data/topicmodel/smplAbstracts/')\n",
    "    elif dataset_name == 'nips':\n",
    "        textData = nipsData('/home/data/topicmodel/papers.csv')\n",
    "    elif dataset_name == 'stackoverflow':\n",
    "        textData = stackoverflowData('/home/data/topicmodel/stack_overflow.csv')\n",
    "    elif dataset_name == 'reuters':\n",
    "        textData = reutersData('/home/data/topicmodel/reuters-21578.txt')\n",
    "    elif dataset_name == 'r52':\n",
    "        textData = r52Data('/home/data/topicmodel/r52/')\n",
    "        should_measure_hungarian = True\n",
    "    return textData, should_measure_hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:05<00:00, 1910.66it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=textData.data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "basesim_path = f\"./save/{args.dataset}_{bert_name_short}_basesim_matrix_full.pkl\"\n",
    "if os.path.isfile(basesim_path) == False:\n",
    "    model = SentenceTransformer(bert_name.split('/')[-1], device='cuda')\n",
    "    base_result_list = []\n",
    "    for text in tqdm_notebook(trainds.nonempty_text):\n",
    "        base_result_list.append(model.encode(text))\n",
    "        \n",
    "    base_result_embedding = np.stack(base_result_list)\n",
    "    basereduced_norm = F.normalize(torch.tensor(base_result_embedding), dim=-1)\n",
    "    basesim_matrix = torch.mm(basereduced_norm, basereduced_norm.t())\n",
    "    ind = np.diag_indices(basesim_matrix.shape[0])\n",
    "    basesim_matrix[ind[0], ind[1]] = torch.ones(basesim_matrix.shape[0]) * -1\n",
    "    torch.save(basesim_matrix, basesim_path)\n",
    "else:\n",
    "    basesim_matrix = torch.load(basesim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Discovery neighborhood pairs and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hidden, alpha=1.0):\n",
    "    device = hidden.device\n",
    "    hidden_dim = hidden.shape[-1]\n",
    "    rand_w = torch.Tensor(np.eye(hidden_dim, dtype=np.float64)).to(device)\n",
    "    loss_dist_match = get_swd_loss(hidden, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t) ** 2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=768)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "    model.cuda(gpu_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = args.bsz = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    losses = AverageMeter()\n",
    "    closses = AverageMeter() \n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "    finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "    optimizer = RangerLars(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    global_step = 0\n",
    "    memory_queue = F.softmax(torch.randn(512, n_cluster).cuda(gpu_ids[0]), dim=1)\n",
    "    for epoch in range(epochs_1):\n",
    "        model.train()\n",
    "        #ema_model.train()\n",
    "        tbar = tqdm_notebook(trainloader)\n",
    "        for batch_idx, batch in enumerate(tbar):       \n",
    "            org_input, pos_input, _, _ = batch\n",
    "            org_input_ids = org_input['input_ids'].cuda(gpu_ids[0])\n",
    "            org_attention_mask = org_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            pos_input_ids = pos_input['input_ids'].cuda(gpu_ids[0])\n",
    "            pos_attention_mask = pos_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            all_input_ids = torch.cat((org_input_ids, pos_input_ids), dim=0)\n",
    "            all_attention_masks = torch.cat((org_attention_mask, pos_attention_mask), dim=0)\n",
    "            all_topics, _ = model(all_input_ids, all_attention_masks, return_topic=True)\n",
    "\n",
    "            orig_topic, pos_topic = torch.split(all_topics, len(all_topics) // 2)\n",
    "            pos_sim = torch.sum(orig_topic * pos_topic, dim=-1)\n",
    "\n",
    "            # consistency loss\n",
    "            consistency_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution matching loss\n",
    "            memory_queue = torch.cat((memory_queue.detach(), all_topics), dim=0)[all_topics.size(0):]\n",
    "            distmatch_loss = dist_match_loss(memory_queue, dirichlet_alpha_1)\n",
    "            loss = args.coeff_1_sim * consistency_loss + \\\n",
    "                   10 * distmatch_loss\n",
    "\n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(consistency_loss.item(), bsz)\n",
    "            dlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            tbar.set_description(\"Epoch-{} / consistency: {:.5f} - dist: {:.5f}\".format(epoch, \n",
    "                                                                                        closses.avg, \n",
    "                                                                                        dlosses.avg), refresh=True)\n",
    "            tbar.refresh()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            global_step += 1\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model_stage1_name = f'./trained_model/{args.dataset}_model_{bert_name_short}_stage1_{args.n_topic}t_{args.n_word}w_{epoch}e.ckpt'\n",
    "    torch.save(model.module.state_dict(), model_stage1_name)\n",
    "else:\n",
    "    model_stage1_name = args.stage_1_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: extract vocab set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del ema_model\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=768)\n",
    "model.cuda(gpu_ids[0])\n",
    "\n",
    "model.load_state_dict(torch.load(model_stage1_name), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004000663757324219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 45,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80074e54e13f49abbd72c484171e8fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "result_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm_notebook(memoryloader)):        \n",
    "        org_input, _, _, _ = batch\n",
    "        org_input_ids = org_input['input_ids'].to(gpu_ids[0])\n",
    "        org_attention_mask = org_input['attention_mask'].to(gpu_ids[0])\n",
    "        topic, embed = model(org_input_ids, org_attention_mask, return_topic = True)\n",
    "        result_list.append(topic)\n",
    "result_embedding = torch.cat(result_list)\n",
    "_, result_topic = torch.max(result_embedding, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': trainds.preprocess_ctm(trainds.nonempty_text), \n",
    "     'cluster_label': result_topic.cpu().numpy()}\n",
    "cluster_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_class = cluster_df.groupby(['cluster_label'], as_index=False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "supertxt = \"\"\n",
    "for idx, row in docs_per_class.iterrows():\n",
    "    supertxt = supertxt + row['text'] + \" \"\n",
    "wwwwwwwwwwwwww = supertxt.split()\n",
    "ssssssssssssss = set(wwwwwwwwwwwwww)\n",
    "print(len(ssssssssssssss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "count_vectorizer = CountVectorizer()\n",
    "ctfidf_vectorizer = CTFIDFVectorizer()\n",
    "count = count_vectorizer.fit_transform(docs_per_class.text)\n",
    "ctfidf = ctfidf_vectorizer.fit_transform(count, n_samples=len(cluster_df)).toarray()\n",
    "words = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transport to gensim\n",
    "(gensim_corpus, gensim_dict) = vect2gensim(count_vectorizer, count)\n",
    "vocab_list = set(gensim_dict.token2id.keys())\n",
    "stopwords = set(line.strip() for line in open('stopwords_en.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "resolution_score = (ctfidf - np.min(ctfidf, axis=1, keepdims=True)) / (np.max(ctfidf, axis=1, keepdims=True) - np.min(ctfidf, axis=1, keepdims=True))\n",
    "\n",
    "n_word = args.n_word\n",
    "# n_topic_word = n_word / len(docs_per_class.cluster_label.index)\n",
    "n_topic_word = n_word\n",
    "n_topic_word = 15\n",
    "\n",
    "topic_word_dict = {}\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    # topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    # topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    # topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['car', 'cars', 'dealer', 'saturn', 'toyota', 'price', 'lights', 'models', 'profit', 'engine', 'door', 'model', 'automotive', 'buying', 'dodge'],\n",
      "1: ['ticket', 'launch', 'lib', 'battery', 'tickets', 'pat', 'rockets', 'doug', 'flight', 'exploration', 'engines', 'cost', 'henry', 'batteries', 'lunar'],\n",
      "2: ['msg', 'food', 'sensitivity', 'chinese', 'superstition', 'foods', 'diet', 'honda', 'taste', 'effects', 'reaction', 'brain', 'eat', 'studies', 'circuit'],\n",
      "3: ['bus', 'ram', 'simms', 'motherboard', 'card', 'bit', 'memory', 'isa', 'cache', 'board', 'controller', 'mac', 'cards', 'eisa', 'simm'],\n",
      "4: ['jesus', 'god', 'bible', 'christ', 'christian', 'church', 'christians', 'faith', 'life', 'matthew', 'christianity', 'lord', 'gods', 'father', 'holy'],\n",
      "5: ['scsi', 'drive', 'ide', 'drives', 'controller', 'mac', 'tape', 'bus', 'hard', 'quadra', 'isa', 'rom', 'disk', 'devices', 'data'],\n",
      "6: ['gun', 'guns', 'firearms', 'handgun', 'amendment', 'control', 'nra', 'safety', 'firearm', 'weapons', 'arms', 'criminals', 'crime', 'carry', 'weapon'],\n",
      "7: ['israel', 'israeli', 'greek', 'jews', 'war', 'arab', 'lebanese', 'turkish', 'lebanon', 'greece', 'peace', 'arabs', 'greeks', 'turkey', 'turks'],\n",
      "8: ['israeli', 'israel', 'jews', 'arab', 'arabs', 'israelis', 'policy', 'palestine', 'jewish', 'israels', 'gaza', 'palestinian', 'jerusalem', 'adam', 'peace'],\n",
      "9: ['image', 'gif', 'files', 'photography', 'bmp', 'format', 'file', 'images', 'pictures', 'bitmap', 'convert', 'thanks', 'object', 'picture', 'program'],\n",
      "10: ['max', 'batf', 'fbi', 'waco', 'compound', 'fire', 'weapons', 'koresh', 'government', 'federal', 'clinton', 'children', 'law', 'gas', 'rights'],\n",
      "11: ['key', 'encryption', 'privacy', 'chip', 'clipper', 'des', 'security', 'keys', 'secure', 'escrow', 'anonymous', 'enforcement', 'nsa', 'cryptography', 'government'],\n",
      "12: ['diet', 'weight', 'order', 'tony', 'chuck', 'kent', 'van', 'day', 'fat', 'usa', 'valley', 'nntp', 'foods', 'host', 'disease'],\n",
      "13: ['phone', 'mail', 'internet', 'email', 'number', 'server', 'thanks', 'address', 'please', 'list', 'line', 'mailing', 'call', 'gateway', 'tcp'],\n",
      "14: ['moon', 'orbit', 'lunar', 'space', 'spacecraft', 'billion', 'earth', 'solar', 'funding', 'henry', 'mars', 'temporary', 'nasa', 'mission', 'spencer'],\n",
      "15: ['printer', 'color', 'image', 'print', 'printers', 'laser', 'printing', 'canon', 'scanner', 'graphics', 'display', 'processing', 'postscript', 'fonts', 'colors'],\n",
      "16: ['health', 'medical', 'patients', 'disease', 'care', 'insurance', 'pain', 'cancer', 'yeast', 'gordon', 'tax', 'aids', 'banks', 'national', 'treatment'],\n",
      "17: ['god', 'truth', 'atheists', 'atheism', 'believe', 'gods', 'belief', 'exist', 'existence', 'christians', 'faith', 'argument', 'absolute', 'religion', 'evidence'],\n",
      "18: ['banks', 'gordon', 'doctor', 'medical', 'patients', 'treatment', 'medicine', 'pittsburgh', 'disease', 'chastity', 'shameful', 'intellect', 'skepticism', 'patient', 'surrender'],\n",
      "19: ['graphics', 'jpeg', 'image', 'pub', 'font', 'available', 'ftp', 'images', 'version', 'server', 'format', 'package', 'gif', 'file', 'contrib'],\n",
      "20: ['mouse', 'keyboard', 'wire', 'wiring', 'ground', 'neutral', 'port', 'serial', 'circuit', 'wires', 'connected', 'irq', 'ports', 'electrical', 'use'],\n",
      "21: ['bike', 'helmet', 'bikes', 'dod', 'motorcycle', 'motorcycles', 'ride', 'honda', 'advice', 'buying', 'miles', 'ama', 'yamaha', 'seat', 'bmw'],\n",
      "22: ['windows', 'dos', 'instruction', 'files', 'memory', 'file', 'running', 'run', 'help', 'swap', 'version', 'using', 'microsoft', 'win', 'computer'],\n",
      "23: ['video', 'vram', 'color', 'monitor', 'card', 'quadra', 'centris', 'apple', 'colors', 'screen', 'bit', 'virtual', 'windows', 'linux', 'mac'],\n",
      "24: ['jesus', 'god', 'christian', 'christ', 'christians', 'church', 'sin', 'faith', 'bible', 'christianity', 'catholic', 'lord', 'kent', 'gods', 'malcolm'],\n",
      "25: ['water', 'drugs', 'science', 'nuclear', 'russia', 'drug', 'senior', 'administration', 'scientific', 'air', 'money', 'russian', 'official', 'president', 'theory'],\n",
      "26: ['morality', 'objective', 'keith', 'atheists', 'moral', 'schneider', 'allan', 'murder', 'livesey', 'tiff', 'values', 'political', 'jon', 'species', 'system'],\n",
      "27: ['hockey', 'nhl', 'team', 'abc', 'devils', 'rangers', 'play', 'dare', 'game', 'coverage', 'gary', 'teams', 'cup', 'gerald', 'islanders'],\n",
      "28: ['atf', 'survivors', 'dividian', 'ranch', 'witnesses', 'trial', 'burns', 'abortion', 'nazis', 'racism', 'genocide', 'freedom', 'death', 'fbi', 'fire'],\n",
      "29: ['drive', 'disk', 'drives', 'floppy', 'hard', 'bios', 'disks', 'controller', 'slave', 'master', 'jumper', 'boot', 'rom', 'feature', 'swap'],\n",
      "30: ['points', 'radius', 'shaft', 'plane', 'algorithm', 'circle', 'surface', 'piece', 'distance', 'point', 'center', 'thanks', 'double', 'nntp', 'university'],\n",
      "31: ['space', 'nasa', 'launch', 'satellite', 'shuttle', 'entry', 'orbit', 'station', 'earth', 'mission', 'spacecraft', 'mars', 'flight', 'satellites', 'lunar'],\n",
      "32: ['card', 'vga', 'drivers', 'monitor', 'ati', 'cards', 'video', 'mode', 'vesa', 'driver', 'diamond', 'graphics', 'svga', 'ultra', 'vlb'],\n",
      "33: ['armenian', 'armenians', 'turkish', 'gun', 'armenia', 'guns', 'serdar', 'argic', 'genocide', 'turks', 'turkey', 'azerbaijan', 'soviet', 'firearms', 'said'],\n",
      "34: ['bible', 'biblical', 'book', 'koresh', 'beast', 'books', 'christian', 'church', 'hebrew', 'word', 'god', 'evidence', 'sea', 'backing', 'testament'],\n",
      "35: ['amp', 'audio', 'sound', 'stereo', 'voltage', 'channel', 'output', 'receiver', 'circuit', 'power', 'speakers', 'signals', 'noise', 'input', 'sale'],\n",
      "36: ['car', 'cars', 'engine', 'speed', 'driving', 'tires', 'road', 'drivers', 'driver', 'wheel', 'rear', 'miles', 'brakes', 'fast', 'insurance'],\n",
      "37: ['window', 'manager', 'xterm', 'motif', 'application', 'menu', 'windows', 'xlib', 'icon', 'title', 'program', 'display', 'set', 'running', 'key'],\n",
      "38: ['apple', 'modem', 'mac', 'duo', 'fpu', 'serial', 'powerbook', 'portable', 'fax', 'modems', 'internal', 'software', 'baud', 'buy', 'port'],\n",
      "39: ['dod', 'bike', 'riding', 'ride', 'rider', 'dog', 'motorcycle', 'bikes', 'riders', 'appears', 'annual', 'art', 'lock', 'cover', 'road'],\n",
      "40: ['radar', 'detector', 'oil', 'car', 'detectors', 'sale', 'cds', 'engine', 'power', 'music', 'radio', 'shipping', 'picture', 'price', 'cars'],\n",
      "41: ['baseball', 'cubs', 'phillies', 'jewish', 'braves', 'team', 'players', 'mets', 'suck', 'season', 'game', 'games', 'philadelphia', 'sox', 'yankees'],\n",
      "42: ['gay', 'islamic', 'islam', 'homosexual', 'sex', 'homosexuality', 'men', 'cramer', 'sexual', 'rushdie', 'homosexuals', 'clayton', 'gregg', 'muslim', 'muslims'],\n",
      "43: ['year', 'runs', 'pitching', 'hit', 'players', 'game', 'hitter', 'team', 'baseball', 'braves', 'morris', 'last', 'league', 'games', 'better'],\n",
      "44: ['output', 'president', 'jobs', 'government', 'file', 'stream', 'entry', 'going', 'program', 'clinton', 'check', 'secretary', 'build', 'senate', 'house'],\n",
      "45: ['widget', 'screen', 'monitor', 'windows', 'display', 'window', 'widgets', 'application', 'motif', 'server', 'monitors', 'resource', 'visual', 'set', 'using'],\n",
      "46: ['team', 'hockey', 'season', 'nhl', 'play', 'game', 'period', 'players', 'flyers', 'teams', 'league', 'games', 'player', 'goal', 'puck'],\n",
      "47: ['game', 'games', 'detroit', 'leafs', 'espn', 'wings', 'win', 'toronto', 'team', 'hawks', 'playoff', 'scores', 'baseball', 'cup', 'fans'],\n",
      "48: ['bmw', 'traffic', 'front', 'battery', 'houston', 'dod', 'new', 'tire', 'nntp', 'bike', 'speed', 'host', 'doug', 'street', 'distribution'],\n",
      "49: ['clipper', 'chip', 'key', 'encryption', 'escrow', 'secret', 'crypto', 'keys', 'pgp', 'algorithm', 'announcement', 'chips', 'security', 'phones', 'phone'],\n"
     ]
    }
   ],
   "source": [
    "for key in topic_word_dict:\n",
    "    print(f\"{key}: {topic_word_dict[key]},\")\n",
    "topic_words_list = list(topic_word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16978964115255435\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "\n",
    "class WhiteSpacePreprocessing():\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
    "                      \"Use WhiteSpacePreprocessingStopwords.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "    \n",
    "def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):  \n",
    "    num_k = class_num\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "  \n",
    "    for c1 in range(0, num_k):\n",
    "        for c2 in range(0, num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "  \n",
    "    match = linear_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "  \n",
    "    return res\n",
    "\n",
    "def get_document_topic(topic_words, preprocessed_documents_lemmatized):\n",
    "    topic_words_flatten = list(itertools.chain.from_iterable(topic_words))\n",
    "    if '' in topic_words_flatten:\n",
    "        topic_words_flatten.remove('')\n",
    "    topic_words_flatten = list(set(topic_words_flatten))\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary = topic_words_flatten)\n",
    "    vectorizer = vectorizer.fit(preprocessed_documents_lemmatized)\n",
    "    count_mat = vectorizer.transform(preprocessed_documents_lemmatized).toarray()\n",
    "    \n",
    "    count_mat_normalized = count_mat + 1e-4\n",
    "    count_mat_normalized = count_mat_normalized / count_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat = vectorizer.transform([' '.join(i) for i in topic_words]).toarray()\n",
    "    topic_mat_normalized = topic_mat + 1e-4\n",
    "    topic_mat_normalized = topic_mat_normalized / topic_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat_inverse = topic_mat_normalized @ topic_mat_normalized.transpose()\n",
    "    topic_mat_inverse = np.linalg.inv(topic_mat_inverse)\n",
    "    topic_mat_inverse = topic_mat_normalized.transpose() @ topic_mat_inverse\n",
    "    document_topic = count_mat_normalized @ topic_mat_inverse\n",
    "    return document_topic\n",
    "\n",
    "class TopicModelDataPreparationNoNumber(TopicModelDataPreparation):\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, wordlist=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b', vocabulary=wordlist)\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(train_contextualized_embeddings, train_bow_embeddings, self.id2token, encoded_labels)\n",
    "    \n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "qt = TopicModelDataPreparationNoNumber(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sp = WhiteSpacePreprocessing(textData.data, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_documents_lemmatized = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()]) for doc in preprocessed_documents]\n",
    "\n",
    "document_topic = get_document_topic(topic_words_list, preprocessed_documents_lemmatized)\n",
    "train_target_filtered = textData.targets.squeeze()[retained_indices]\n",
    "flat_predict = torch.tensor(np.argmax(document_topic, axis=1))\n",
    "flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "num_samples = flat_predict.shape[0]\n",
    "match = _hungarian_match(flat_predict, flat_target, num_samples, 20)    \n",
    "reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "for pred_i, target_i in match:\n",
    "    reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19309600164021448"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(reordered_preds, flat_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "[-1.7659, -2.75958, -5.73115, -3.87078, -1.51117, -2.65345, -3.40299, -3.64834, -3.5648, -6.31774, -2.86435, -4.14767, -5.16925, -3.03433, -2.76406, -4.74539, -3.82516, -1.5307, -4.25273, -5.6412, -4.97686, -3.95081, -3.79115, -5.95325, -2.05163, -1.90816, -3.0101, -2.22296, -2.23495, -3.61482, -5.7169, -2.20765, -4.27635, -8.62208, -5.53544, -1.84872, -3.67992, -5.81377, -4.49597, -2.59091, -3.00314, -5.36454, -3.68631, -4.5383, -3.13057, -2.02037, -3.83416, -5.56778, -4.50328, -6.3241]\n",
      "-3.8735137999999996\n",
      "[0.02447, -0.096, -0.09703, -0.00554, 0.14874, 0.03883, 0.02828, 0.08621, 0.09544, -0.01558, -0.05051, -0.03389, -0.0938, 0.04603, 0.05896, -0.03482, -0.0055, 0.10847, -0.11768, -0.01343, -0.05341, -0.05391, -0.06132, -0.06, 0.10642, 0.00153, -0.1337, 0.02273, -0.05109, -0.0062, -0.04994, 0.16947, 0.00989, -0.01169, -0.03567, 0.08334, 0.02871, -0.06607, -0.04349, -0.04062, -0.06495, -0.0105, -0.07653, -0.04343, -0.06654, 0.01004, 0.09758, 0.00865, -0.08983, -0.08316]\n",
      "-0.0098408\n",
      "[0.20939, -0.29247, -0.17461, 0.38795, 0.5405, 0.27657, 0.12121, 0.34194, 0.34175, -0.05578, -0.08022, 0.22042, -0.12802, 0.27656, 0.33189, 0.04608, -0.01642, 0.24382, -0.2746, -0.00898, -0.12663, -0.00603, -0.00398, 0.01458, 0.4695, -0.06536, -0.35588, 0.14058, -0.27386, 0.05307, -0.1288, 0.5573, 0.0289, -0.23175, 0.01263, 0.34338, 0.11407, -0.10852, 0.11238, -0.12865, -0.08398, 0.10126, -0.08698, -0.11111, -0.02791, 0.08599, 0.44452, 0.12144, -0.09461, -0.10487]\n",
      "0.05935320000000001\n",
      "[-0.27856, -2.82888, -3.16977, -1.30406, 1.62228, -0.26098, -0.37971, 0.54599, 0.02747, -1.47331, -1.70215, -2.09673, -2.55453, -0.01231, -0.35512, -1.96395, -1.22614, 1.33546, -3.83852, -1.57242, -2.14748, -2.76955, -2.15202, -2.46521, 0.6636, -0.04786, -3.70458, -0.39661, -1.73271, -1.3645, -1.75256, 1.92903, -0.73284, -1.67958, -2.09698, 0.80687, -0.25533, -2.40473, -2.25131, -1.72933, -2.15314, -1.9155, -3.14512, -2.02284, -2.09902, -0.61723, 0.19854, -1.09222, -2.6512, -3.29024]\n",
      "-1.3311518000000004\n",
      "[0.32001, 0.32869, 0.42837, 0.412, 0.46935, 0.39858, 0.38309, 0.47833, 0.63953, 0.46734, 0.36674, 0.44598, 0.38223, 0.38223, 0.47325, 0.46016, 0.4404, 0.34332, 0.45153, 0.442, 0.44395, 0.52251, 0.40522, 0.48293, 0.47392, 0.28551, 0.39728, 0.36536, 0.33569, 0.42388, 0.42612, 0.48966, 0.44644, 0.51669, 0.50285, 0.3487, 0.35098, 0.44903, 0.49692, 0.42392, 0.35378, 0.5588, 0.54576, 0.45338, 0.36927, 0.33045, 0.54025, 0.49271, 0.38744, 0.53257]\n",
      "0.4333020000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic_N': 50,\n",
       " 'umass_wiki': -3.8735137999999996,\n",
       " 'npmi_wiki': -0.0098408,\n",
       " 'uci_wiki': -1.3311518000000004,\n",
       " 'CV_wiki': 0.4333020000000001,\n",
       " 'cp_wiki': 0.05935320000000001,\n",
       " 'sim_w2v': 0.18555901170050965,\n",
       " 'diversity': 0.7453333333333333,\n",
       " 'filename': 'results/240517_152354.txt'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "results = get_topic_qualities(topic_words_list, args.palmetto_dir, reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                              filename=f'results/{now}.txt')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "\n",
    "n_word = args.n_word\n",
    "n_topic_word = n_word\n",
    "\n",
    "words_to_idx = {k: v for v, k in enumerate(words)}\n",
    "topic_word_dict = {}\n",
    "topic_score_dict = {}\n",
    "total_score_cat = []\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    total_score_cat.append(total_score)\n",
    "    # topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    # topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]\n",
    "    topic_score_dict[docs_per_class.cluster_label.iloc[label]] = [total_score[words_to_idx[top_word]] for top_word in topic_word_list[:int(n_topic_word)]]\n",
    "total_score_cat = np.stack(total_score_cat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "topic_word_set = list(itertools.chain.from_iterable(pd.DataFrame.from_dict(topic_word_dict).values))\n",
    "word_candidates = remove_dup(topic_word_set)[:n_word]\n",
    "n_word = len(word_candidates)\n",
    "n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('our_word_candidates_10000.pkl', 'wb') as f:\n",
    "    pickle.dump(word_candidates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_candidates = {}\n",
    "for candidate in word_candidates:\n",
    "    weight_candidates[candidate] = [total_score_cat[label, words_to_idx[candidate]] for label in range(n_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cand_to_idx = {k: v for v, k in enumerate(list(weight_candidates.keys()))}\n",
    "weight_cand_matrix = np.array(list(weight_candidates.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, basesim_matrix, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        sim_weight, sim_indices = basesim_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2TestDataset(Dataset):\n",
    "    def __init__(self, encoder, ds, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embedding_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:04<00:00, 2738.14it/s]\n",
      "100%|██████████| 11314/11314 [00:56<00:00, 201.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, basesim_matrix, word_candidates, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_cands = torch.tensor(weight_cand_matrix.max(axis=1)).cuda(gpu_ids[0]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7532/7532 [00:03<00:00, 2013.88it/s]\n",
      "100%|██████████| 7532/7532 [00:02<00:00, 2771.11it/s]\n",
      "100%|██████████| 7532/7532 [00:37<00:00, 200.86it/s]\n"
     ]
    }
   ],
   "source": [
    "testds = BertDataset(bert=bert_name, text_list=textData.test_data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds2 = Stage2TestDataset(model.encoder, testds, word_candidates, lemmatize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.54389 - dist: 0.07809 - cons: -0.27672\n",
      "Epoch-1 / recon: 2.43787 - dist: 0.05814 - cons: -0.32910\n",
      "Epoch-2 / recon: 2.38195 - dist: 0.05084 - cons: -0.35136\n",
      "Epoch-3 / recon: 2.34941 - dist: 0.04676 - cons: -0.36403\n",
      "Epoch-4 / recon: 2.32836 - dist: 0.04424 - cons: -0.37254\n",
      "Epoch-5 / recon: 2.31367 - dist: 0.04250 - cons: -0.37886\n",
      "Epoch-6 / recon: 2.30267 - dist: 0.04112 - cons: -0.38391\n",
      "Epoch-7 / recon: 2.29419 - dist: 0.04006 - cons: -0.38790\n",
      "Epoch-8 / recon: 2.28749 - dist: 0.03916 - cons: -0.39119\n",
      "Epoch-9 / recon: 2.28200 - dist: 0.03846 - cons: -0.39398\n",
      "Epoch-10 / recon: 2.27746 - dist: 0.03788 - cons: -0.39632\n",
      "Epoch-11 / recon: 2.27360 - dist: 0.03738 - cons: -0.39856\n",
      "Epoch-12 / recon: 2.27022 - dist: 0.03693 - cons: -0.40075\n",
      "Epoch-13 / recon: 2.26728 - dist: 0.03655 - cons: -0.40279\n",
      "Epoch-14 / recon: 2.26471 - dist: 0.03620 - cons: -0.40447\n",
      "Epoch-15 / recon: 2.26245 - dist: 0.03586 - cons: -0.40611\n",
      "Epoch-16 / recon: 2.26042 - dist: 0.03554 - cons: -0.40757\n",
      "Epoch-17 / recon: 2.25858 - dist: 0.03526 - cons: -0.40888\n",
      "Epoch-18 / recon: 2.25688 - dist: 0.03502 - cons: -0.41014\n",
      "Epoch-19 / recon: 2.25534 - dist: 0.03480 - cons: -0.41131\n",
      "Epoch-20 / recon: 2.25394 - dist: 0.03461 - cons: -0.41241\n",
      "Epoch-21 / recon: 2.25268 - dist: 0.03443 - cons: -0.41348\n",
      "Epoch-22 / recon: 2.25151 - dist: 0.03429 - cons: -0.41443\n",
      "Epoch-23 / recon: 2.25044 - dist: 0.03412 - cons: -0.41532\n",
      "Epoch-24 / recon: 2.24944 - dist: 0.03398 - cons: -0.41620\n",
      "Epoch-25 / recon: 2.24853 - dist: 0.03384 - cons: -0.41701\n",
      "Epoch-26 / recon: 2.24769 - dist: 0.03373 - cons: -0.41767\n",
      "Epoch-27 / recon: 2.24690 - dist: 0.03365 - cons: -0.41840\n",
      "Epoch-28 / recon: 2.24615 - dist: 0.03353 - cons: -0.41904\n",
      "Epoch-29 / recon: 2.24547 - dist: 0.03343 - cons: -0.41966\n",
      "Epoch-30 / recon: 2.24480 - dist: 0.03333 - cons: -0.42026\n",
      "Epoch-31 / recon: 2.24417 - dist: 0.03324 - cons: -0.42083\n",
      "Epoch-32 / recon: 2.24357 - dist: 0.03316 - cons: -0.42143\n",
      "Epoch-33 / recon: 2.24301 - dist: 0.03310 - cons: -0.42201\n",
      "Epoch-34 / recon: 2.24249 - dist: 0.03303 - cons: -0.42250\n",
      "Epoch-35 / recon: 2.24197 - dist: 0.03296 - cons: -0.42301\n",
      "Epoch-36 / recon: 2.24150 - dist: 0.03289 - cons: -0.42350\n",
      "Epoch-37 / recon: 2.24105 - dist: 0.03284 - cons: -0.42392\n",
      "Epoch-38 / recon: 2.24061 - dist: 0.03278 - cons: -0.42438\n",
      "Epoch-39 / recon: 2.24021 - dist: 0.03273 - cons: -0.42477\n",
      "Epoch-40 / recon: 2.23981 - dist: 0.03269 - cons: -0.42518\n",
      "Epoch-41 / recon: 2.23944 - dist: 0.03262 - cons: -0.42555\n",
      "Epoch-42 / recon: 2.23908 - dist: 0.03258 - cons: -0.42589\n",
      "Epoch-43 / recon: 2.23873 - dist: 0.03253 - cons: -0.42626\n",
      "Epoch-44 / recon: 2.23840 - dist: 0.03249 - cons: -0.42659\n",
      "Epoch-45 / recon: 2.23808 - dist: 0.03247 - cons: -0.42696\n",
      "Epoch-46 / recon: 2.23778 - dist: 0.03243 - cons: -0.42726\n",
      "Epoch-47 / recon: 2.23749 - dist: 0.03239 - cons: -0.42753\n",
      "Epoch-48 / recon: 2.23720 - dist: 0.03235 - cons: -0.42786\n",
      "Epoch-49 / recon: 2.23691 - dist: 0.03232 - cons: -0.42817\n",
      "------- Evaluation results -------\n",
      "topic-0 ['subject', 'gordon', 'bank', 'doctor', 'patient', 'article', 'disease', 'organization', 'line', 'writes', 'medical', 'pain', 'pittsburgh', 'health', 'chastity']\n",
      "topic-1 ['subject', 'writes', 'organization', 'line', 'article', 'atf', 'people', 'death', 'one', 'university', 'god', 'posting', 'racism', 'dividian', 'right']\n",
      "topic-2 ['subject', 'window', 'graphic', 'sale', 'software', 'disk', 'file', 'organization', 'line', 'mouse', 'port', 'program', 'mail', 'package', 'drive']\n",
      "topic-3 ['subject', 'line', 'organization', 'nntp', 'mail', 'host', 'posting', 'internet', 'file', 'phone', 'thanks', 'please', 'address', 'ftp', 'number']\n",
      "topic-4 ['morality', 'homosexual', 'atheist', 'objective', 'writes', 'islam', 'gay', 'keith', 'moral', 'people', 'article', 'islamic', 'line', 'organization', 'god']\n",
      "topic-5 ['max', 'subject', 'batf', 'line', 'organization', 'writes', 'fbi', 'people', 'waco', 'article', 'weapon', 'law', 'right', 'clinton', 'fire']\n",
      "topic-6 ['god', 'christian', 'jesus', 'bible', 'church', 'line', 'christ', 'organization', 'writes', 'one', 'article', 'book', 'sin', 'christianity', 'people']\n",
      "topic-7 ['space', 'moon', 'orbit', 'line', 'organization', 'nasa', 'spacecraft', 'article', 'satellite', 'writes', 'year', 'shuttle', 'earth', 'billion', 'lunar']\n",
      "topic-8 ['subject', 'line', 'organization', 'ticket', 'battery', 'writes', 'posting', 'university', 'host', 'nntp', 'article', 'launch', 'one', 'pat', 'lib']\n",
      "topic-9 ['subject', 'organization', 'line', 'posting', 'one', 'host', 'nntp', 'university', 'know', 'writes', 'article', 'like', 'use', 'mouse', 'thanks']\n",
      "topic-10 ['disk', 'monitor', 'ram', 'video', 'drive', 'sale', 'floppy', 'shipping', 'motherboard', 'card', 'software', 'meg', 'color', 'window', 'team']\n",
      "topic-11 ['israeli', 'israel', 'arab', 'jew', 'palestinian', 'greek', 'writes', 'turkish', 'article', 'policy', 'jewish', 'people', 'lebanese', 'peace', 'organization']\n",
      "topic-12 ['one', 'line', 'used', 'organization', 'use', 'writes', 'version', 'much', 'work', 'university', 'know', 'like', 'make', 'using', 'anyone']\n",
      "topic-13 ['subject', 'window', 'disk', 'software', 'graphic', 'line', 'port', 'mouse', 'sale', 'organization', 'modem', 'package', 'serial', 'card', 'program']\n",
      "topic-14 ['subject', 'line', 'organization', 'one', 'posting', 'university', 'host', 'nntp', 'use', 'writes', 'know', 'article', 'mouse', 'like', 'anyone']\n",
      "topic-15 ['organization', 'line', 'one', 'subject', 'posting', 'writes', 'university', 'article', 'know', 'host', 'nntp', 'like', 'make', 'anyone', 'use']\n",
      "topic-16 ['graphic', 'subject', 'window', 'monitor', 'file', 'team', 'color', 'disk', 'package', 'software', 'program', 'player', 'sale', 'shipping', 'game']\n",
      "topic-17 ['organization', 'subject', 'line', 'posting', 'university', 'one', 'nntp', 'host', 'image', 'article', 'writes', 'know', 'mouse', 'thanks', 'use']\n",
      "topic-18 ['monitor', 'widget', 'screen', 'line', 'organization', 'window', 'problem', 'display', 'motif', 'university', 'host', 'nntp', 'posting', 'computer', 'get']\n",
      "topic-19 ['card', 'video', 'driver', 'vga', 'line', 'organization', 'monitor', 'window', 'color', 'graphic', 'mode', 'vram', 'thanks', 'university', 'posting']\n",
      "topic-20 ['bike', 'subject', 'dod', 'motorcycle', 'article', 'line', 'writes', 'organization', 'riding', 'dog', 'rider', 'ride', 'posting', 'helmet', 'one']\n",
      "topic-21 ['mac', 'card', 'line', 'modem', 'organization', 'apple', 'bus', 'simms', 'slot', 'university', 'motherboard', 'ram', 'isa', 'article', 'one']\n",
      "topic-22 ['disk', 'monitor', 'subject', 'sale', 'ram', 'video', 'window', 'shipping', 'drive', 'software', 'floppy', 'card', 'motherboard', 'modem', 'manual']\n",
      "topic-23 ['subject', 'line', 'organization', 'university', 'point', 'article', 'posting', 'nntp', 'host', 'thanks', 'writes', 'valley', 'algorithm', 'chuck', 'order']\n",
      "topic-24 ['disk', 'monitor', 'sale', 'video', 'ram', 'drive', 'subject', 'shipping', 'card', 'floppy', 'motherboard', 'window', 'manual', 'software', 'team']\n",
      "topic-25 ['subject', 'line', 'organization', 'posting', 'nntp', 'host', 'university', 'graphic', 'mouse', 'image', 'thanks', 'one', 'use', 'port', 'know']\n",
      "topic-26 ['disk', 'sale', 'monitor', 'window', 'subject', 'shipping', 'video', 'ram', 'drive', 'team', 'software', 'floppy', 'car', 'graphic', 'card']\n",
      "topic-27 ['window', 'line', 'organization', 'file', 'manager', 'program', 'use', 'application', 'icon', 'host', 'run', 'university', 'writes', 'posting', 'running']\n",
      "topic-28 ['gun', 'armenian', 'firearm', 'subject', 'turkish', 'people', 'weapon', 'armenia', 'argic', 'handgun', 'serdar', 'criminal', 'right', 'crime', 'genocide']\n",
      "topic-29 ['team', 'hockey', 'nhl', 'player', 'game', 'play', 'goal', 'season', 'line', 'period', 'organization', 'devil', 'playoff', 'traded', 'captain']\n",
      "topic-30 ['subject', 'graphic', 'line', 'organization', 'file', 'posting', 'nntp', 'host', 'mouse', 'window', 'program', 'thanks', 'port', 'university', 'package']\n",
      "topic-31 ['game', 'subject', 'line', 'organization', 'team', 'university', 'win', 'playoff', 'writes', 'detroit', 'posting', 'wing', 'host', 'article', 'nntp']\n",
      "topic-32 ['car', 'subject', 'article', 'line', 'writes', 'organization', 'driver', 'saturn', 'dealer', 'speed', 'engine', 'one', 'toyota', 'price', 'get']\n",
      "topic-33 ['key', 'chip', 'clipper', 'encryption', 'line', 'organization', 'escrow', 'secret', 'system', 'writes', 'crypto', 'posting', 'algorithm', 'host', 'government']\n",
      "topic-34 ['baseball', 'game', 'team', 'player', 'line', 'organization', 'year', 'jewish', 'writes', 'article', 'phillies', 'season', 'pitcher', 'run', 'pitching']\n",
      "topic-35 ['line', 'subject', 'organization', 'one', 'posting', 'university', 'host', 'nntp', 'use', 'writes', 'know', 'image', 'mouse', 'article', 'like']\n",
      "topic-36 ['subject', 'organization', 'line', 'bmw', 'posting', 'nntp', 'host', 'sale', 'new', 'university', 'article', 'traffic', 'distribution', 'tire', 'like']\n",
      "topic-37 ['subject', 'line', 'organization', 'radar', 'detector', 'car', 'oil', 'university', 'power', 'amp', 'sale', 'audio', 'sound', 'nntp', 'host']\n",
      "topic-38 ['subject', 'msg', 'food', 'organization', 'line', 'article', 'writes', 'drug', 'air', 'one', 'water', 'nuclear', 'know', 'circuit', 'sensitivity']\n",
      "topic-39 ['subject', 'printer', 'font', 'line', 'color', 'organization', 'window', 'image', 'gif', 'print', 'thanks', 'file', 'graphic', 'posting', 'host']\n",
      "topic-40 ['drive', 'scsi', 'disk', 'ide', 'floppy', 'hard', 'organization', 'controller', 'line', 'mac', 'problem', 'boot', 'system', 'file', 'host']\n",
      "topic-41 ['disk', 'subject', 'monitor', 'sale', 'shipping', 'team', 'window', 'car', 'video', 'hockey', 'player', 'game', 'ram', 'software', 'drive']\n",
      "topic-42 ['organization', 'subject', 'line', 'posting', 'host', 'nntp', 'one', 'university', 'writes', 'know', 'article', 'mouse', 'thanks', 'like', 'anyone']\n",
      "topic-43 ['line', 'one', 'organization', 'use', 'university', 'writes', 'work', 'version', 'posting', 'host', 'like', 'mouse', 'nntp', 'used', 'know']\n",
      "topic-44 ['subject', 'line', 'organization', 'mouse', 'one', 'university', 'modem', 'posting', 'port', 'nntp', 'image', 'host', 'know', 'article', 'writes']\n",
      "topic-45 ['line', 'organization', 'subject', 'one', 'posting', 'university', 'host', 'nntp', 'use', 'writes', 'know', 'article', 'mouse', 'anyone', 'like']\n",
      "topic-46 ['god', 'christian', 'jesus', 'people', 'faith', 'one', 'believe', 'belief', 'church', 'bible', 'say', 'truth', 'life', 'atheist', 'christianity']\n",
      "topic-47 ['honda', 'bike', 'motorcycle', 'yamaha', 'subject', 'phillies', 'baseball', 'helmet', 'riding', 'pitcher', 'espn', 'radar', 'mouse', 'game', 'hitter']\n",
      "topic-48 ['key', 'government', 'chip', 'encryption', 'clipper', 'nsa', 'one', 'bit', 'escrow', 'people', 'wiretap', 'system', 'crypto', 'tapped', 'secure']\n",
      "topic-49 ['subject', 'disk', 'sale', 'card', 'monitor', 'shipping', 'drive', 'software', 'video', 'ram', 'window', 'manual', 'modem', 'port', 'chip']\n",
      "./\n",
      "[-2.61054, -2.21306, -2.43325, -3.49771, -1.78725, -2.40986, -1.58275, -2.23656, -3.83185, -2.66244, -3.46574, -1.76825, -1.92158, -2.70769, -2.74991, -2.3811, -2.72609, -2.66878, -3.65223, -4.06862, -3.47674, -3.07626, -3.86055, -3.56976, -3.2709, -2.71613, -2.97974, -3.72006, -5.92703, -2.63968, -3.36814, -3.93382, -2.24042, -3.30261, -2.37171, -2.4745, -3.3481, -4.30567, -2.12165, -3.41787, -2.12667, -2.66927, -2.87575, -4.74052, -3.06706, -2.73908, -2.12766, -5.55809, -6.3711, -3.22916]\n",
      "-3.1000192\n",
      "[-0.03544, -0.06268, 0.03638, -0.05022, 0.01192, -0.04638, 0.06237, 0.0893, -0.11212, -0.07542, -0.03216, 0.10084, -0.05917, 0.0428, -0.06853, -0.06481, -0.01108, -0.07762, -0.08664, -0.04101, -0.07218, -0.02203, 0.02602, -0.09379, -0.00573, -0.08404, -0.00373, -0.06698, -0.02457, 0.05958, -0.08561, -0.08054, 7e-05, -0.03596, 0.02808, -0.0731, -0.09118, -0.07711, -0.04005, -0.03749, 0.08379, -0.04031, -0.08035, -0.09181, -0.08785, -0.06853, 0.1291, -0.15203, -0.0375, 0.01832]\n",
      "-0.031743600000000004\n",
      "[-0.01051, -0.12934, 0.2289, 0.09709, 0.10702, -0.10671, 0.19846, 0.25088, -0.22248, -0.10455, 0.19655, 0.34887, -0.16093, 0.28973, -0.08032, -0.09099, 0.14822, -0.11222, 0.00665, 0.08548, -0.18212, 0.20492, 0.34716, -0.19113, 0.22979, -0.1012, 0.19882, -0.05349, -0.05394, 0.36293, -0.03427, -0.11714, 0.00361, -0.00507, 0.12294, -0.08827, -0.08074, -0.11113, -0.08182, 0.12499, 0.39028, 0.04513, -0.12076, -0.22123, -0.08107, -0.08056, 0.50988, -0.17217, 0.03144, 0.29084]\n",
      "0.0405284\n",
      "[-1.2614, -1.6619, 0.40065, -1.83529, -0.25626, -1.34421, 0.63747, 0.99006, -2.85778, -1.8941, -1.91948, 1.15743, -1.62025, 0.40524, -1.75355, -1.67327, -0.91785, -1.91455, -2.59639, -1.66074, -2.37783, -1.12426, -0.61611, -2.32759, -1.16178, -2.13697, -1.05763, -2.04763, -1.71075, 0.06689, -2.36386, -2.28149, -0.30904, -1.52051, -0.129, -1.82493, -2.32822, -1.95654, -1.00196, -1.61574, 0.73843, -1.68594, -2.04553, -2.40898, -2.24071, -1.75355, 1.45412, -5.03875, -1.77764, -0.45587]\n",
      "-1.3323108\n",
      "[0.32321, 0.26875, 0.31085, 0.37716, 0.31462, 0.34703, 0.38094, 0.4303, 0.34246, 0.3194, 0.41948, 0.41044, 0.33056, 0.32634, 0.31352, 0.30075, 0.342, 0.3269, 0.36426, 0.43471, 0.43076, 0.36062, 0.42635, 0.35766, 0.39987, 0.35001, 0.36871, 0.36222, 0.4342, 0.45097, 0.36882, 0.38442, 0.30248, 0.37654, 0.40759, 0.31449, 0.33952, 0.36541, 0.28249, 0.37032, 0.39371, 0.35247, 0.32567, 0.36318, 0.3268, 0.31352, 0.4101, 0.57761, 0.37237, 0.3611]\n",
      "0.36467320000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 186.58it/s]\n",
      "59it [00:00, 267.54it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 261.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -3.1000192, 'npmi_wiki': -0.031743600000000004, 'uci_wiki': -1.3323108, 'CV_wiki': 0.36467320000000003, 'cp_wiki': 0.0405284, 'sim_w2v': 0.12225475113772923, 'diversity': 0.328, 'filename': 'results/240517_153125.txt', 'acc': 0.49402549123738715, 'macro-F1': 0.4674685936857392, 'Purity': 0.5114179500796601, 'NMI': 0.48645531376616835, 'label_match': 0.43344528902245005}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.54682 - dist: 0.08964 - cons: -0.27331\n",
      "Epoch-1 / recon: 2.44628 - dist: 0.07193 - cons: -0.31212\n",
      "Epoch-2 / recon: 2.39432 - dist: 0.06492 - cons: -0.32813\n",
      "Epoch-3 / recon: 2.36389 - dist: 0.06105 - cons: -0.33729\n",
      "Epoch-4 / recon: 2.34408 - dist: 0.05873 - cons: -0.34304\n",
      "Epoch-5 / recon: 2.33015 - dist: 0.05702 - cons: -0.34696\n",
      "Epoch-6 / recon: 2.31965 - dist: 0.05548 - cons: -0.35053\n",
      "Epoch-7 / recon: 2.31144 - dist: 0.05421 - cons: -0.35408\n",
      "Epoch-8 / recon: 2.30486 - dist: 0.05305 - cons: -0.35684\n",
      "Epoch-9 / recon: 2.29949 - dist: 0.05210 - cons: -0.35920\n",
      "Epoch-10 / recon: 2.29504 - dist: 0.05134 - cons: -0.36109\n",
      "Epoch-11 / recon: 2.29127 - dist: 0.05073 - cons: -0.36274\n",
      "Epoch-12 / recon: 2.28802 - dist: 0.05015 - cons: -0.36431\n",
      "Epoch-13 / recon: 2.28525 - dist: 0.04969 - cons: -0.36572\n",
      "Epoch-14 / recon: 2.28283 - dist: 0.04925 - cons: -0.36690\n",
      "Epoch-15 / recon: 2.28071 - dist: 0.04889 - cons: -0.36799\n",
      "Epoch-16 / recon: 2.27881 - dist: 0.04851 - cons: -0.36890\n",
      "Epoch-17 / recon: 2.27714 - dist: 0.04822 - cons: -0.36976\n",
      "Epoch-18 / recon: 2.27560 - dist: 0.04793 - cons: -0.37061\n",
      "Epoch-19 / recon: 2.27422 - dist: 0.04770 - cons: -0.37142\n",
      "Epoch-20 / recon: 2.27298 - dist: 0.04748 - cons: -0.37207\n",
      "Epoch-21 / recon: 2.27185 - dist: 0.04726 - cons: -0.37276\n",
      "Epoch-22 / recon: 2.27081 - dist: 0.04705 - cons: -0.37335\n",
      "Epoch-23 / recon: 2.26985 - dist: 0.04692 - cons: -0.37393\n",
      "Epoch-24 / recon: 2.26897 - dist: 0.04677 - cons: -0.37445\n",
      "Epoch-25 / recon: 2.26815 - dist: 0.04661 - cons: -0.37492\n",
      "Epoch-26 / recon: 2.26739 - dist: 0.04649 - cons: -0.37542\n",
      "Epoch-27 / recon: 2.26668 - dist: 0.04639 - cons: -0.37591\n",
      "Epoch-28 / recon: 2.26603 - dist: 0.04631 - cons: -0.37636\n",
      "Epoch-29 / recon: 2.26540 - dist: 0.04619 - cons: -0.37678\n",
      "Epoch-30 / recon: 2.26481 - dist: 0.04609 - cons: -0.37718\n",
      "Epoch-31 / recon: 2.26425 - dist: 0.04599 - cons: -0.37756\n",
      "Epoch-32 / recon: 2.26374 - dist: 0.04591 - cons: -0.37791\n",
      "Epoch-33 / recon: 2.26326 - dist: 0.04582 - cons: -0.37826\n",
      "Epoch-34 / recon: 2.26279 - dist: 0.04572 - cons: -0.37857\n",
      "Epoch-35 / recon: 2.26237 - dist: 0.04563 - cons: -0.37886\n",
      "Epoch-36 / recon: 2.26194 - dist: 0.04555 - cons: -0.37917\n",
      "Epoch-37 / recon: 2.26152 - dist: 0.04548 - cons: -0.37946\n",
      "Epoch-38 / recon: 2.26114 - dist: 0.04542 - cons: -0.37975\n",
      "Epoch-39 / recon: 2.26076 - dist: 0.04536 - cons: -0.38004\n",
      "Epoch-40 / recon: 2.26034 - dist: 0.04521 - cons: -0.38045\n",
      "Epoch-41 / recon: 2.25991 - dist: 0.04506 - cons: -0.38090\n",
      "Epoch-42 / recon: 2.25951 - dist: 0.04494 - cons: -0.38132\n",
      "Epoch-43 / recon: 2.25909 - dist: 0.04483 - cons: -0.38174\n",
      "Epoch-44 / recon: 2.25870 - dist: 0.04473 - cons: -0.38216\n",
      "Epoch-45 / recon: 2.25833 - dist: 0.04462 - cons: -0.38255\n",
      "Epoch-46 / recon: 2.25797 - dist: 0.04452 - cons: -0.38290\n",
      "Epoch-47 / recon: 2.25761 - dist: 0.04441 - cons: -0.38326\n",
      "Epoch-48 / recon: 2.25726 - dist: 0.04432 - cons: -0.38361\n",
      "Epoch-49 / recon: 2.25690 - dist: 0.04418 - cons: -0.38410\n",
      "------- Evaluation results -------\n",
      "topic-0 ['subject', 'line', 'organization', 'game', 'university', 'host', 'posting', 'nntp', 'baseball', 'article', 'writes', 'team', 'player', 'one', 'thanks']\n",
      "topic-1 ['card', 'line', 'organization', 'game', 'university', 'vga', 'video', 'driver', 'article', 'monitor', 'subject', 'writes', 'baseball', 'thanks', 'nntp']\n",
      "topic-2 ['line', 'card', 'organization', 'game', 'subject', 'university', 'article', 'writes', 'host', 'nntp', 'baseball', 'year', 'posting', 'thanks', 'monitor']\n",
      "topic-3 ['printer', 'font', 'image', 'line', 'file', 'organization', 'window', 'color', 'gif', 'format', 'print', 'thanks', 'graphic', 'university', 'posting']\n",
      "topic-4 ['subject', 'line', 'organization', 'game', 'posting', 'host', 'nntp', 'writes', 'university', 'article', 'baseball', 'team', 'year', 'player', 'one']\n",
      "topic-5 ['card', 'line', 'vga', 'university', 'organization', 'video', 'game', 'article', 'driver', 'monitor', 'writes', 'thanks', 'baseball', 'know', 'nntp']\n",
      "topic-6 ['bike', 'subject', 'dod', 'motorcycle', 'line', 'organization', 'article', 'writes', 'riding', 'ride', 'helmet', 'dog', 'rider', 'posting', 'nntp']\n",
      "topic-7 ['subject', 'widget', 'motif', 'package', 'god', 'christian', 'program', 'bike', 'graphic', 'library', 'clipper', 'window', 'interface', 'dod', 'screen']\n",
      "topic-8 ['line', 'organization', 'bmw', 'ticket', 'battery', 'posting', 'nntp', 'host', 'article', 'writes', 'one', 'university', 'traffic', 'lib', 'doug']\n",
      "topic-9 ['car', 'driver', 'line', 'organization', 'article', 'writes', 'saturn', 'one', 'speed', 'dealer', 'engine', 'price', 'get', 'toyota', 'good']\n",
      "topic-10 ['key', 'clipper', 'chip', 'encryption', 'escrow', 'government', 'crypto', 'nsa', 'secret', 'algorithm', 'secure', 'wiretap', 'system', 'security', 'line']\n",
      "topic-11 ['team', 'hockey', 'nhl', 'game', 'player', 'play', 'playoff', 'season', 'goal', 'devil', 'cup', 'period', 'line', 'jet', 'organization']\n",
      "topic-12 ['card', 'line', 'vga', 'game', 'organization', 'university', 'article', 'driver', 'video', 'writes', 'monitor', 'subject', 'baseball', 'year', 'know']\n",
      "topic-13 ['subject', 'vga', 'bike', 'escrow', 'encryption', 'honda', 'clipper', 'crypto', 'video', 'monitor', 'card', 'phillies', 'baseball', 'nsa', 'pitcher']\n",
      "topic-14 ['line', 'subject', 'organization', 'card', 'university', 'game', 'article', 'writes', 'nntp', 'host', 'posting', 'baseball', 'video', 'monitor', 'thanks']\n",
      "topic-15 ['subject', 'line', 'organization', 'university', 'posting', 'nntp', 'point', 'host', 'article', 'thanks', 'valley', 'algorithm', 'writes', 'distribution', 'usa']\n",
      "topic-16 ['subject', 'msg', 'food', 'organization', 'line', 'article', 'drug', 'writes', 'air', 'one', 'water', 'nuclear', 'posting', 'circuit', 'host']\n",
      "topic-17 ['mac', 'modem', 'card', 'line', 'apple', 'organization', 'bus', 'simms', 'motherboard', 'ram', 'slot', 'board', 'university', 'controller', 'fax']\n",
      "topic-18 ['max', 'batf', 'line', 'organization', 'fbi', 'subject', 'writes', 'waco', 'people', 'weapon', 'article', 'law', 'right', 'clinton', 'posting']\n",
      "topic-19 ['subject', 'line', 'organization', 'game', 'posting', 'host', 'nntp', 'university', 'team', 'article', 'writes', 'player', 'baseball', 'year', 'one']\n",
      "topic-20 ['god', 'christian', 'bible', 'clipper', 'subject', 'jesus', 'encryption', 'widget', 'christ', 'motif', 'religion', 'dod', 'bike', 'package', 'motorcycle']\n",
      "topic-21 ['god', 'subject', 'christian', 'jesus', 'people', 'one', 'believe', 'faith', 'church', 'belief', 'bible', 'say', 'life', 'truth', 'atheist']\n",
      "topic-22 ['subject', 'god', 'christian', 'jesus', 'bible', 'line', 'church', 'organization', 'christ', 'writes', 'one', 'article', 'book', 'sin', 'university']\n",
      "topic-23 ['subject', 'line', 'organization', 'game', 'university', 'posting', 'host', 'nntp', 'writes', 'baseball', 'article', 'year', 'team', 'one', 'player']\n",
      "topic-24 ['gun', 'armenian', 'firearm', 'turkish', 'people', 'weapon', 'handgun', 'armenia', 'argic', 'serdar', 'criminal', 'right', 'crime', 'one', 'genocide']\n",
      "topic-25 ['subject', 'widget', 'motif', 'organization', 'god', 'package', 'line', 'window', 'game', 'posting', 'team', 'program', 'software', 'host', 'graphic']\n",
      "topic-26 ['window', 'file', 'line', 'organization', 'manager', 'program', 'icon', 'use', 'application', 'running', 'version', 'run', 'host', 'using', 'set']\n",
      "topic-27 ['subject', 'widget', 'god', 'motif', 'organization', 'package', 'christian', 'line', 'posting', 'software', 'game', 'team', 'window', 'program', 'host']\n",
      "topic-28 ['subject', 'line', 'organization', 'game', 'university', 'article', 'writes', 'card', 'nntp', 'host', 'posting', 'baseball', 'year', 'thanks', 'player']\n",
      "topic-29 ['space', 'orbit', 'moon', 'line', 'organization', 'nasa', 'spacecraft', 'satellite', 'article', 'earth', 'writes', 'billion', 'shuttle', 'lunar', 'planet']\n",
      "topic-30 ['israeli', 'israel', 'arab', 'jew', 'palestinian', 'greek', 'turkish', 'writes', 'policy', 'article', 'people', 'jewish', 'lebanese', 'organization', 'line']\n",
      "topic-31 ['subject', 'widget', 'motif', 'god', 'organization', 'package', 'line', 'christian', 'posting', 'bible', 'host', 'program', 'nntp', 'window', 'software']\n",
      "topic-32 ['subject', 'line', 'organization', 'game', 'university', 'article', 'host', 'posting', 'nntp', 'writes', 'baseball', 'team', 'player', 'year', 'run']\n",
      "topic-33 ['subject', 'widget', 'motif', 'god', 'package', 'christian', 'organization', 'window', 'line', 'program', 'bible', 'team', 'software', 'posting', 'screen']\n",
      "topic-34 ['subject', 'widget', 'motif', 'package', 'god', 'program', 'bike', 'dod', 'window', 'christian', 'software', 'bible', 'clipper', 'machine', 'jesus']\n",
      "topic-35 ['subject', 'line', 'organization', 'mail', 'nntp', 'host', 'posting', 'internet', 'phone', 'thanks', 'number', 'file', 'address', 'ftp', 'please']\n",
      "topic-36 ['subject', 'gordon', 'bank', 'doctor', 'disease', 'patient', 'organization', 'line', 'article', 'writes', 'medical', 'pain', 'health', 'chastity', 'shameful']\n",
      "topic-37 ['subject', 'card', 'video', 'driver', 'monitor', 'vga', 'line', 'color', 'organization', 'window', 'mode', 'graphic', 'thanks', 'university', 'host']\n",
      "topic-38 ['morality', 'homosexual', 'objective', 'atheist', 'writes', 'gay', 'line', 'keith', 'article', 'moral', 'people', 'organization', 'islam', 'islamic', 'sexual']\n",
      "topic-39 ['card', 'line', 'game', 'subject', 'organization', 'university', 'monitor', 'video', 'baseball', 'vga', 'thanks', 'driver', 'article', 'host', 'nntp']\n",
      "topic-40 ['subject', 'god', 'widget', 'motif', 'christian', 'clipper', 'package', 'encryption', 'bible', 'bike', 'dod', 'jesus', 'key', 'government', 'motorcycle']\n",
      "topic-41 ['card', 'line', 'organization', 'university', 'game', 'subject', 'video', 'article', 'driver', 'vga', 'writes', 'baseball', 'monitor', 'host', 'nntp']\n",
      "topic-42 ['subject', 'line', 'organization', 'game', 'player', 'baseball', 'team', 'posting', 'host', 'nntp', 'university', 'writes', 'article', 'year', 'widget']\n",
      "topic-43 ['card', 'vga', 'video', 'line', 'monitor', 'game', 'driver', 'university', 'organization', 'article', 'baseball', 'subject', 'thanks', 'mode', 'writes']\n",
      "topic-44 ['mouse', 'subject', 'line', 'organization', 'port', 'keyboard', 'amp', 'use', 'university', 'one', 'audio', 'sound', 'channel', 'host', 'posting']\n",
      "topic-45 ['subject', 'writes', 'line', 'organization', 'article', 'atf', 'one', 'death', 'university', 'people', 'racism', 'posting', 'god', 'dividian', 'ranch']\n",
      "topic-46 ['subject', 'game', 'line', 'organization', 'team', 'player', 'year', 'baseball', 'university', 'writes', 'article', 'posting', 'host', 'nntp', 'win']\n",
      "topic-47 ['card', 'vga', 'video', 'monitor', 'line', 'driver', 'university', 'game', 'article', 'organization', 'mode', 'writes', 'baseball', 'thanks', 'year']\n",
      "topic-48 ['drive', 'scsi', 'disk', 'ide', 'floppy', 'hard', 'controller', 'organization', 'line', 'mac', 'problem', 'boot', 'file', 'system', 'rom']\n",
      "topic-49 ['subject', 'car', 'sale', 'line', 'organization', 'radar', 'detector', 'oil', 'university', 'posting', 'host', 'nntp', 'price', 'engine', 'new']\n",
      "./\n",
      "[-3.02267, -3.81059, -3.41573, -3.36448, -2.33374, -3.48805, -5.63313, -3.77445, -4.12553, -2.13963, -3.22887, -2.11514, -1.93393, -7.25594, -3.41368, -3.28712, -2.40283, -3.49088, -3.04314, -2.35649, -5.1865, -2.13145, -1.52555, -2.49167, -5.57996, -2.65997, -2.73911, -2.54494, -3.28731, -2.3148, -1.73423, -4.37204, -2.72999, -3.11432, -3.96462, -3.62059, -2.82741, -2.55398, -2.09378, -4.26378, -4.52433, -3.72747, -3.55245, -2.37378, -3.01689, -2.57083, -3.26856, -2.11671, -2.39021, -4.24887]\n",
      "-3.2232423999999997\n",
      "[-0.07486, -0.06079, -0.08407, -0.01608, -0.06997, -0.05936, -0.10113, -0.06698, -0.11809, 0.00885, 0.01402, 0.09245, -0.0242, -0.13622, -0.07815, -0.09133, -0.07066, -0.00679, -0.07388, -0.06997, -0.16305, 0.1095, 0.04399, -0.06997, -0.02954, -0.05576, -0.04816, -0.07197, -0.07467, 0.11106, 0.08074, -0.09208, -0.07298, -0.07495, -0.10329, -0.05022, -0.06913, 0.00468, 0.0097, -0.06331, -0.14888, -0.05178, -0.09136, -0.02031, -0.05547, -0.07013, -0.07265, -0.01952, 0.09457, -0.10143]\n",
      "-0.0460716\n",
      "[-0.09758, -0.11183, -0.13366, 0.16444, -0.10619, -0.11244, -0.2674, -0.00732, -0.23842, 0.03868, 0.26534, 0.37166, -0.05473, -0.25188, -0.12484, -0.1406, -0.10771, 0.25849, -0.13771, -0.07692, -0.24813, 0.44491, 0.15194, -0.09584, -0.05958, 0.02721, -0.00282, -0.0307, -0.15157, 0.33396, 0.27162, -0.0616, -0.07147, -0.04611, -0.11132, 0.09732, 0.00819, 0.11577, 0.1098, -0.06185, -0.23491, -0.07676, -0.11139, 0.01628, 0.00145, -0.18197, -0.08278, -0.00364, 0.45744, -0.12227]\n",
      "-0.011788799999999999\n",
      "[-2.08718, -1.7152, -2.17456, -1.17182, -2.00443, -1.64284, -3.18887, -2.32744, -3.05563, -0.13324, -0.82782, 0.80572, -0.5982, -4.42391, -2.11155, -2.30341, -1.87184, -0.98462, -2.11385, -2.00443, -5.22186, 1.22377, 0.42552, -2.00443, -1.79991, -1.78066, -1.67998, -2.05275, -2.04763, 1.23471, 0.91315, -2.79172, -2.10081, -2.1756, -3.30957, -1.83529, -2.23124, -0.22242, -0.34395, -1.7933, -4.40907, -1.5041, -2.5706, -0.68041, -1.47148, -1.77864, -2.11869, -0.60943, 0.79882, -2.6616]\n",
      "-1.6506857999999993\n",
      "[0.36891, 0.39122, 0.34772, 0.38046, 0.36732, 0.38273, 0.47498, 0.39074, 0.35963, 0.30183, 0.41288, 0.46638, 0.32058, 0.56642, 0.3483, 0.34307, 0.29001, 0.37991, 0.37191, 0.36732, 0.56765, 0.37628, 0.35256, 0.36732, 0.42661, 0.35168, 0.34457, 0.34792, 0.36632, 0.4571, 0.40134, 0.41128, 0.37033, 0.36812, 0.43809, 0.37716, 0.32802, 0.34666, 0.32052, 0.39899, 0.49604, 0.3799, 0.38233, 0.33728, 0.3127, 0.28244, 0.38922, 0.32633, 0.42202, 0.3652]\n",
      "0.380886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 195.60it/s]\n",
      "59it [00:00, 282.30it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 258.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'umass_wiki': -3.2232423999999997, 'npmi_wiki': -0.0460716, 'uci_wiki': -1.6506857999999993, 'CV_wiki': 0.380886, 'cp_wiki': -0.011788799999999999, 'sim_w2v': 0.11873108914489816, 'diversity': 0.30933333333333335, 'filename': 'results/240517_154244.txt', 'acc': 0.47437599575146044, 'macro-F1': 0.44354017434120074, 'Purity': 0.49070631970260226, 'NMI': 0.4733430637891917, 'label_match': 0.46049142655117553}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.54453 - dist: 0.08944 - cons: -0.25046\n",
      "Epoch-1 / recon: 2.44148 - dist: 0.06865 - cons: -0.29795\n",
      "Epoch-2 / recon: 2.38818 - dist: 0.06046 - cons: -0.31811\n",
      "Epoch-3 / recon: 2.35742 - dist: 0.05631 - cons: -0.32885\n",
      "Epoch-4 / recon: 2.33767 - dist: 0.05383 - cons: -0.33614\n",
      "Epoch-5 / recon: 2.32390 - dist: 0.05220 - cons: -0.34096\n",
      "Epoch-6 / recon: 2.31364 - dist: 0.05084 - cons: -0.34485\n",
      "Epoch-7 / recon: 2.30584 - dist: 0.04980 - cons: -0.34777\n",
      "Epoch-8 / recon: 2.29946 - dist: 0.04898 - cons: -0.35095\n",
      "Epoch-9 / recon: 2.29412 - dist: 0.04817 - cons: -0.35408\n",
      "Epoch-10 / recon: 2.28956 - dist: 0.04746 - cons: -0.35674\n",
      "Epoch-11 / recon: 2.28566 - dist: 0.04688 - cons: -0.35923\n",
      "Epoch-12 / recon: 2.28234 - dist: 0.04644 - cons: -0.36146\n",
      "Epoch-13 / recon: 2.27948 - dist: 0.04604 - cons: -0.36336\n",
      "Epoch-14 / recon: 2.27693 - dist: 0.04569 - cons: -0.36516\n",
      "Epoch-15 / recon: 2.27469 - dist: 0.04537 - cons: -0.36672\n",
      "Epoch-16 / recon: 2.27267 - dist: 0.04503 - cons: -0.36813\n",
      "Epoch-17 / recon: 2.27084 - dist: 0.04469 - cons: -0.36939\n",
      "Epoch-18 / recon: 2.26916 - dist: 0.04438 - cons: -0.37057\n",
      "Epoch-19 / recon: 2.26766 - dist: 0.04414 - cons: -0.37151\n",
      "Epoch-20 / recon: 2.26625 - dist: 0.04388 - cons: -0.37251\n",
      "Epoch-21 / recon: 2.26499 - dist: 0.04365 - cons: -0.37341\n",
      "Epoch-22 / recon: 2.26382 - dist: 0.04345 - cons: -0.37428\n",
      "Epoch-23 / recon: 2.26275 - dist: 0.04328 - cons: -0.37498\n",
      "Epoch-24 / recon: 2.26176 - dist: 0.04311 - cons: -0.37569\n",
      "Epoch-25 / recon: 2.26084 - dist: 0.04297 - cons: -0.37640\n",
      "Epoch-26 / recon: 2.25998 - dist: 0.04282 - cons: -0.37710\n",
      "Epoch-27 / recon: 2.25918 - dist: 0.04268 - cons: -0.37775\n",
      "Epoch-28 / recon: 2.25843 - dist: 0.04254 - cons: -0.37835\n",
      "Epoch-29 / recon: 2.25773 - dist: 0.04242 - cons: -0.37896\n",
      "Epoch-30 / recon: 2.25707 - dist: 0.04230 - cons: -0.37955\n",
      "Epoch-31 / recon: 2.25644 - dist: 0.04220 - cons: -0.38005\n",
      "Epoch-32 / recon: 2.25587 - dist: 0.04210 - cons: -0.38060\n",
      "Epoch-33 / recon: 2.25534 - dist: 0.04200 - cons: -0.38108\n",
      "Epoch-34 / recon: 2.25482 - dist: 0.04191 - cons: -0.38152\n",
      "Epoch-35 / recon: 2.25433 - dist: 0.04184 - cons: -0.38194\n",
      "Epoch-36 / recon: 2.25385 - dist: 0.04175 - cons: -0.38238\n",
      "Epoch-37 / recon: 2.25339 - dist: 0.04167 - cons: -0.38274\n",
      "Epoch-38 / recon: 2.25294 - dist: 0.04162 - cons: -0.38314\n",
      "Epoch-39 / recon: 2.25254 - dist: 0.04153 - cons: -0.38353\n",
      "Epoch-40 / recon: 2.25213 - dist: 0.04141 - cons: -0.38405\n",
      "Epoch-41 / recon: 2.25173 - dist: 0.04131 - cons: -0.38457\n",
      "Epoch-42 / recon: 2.25136 - dist: 0.04120 - cons: -0.38506\n",
      "Epoch-43 / recon: 2.25099 - dist: 0.04111 - cons: -0.38557\n",
      "Epoch-44 / recon: 2.25063 - dist: 0.04101 - cons: -0.38606\n",
      "Epoch-45 / recon: 2.25028 - dist: 0.04092 - cons: -0.38649\n",
      "Epoch-46 / recon: 2.24994 - dist: 0.04083 - cons: -0.38696\n",
      "Epoch-47 / recon: 2.24960 - dist: 0.04074 - cons: -0.38738\n",
      "Epoch-48 / recon: 2.24928 - dist: 0.04065 - cons: -0.38781\n",
      "Epoch-49 / recon: 2.24896 - dist: 0.04056 - cons: -0.38822\n",
      "------- Evaluation results -------\n",
      "topic-0 ['subject', 'msg', 'food', 'organization', 'line', 'article', 'writes', 'drug', 'air', 'one', 'water', 'know', 'nuclear', 'circuit', 'sensitivity']\n",
      "topic-1 ['window', 'subject', 'screen', 'disk', 'file', 'software', 'line', 'graphic', 'program', 'drive', 'monitor', 'card', 'memory', 'display', 'video']\n",
      "topic-2 ['subject', 'car', 'graphic', 'window', 'team', 'game', 'gun', 'screen', 'file', 'hockey', 'package', 'program', 'player', 'engine', 'image']\n",
      "topic-3 ['subject', 'graphic', 'gun', 'game', 'team', 'image', 'window', 'package', 'program', 'screen', 'organization', 'line', 'animation', 'file', 'government']\n",
      "topic-4 ['subject', 'window', 'line', 'organization', 'sale', 'university', 'software', 'problem', 'system', 'disk', 'host', 'screen', 'posting', 'computer', 'nntp']\n",
      "topic-5 ['subject', 'organization', 'line', 'posting', 'nntp', 'host', 'article', 'people', 'god', 'christian', 'writes', 'graphic', 'university', 'like', 'one']\n",
      "topic-6 ['subject', 'line', 'organization', 'university', 'point', 'posting', 'nntp', 'article', 'host', 'valley', 'thanks', 'writes', 'algorithm', 'chuck', 'distribution']\n",
      "topic-7 ['subject', 'organization', 'line', 'sale', 'posting', 'graphic', 'host', 'car', 'article', 'nntp', 'animation', 'university', 'writes', 'gun', 'like']\n",
      "topic-8 ['car', 'subject', 'detector', 'radar', 'article', 'writes', 'line', 'organization', 'engine', 'oil', 'dealer', 'one', 'saturn', 'good', 'speed']\n",
      "topic-9 ['key', 'chip', 'government', 'encryption', 'clipper', 'nsa', 'one', 'escrow', 'crypto', 'people', 'secure', 'system', 'bit', 'clinton', 'security']\n",
      "topic-10 ['drive', 'scsi', 'disk', 'ide', 'floppy', 'hard', 'controller', 'organization', 'line', 'mac', 'problem', 'boot', 'system', 'host', 'rom']\n",
      "topic-11 ['bike', 'subject', 'dod', 'motorcycle', 'line', 'article', 'organization', 'writes', 'riding', 'ride', 'dog', 'helmet', 'rider', 'posting', 'nntp']\n",
      "topic-12 ['gun', 'armenian', 'subject', 'firearm', 'people', 'weapon', 'turkish', 'handgun', 'right', 'one', 'armenia', 'line', 'argic', 'article', 'serdar']\n",
      "topic-13 ['baseball', 'team', 'game', 'player', 'year', 'line', 'organization', 'writes', 'article', 'jewish', 'phillies', 'season', 'run', 'pitcher', 'pitching']\n",
      "topic-14 ['window', 'line', 'file', 'organization', 'manager', 'widget', 'program', 'icon', 'application', 'use', 'motif', 'set', 'host', 'running', 'nntp']\n",
      "topic-15 ['subject', 'gordon', 'bank', 'doctor', 'article', 'organization', 'patient', 'disease', 'line', 'writes', 'medical', 'pain', 'pittsburgh', 'health', 'shameful']\n",
      "topic-16 ['line', 'subject', 'organization', 'system', 'software', 'thanks', 'computer', 'university', 'sale', 'host', 'nntp', 'mail', 'problem', 'posting', 'use']\n",
      "topic-17 ['space', 'subject', 'orbit', 'moon', 'line', 'organization', 'nasa', 'spacecraft', 'article', 'writes', 'satellite', 'earth', 'shuttle', 'year', 'billion']\n",
      "topic-18 ['game', 'organization', 'line', 'university', 'team', 'win', 'playoff', 'detroit', 'wing', 'posting', 'writes', 'espn', 'host', 'nntp', 'toronto']\n",
      "topic-19 ['team', 'hockey', 'nhl', 'player', 'game', 'play', 'goal', 'season', 'playoff', 'devil', 'line', 'period', 'organization', 'traded', 'captain']\n",
      "topic-20 ['subject', 'car', 'team', 'organization', 'posting', 'nntp', 'graphic', 'host', 'line', 'game', 'hockey', 'article', 'honda', 'player', 'nhl']\n",
      "topic-21 ['subject', 'line', 'organization', 'posting', 'nntp', 'host', 'sale', 'university', 'distribution', 'thanks', 'please', 'package', 'car', 'like', 'reply']\n",
      "topic-22 ['subject', 'car', 'hockey', 'team', 'graphic', 'cup', 'nhl', 'posting', 'organization', 'game', 'line', 'season', 'nntp', 'host', 'playoff']\n",
      "topic-23 ['line', 'organization', 'subject', 'posting', 'host', 'nntp', 'university', 'thanks', 'sale', 'distribution', 'mail', 'system', 'new', 'please', 'use']\n",
      "topic-24 ['organization', 'subject', 'line', 'posting', 'host', 'nntp', 'university', 'thanks', 'please', 'one', 'distribution', 'like', 'sale', 'reply', 'mail']\n",
      "topic-25 ['israeli', 'israel', 'arab', 'armenian', 'palestinian', 'subject', 'jewish', 'gaza', 'pitcher', 'turkish', 'baseball', 'gun', 'firearm', 'armenia', 'jew']\n",
      "topic-26 ['max', 'subject', 'writes', 'fbi', 'batf', 'line', 'organization', 'atf', 'article', 'people', 'waco', 'fire', 'death', 'law', 'trial']\n",
      "topic-27 ['god', 'christian', 'jesus', 'bible', 'church', 'line', 'christ', 'organization', 'writes', 'one', 'article', 'book', 'sin', 'christianity', 'university']\n",
      "topic-28 ['subject', 'line', 'organization', 'ticket', 'bmw', 'battery', 'posting', 'article', 'nntp', 'host', 'writes', 'one', 'university', 'lib', 'traffic']\n",
      "topic-29 ['israel', 'israeli', 'arab', 'jew', 'subject', 'writes', 'article', 'palestinian', 'greek', 'policy', 'turkish', 'people', 'line', 'organization', 'jewish']\n",
      "topic-30 ['subject', 'line', 'organization', 'window', 'graphic', 'posting', 'sale', 'nntp', 'car', 'host', 'thanks', 'university', 'computer', 'distribution', 'please']\n",
      "topic-31 ['window', 'subject', 'line', 'software', 'organization', 'file', 'disk', 'system', 'program', 'screen', 'problem', 'computer', 'sale', 'graphic', 'using']\n",
      "topic-32 ['card', 'monitor', 'video', 'driver', 'vga', 'line', 'organization', 'window', 'color', 'mode', 'graphic', 'vram', 'thanks', 'svga', 'university']\n",
      "topic-33 ['line', 'organization', 'subject', 'system', 'university', 'host', 'thanks', 'computer', 'nntp', 'use', 'posting', 'software', 'new', 'used', 'sale']\n",
      "topic-34 ['organization', 'line', 'subject', 'nntp', 'posting', 'host', 'university', 'sale', 'new', 'mail', 'thanks', 'distribution', 'please', 'offer', 'reply']\n",
      "topic-35 ['subject', 'morality', 'atheist', 'objective', 'homosexual', 'writes', 'islam', 'keith', 'gay', 'article', 'people', 'moral', 'islamic', 'line', 'god']\n",
      "topic-36 ['subject', 'car', 'gun', 'team', 'game', 'graphic', 'government', 'dod', 'honda', 'engine', 'israeli', 'organization', 'weapon', 'testing', 'insurance']\n",
      "topic-37 ['subject', 'line', 'organization', 'sale', 'posting', 'host', 'thanks', 'nntp', 'university', 'software', 'mail', 'computer', 'distribution', 'offer', 'please']\n",
      "topic-38 ['line', 'organization', 'key', 'clipper', 'chip', 'phone', 'internet', 'encryption', 'nntp', 'host', 'posting', 'mail', 'escrow', 'number', 'crypto']\n",
      "topic-39 ['printer', 'font', 'organization', 'image', 'line', 'file', 'color', 'window', 'gif', 'print', 'format', 'university', 'posting', 'thanks', 'nntp']\n",
      "topic-40 ['window', 'screen', 'file', 'display', 'disk', 'monitor', 'card', 'problem', 'running', 'subject', 'video', 'driver', 'program', 'widget', 'ram']\n",
      "topic-41 ['window', 'subject', 'screen', 'line', 'disk', 'problem', 'software', 'system', 'ram', 'computer', 'display', 'memory', 'drive', 'using', 'file']\n",
      "topic-42 ['subject', 'car', 'gun', 'graphic', 'game', 'image', 'organization', 'team', 'government', 'package', 'sale', 'people', 'library', 'article', 'israeli']\n",
      "topic-43 ['subject', 'car', 'organization', 'sale', 'line', 'graphic', 'posting', 'gun', 'article', 'nntp', 'host', 'university', 'package', 'game', 'writes']\n",
      "topic-44 ['window', 'screen', 'display', 'file', 'monitor', 'widget', 'disk', 'color', 'video', 'running', 'program', 'driver', 'server', 'subject', 'problem']\n",
      "topic-45 ['line', 'organization', 'subject', 'system', 'window', 'thanks', 'computer', 'use', 'university', 'host', 'nntp', 'using', 'software', 'posting', 'please']\n",
      "topic-46 ['line', 'window', 'subject', 'organization', 'system', 'university', 'software', 'computer', 'problem', 'thanks', 'host', 'use', 'using', 'nntp', 'posting']\n",
      "topic-47 ['subject', 'mouse', 'line', 'organization', 'keyboard', 'port', 'use', 'one', 'university', 'amp', 'posting', 'host', 'nntp', 'channel', 'sound']\n",
      "topic-48 ['mac', 'modem', 'card', 'line', 'organization', 'apple', 'bus', 'simms', 'motherboard', 'slot', 'university', 'ram', 'board', 'controller', 'fax']\n",
      "topic-49 ['god', 'christian', 'jesus', 'people', 'one', 'faith', 'believe', 'belief', 'church', 'bible', 'say', 'truth', 'life', 'christianity', 'atheist']\n",
      "./\n",
      "[-2.15193, -2.16179, -2.44754, -2.27835, -3.89408, -2.25211, -3.63636, -3.31236, -2.16214, -2.92579, -2.4313, -5.65362, -5.99441, -2.44586, -5.31141, -2.63652, -3.56294, -2.08906, -4.53203, -2.51522, -4.22228, -3.72105, -4.55137, -2.99538, -3.50141, -3.34132, -2.33607, -1.5906, -3.96965, -1.69444, -3.85251, -1.96677, -4.24649, -5.81487, -3.49921, -1.99228, -4.60995, -3.61175, -5.32737, -4.87214, -4.12842, -1.8942, -2.3693, -3.22875, -3.80861, -3.50144, -3.28974, -4.20491, -3.5515, -2.17238]\n",
      "-3.3652196000000005\n",
      "[-0.04005, 0.07422, 0.00094, -0.00394, -0.06247, -0.07826, -0.09992, -0.09061, -0.02482, -0.00876, 0.08719, -0.10113, -0.05571, 0.02808, -0.04954, -0.03823, -0.06188, 0.06357, -0.05448, 0.05958, -0.09347, -0.09418, -0.07362, -0.07422, -0.08817, -0.04383, -0.04924, 0.05551, -0.11058, 0.06678, -0.08789, 0.02235, -0.00421, -0.08532, -0.07829, 0.01404, -0.09679, -0.0704, -0.08219, -0.06106, -0.00542, 0.04637, -0.02321, -0.08854, 0.00377, -0.0787, -0.07672, -0.09115, -0.00679, 0.1291]\n",
      "-0.03364579999999999\n",
      "[-0.08186, 0.36312, 0.0862, 0.05911, 0.06092, -0.10129, -0.18114, -0.1323, -0.04645, 0.22165, 0.41399, -0.26738, -0.17122, 0.11412, 0.03027, 0.01601, 0.05442, 0.17304, -0.01558, 0.36315, -0.09255, -0.11224, -0.06992, -0.04059, -0.05613, 0.10116, -0.14433, 0.18594, -0.19178, 0.19995, -0.10146, 0.24619, 0.17511, -0.09256, -0.0345, 0.12263, -0.17797, 0.01483, 0.03984, 0.04785, 0.17746, 0.31301, -0.03021, -0.11468, 0.17746, -0.01691, 0.02428, -0.1034, 0.25852, 0.51001]\n",
      "0.04347580000000001\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_classification, evaluate_clustering\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    testloader = DataLoader(testds2, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * ((1-org_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * ((1-pos_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic, pos_topic), dim=0), dirichlet_alpha_2)\n",
    "            \n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(15, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "    now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "                                  reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                                  filename=f'results/{now}.txt')\n",
    "    train_theta = []\n",
    "    test_theta = []\n",
    "    for batch_idx, batch in tqdm(enumerate(trainloader)):\n",
    "        org_input, _, org_bow, _ = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        train_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    train_theta = np.concatenate(train_theta, axis=0)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(testloader)): \n",
    "        org_input, org_bow = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        test_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    test_theta = np.concatenate(test_theta, axis=0)\n",
    "    \n",
    "    classification_res = evaluate_classification(train_theta, test_theta, textData.targets, textData.test_targets)\n",
    "    clustering_res = evaluate_clustering(test_theta, textData.test_targets)\n",
    "    \n",
    "    results.update(classification_res)\n",
    "    results.update(clustering_res)\n",
    "    \n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(textData.data) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_N   CV_wiki   sim_w2v  diversity                   filename      acc  \\\n",
      "0       50  0.367416  0.198575   0.290667  results/240516_233645.txt  0.58284   \n",
      "1       50  0.367488  0.197549   0.266667  results/240516_234214.txt  0.58976   \n",
      "2       50  0.364076  0.192047   0.293333  results/240516_234737.txt  0.58264   \n",
      "3       50  0.361422  0.191338   0.269333  results/240516_235509.txt  0.58844   \n",
      "4       50  0.368351  0.197752   0.264000  results/240517_000053.txt  0.58924   \n",
      "\n",
      "   macro-F1   Purity       NMI  \n",
      "0  0.581245  0.57880  0.007457  \n",
      "1  0.589379  0.57484  0.007546  \n",
      "2  0.581773  0.57700  0.007772  \n",
      "3  0.587958  0.57088  0.006728  \n",
      "4  0.588312  0.57708  0.007461  \n",
      "mean\n",
      "topic_N      50.000000\n",
      "CV_wiki       0.365751\n",
      "sim_w2v       0.195452\n",
      "diversity     0.276800\n",
      "acc           0.586584\n",
      "macro-F1      0.585733\n",
      "Purity        0.575720\n",
      "NMI           0.007393\n",
      "dtype: float64\n",
      "std\n",
      "topic_N      0.000000\n",
      "CV_wiki      0.002920\n",
      "sim_w2v      0.003463\n",
      "diversity    0.014035\n",
      "acc          0.003541\n",
      "macro-F1     0.003896\n",
      "Purity       0.003049\n",
      "NMI          0.000393\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)\n",
    "print('mean')\n",
    "print(results_df.mean())\n",
    "print('std')\n",
    "print(results_df.std())\n",
    "\n",
    "if args.result_file is not None:\n",
    "    result_filename = f'results/{args.result_file}'\n",
    "else:\n",
    "    result_filename = f'results/{now}.tsv'\n",
    "\n",
    "results_df.to_csv(result_filename, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
