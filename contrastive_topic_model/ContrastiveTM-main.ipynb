{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Arguments\n",
    "args_text = '--base-model sentence-transformers/all-MiniLM-L6-v2 '+\\\n",
    "            '--dataset news --n-word 2000 --epochs-1 100 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 1 '+ \\\n",
    "            '--n-cluster 50 ' #+ \\\n",
    "            #'--stage-1-ckpt trained_model/news_model_all-MiniLM-L6-v2_stage1_50t_5000w_99e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    parser.add_argument('--epochs-1', default=100, type=int,\n",
    "                        help='Number of training epochs for Stage 1')\n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--dataset', default='news', type=str,\n",
    "                        choices=['news', 'twitter', 'wiki', 'nips', 'stackoverflow', 'reuters', 'r52', 'imdb', 'agnews', 'yahoo'],\n",
    "                        help='Name of the dataset')\n",
    "    parser.add_argument('--n-cluster', default=50, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "    \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    "    \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str, default='./',\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "def data_load(dataset_name):\n",
    "    should_measure_hungarian = False\n",
    "    if dataset_name == 'news':\n",
    "        textData = newsData()\n",
    "        should_measure_hungarian = True\n",
    "    elif dataset_name == 'imdb':\n",
    "        textData = IMDBData()\n",
    "    elif dataset_name == 'agnews':\n",
    "        textData = AGNewsData()\n",
    "    elif dataset_name == 'yahoo':\n",
    "        textData = YahooData()\n",
    "    elif dataset_name == 'twitter':\n",
    "        textData = twitterData('/home/data/topicmodel/twitter_covid19.tsv')\n",
    "    elif dataset_name == 'wiki':\n",
    "        textData = wikiData('/home/data/topicmodel/smplAbstracts/')\n",
    "    elif dataset_name == 'nips':\n",
    "        textData = nipsData('/home/data/topicmodel/papers.csv')\n",
    "    elif dataset_name == 'stackoverflow':\n",
    "        textData = stackoverflowData('/home/data/topicmodel/stack_overflow.csv')\n",
    "    elif dataset_name == 'reuters':\n",
    "        textData = reutersData('/home/data/topicmodel/reuters-21578.txt')\n",
    "    elif dataset_name == 'r52':\n",
    "        textData = r52Data('/home/data/topicmodel/r52/')\n",
    "        should_measure_hungarian = True\n",
    "    return textData, should_measure_hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:15<00:00, 734.60it/s]\n"
     ]
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=textData.data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "basesim_path = f\"./save/{args.dataset}_{bert_name_short}_basesim_matrix_full.pkl\"\n",
    "if os.path.isfile(basesim_path) == False:\n",
    "    model = SentenceTransformer(bert_name.split('/')[-1], device='cuda')\n",
    "    base_result_list = []\n",
    "    for text in tqdm_notebook(trainds.nonempty_text):\n",
    "        base_result_list.append(model.encode(text))\n",
    "        \n",
    "    base_result_embedding = np.stack(base_result_list)\n",
    "    basereduced_norm = F.normalize(torch.tensor(base_result_embedding), dim=-1)\n",
    "    basesim_matrix = torch.mm(basereduced_norm, basereduced_norm.t())\n",
    "    ind = np.diag_indices(basesim_matrix.shape[0])\n",
    "    basesim_matrix[ind[0], ind[1]] = torch.ones(basesim_matrix.shape[0]) * -1\n",
    "    torch.save(basesim_matrix, basesim_path)\n",
    "else:\n",
    "    basesim_matrix = torch.load(basesim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Discovery neighborhood pairs and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hidden, alpha=1.0):\n",
    "    device = hidden.device\n",
    "    hidden_dim = hidden.shape[-1]\n",
    "    rand_w = torch.Tensor(np.eye(hidden_dim, dtype=np.float64)).to(device)\n",
    "    loss_dist_match = get_swd_loss(hidden, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t) ** 2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=384)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "    model.cuda(gpu_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = args.bsz =16#= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004000186920166016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bd6fbdab2d42fdad866eb38d181fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004000663757324219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7001eaa5d5844db9cfccf13453a7292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003000020980834961,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef950033eec9419f9577c1d3d27b41b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004001617431640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5f94c5dc4a4e18a8420d8aa313cb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00400090217590332,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e370f84e984c3eb42f12ed6f7c793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002999544143676758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402fb85108bc4b189e12757cb22e7248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003000497817993164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3baa602b1014369a0b0a6e6d28c23cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004000425338745117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd440410d3450d88a2be70e64accde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0050008296966552734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f893e36c204de9a9f5ab0e1cc6591a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0029993057250976562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 708,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd354c19af3647c0bb2adf24603d69db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not skip_stage_1:\n",
    "    losses = AverageMeter()\n",
    "    closses = AverageMeter() \n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "    finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "    optimizer = RangerLars(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    global_step = 0\n",
    "    memory_queue = F.softmax(torch.randn(512, n_cluster).cuda(gpu_ids[0]), dim=1)\n",
    "    for epoch in range(epochs_1):\n",
    "        model.train()\n",
    "        #ema_model.train()\n",
    "        tbar = tqdm_notebook(trainloader)\n",
    "        for batch_idx, batch in enumerate(tbar):       \n",
    "            org_input, pos_input, _, _ = batch\n",
    "            org_input_ids = org_input['input_ids'].cuda(gpu_ids[0])\n",
    "            org_attention_mask = org_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            pos_input_ids = pos_input['input_ids'].cuda(gpu_ids[0])\n",
    "            pos_attention_mask = pos_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            all_input_ids = torch.cat((org_input_ids, pos_input_ids), dim=0)\n",
    "            all_attention_masks = torch.cat((org_attention_mask, pos_attention_mask), dim=0)\n",
    "            all_topics, _ = model(all_input_ids, all_attention_masks, return_topic=True)\n",
    "\n",
    "            orig_topic, pos_topic = torch.split(all_topics, len(all_topics) // 2)\n",
    "            pos_sim = torch.sum(orig_topic * pos_topic, dim=-1)\n",
    "\n",
    "            # consistency loss\n",
    "            consistency_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution matching loss\n",
    "            memory_queue = torch.cat((memory_queue.detach(), all_topics), dim=0)[all_topics.size(0):]\n",
    "            distmatch_loss = dist_match_loss(memory_queue, dirichlet_alpha_1)\n",
    "            loss = args.coeff_1_sim * consistency_loss + \\\n",
    "                   args.coeff_1_dist * distmatch_loss\n",
    "\n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(consistency_loss.item(), bsz)\n",
    "            dlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            tbar.set_description(\"Epoch-{} / consistency: {:.5f} - dist: {:.5f}\".format(epoch, \n",
    "                                                                                        closses.avg, \n",
    "                                                                                        dlosses.avg), refresh=True)\n",
    "            tbar.refresh()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            global_step += 1\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model_stage1_name = f'./trained_model/{args.dataset}_model_{bert_name_short}_stage1_{args.n_topic}t_{args.n_word}w_{epoch}e.ckpt'\n",
    "    torch.save(model.state_dict(), model_stage1_name)\n",
    "else:\n",
    "    model_stage1_name = args.stage_1_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: extract vocab set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del ema_model\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=768)\n",
    "model.cuda(gpu_ids[0])\n",
    "\n",
    "model.load_state_dict(torch.load(model_stage1_name), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007999897003173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 354,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45013e4374041c2b18ac0a880392917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "result_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm_notebook(memoryloader)):        \n",
    "        org_input, _, _, _ = batch\n",
    "        org_input_ids = org_input['input_ids'].to(gpu_ids[0])\n",
    "        org_attention_mask = org_input['attention_mask'].to(gpu_ids[0])\n",
    "        topic, embed = model(org_input_ids, org_attention_mask, return_topic = True)\n",
    "        result_list.append(topic)\n",
    "result_embedding = torch.cat(result_list)\n",
    "_, result_topic = torch.max(result_embedding, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': trainds.preprocess_ctm(trainds.nonempty_text), \n",
    "     'cluster_label': result_topic.cpu().numpy()}\n",
    "cluster_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_class = cluster_df.groupby(['cluster_label'], as_index=False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96352\n"
     ]
    }
   ],
   "source": [
    "supertxt = \"\"\n",
    "for idx, row in docs_per_class.iterrows():\n",
    "    supertxt = supertxt + row['text'] + \" \"\n",
    "wwwwwwwwwwwwww = supertxt.split()\n",
    "ssssssssssssss = set(wwwwwwwwwwwwww)\n",
    "print(len(ssssssssssssss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "# count_vectorizer = CountVectorizer()\n",
    "ctfidf_vectorizer = CTFIDFVectorizer()\n",
    "count = count_vectorizer.fit_transform(docs_per_class.text)\n",
    "ctfidf = ctfidf_vectorizer.fit_transform(count, n_samples=len(cluster_df)).toarray()\n",
    "words = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68756"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transport to gensim\n",
    "(gensim_corpus, gensim_dict) = vect2gensim(count_vectorizer, count)\n",
    "vocab_list = set(gensim_dict.token2id.keys())\n",
    "stopwords = set(line.strip() for line in open('stopwords_en.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "resolution_score = (ctfidf - np.min(ctfidf, axis=1, keepdims=True)) / (np.max(ctfidf, axis=1, keepdims=True) - np.min(ctfidf, axis=1, keepdims=True))\n",
    "\n",
    "n_word = args.n_word\n",
    "# n_topic_word = n_word / len(docs_per_class.cluster_label.index)\n",
    "n_topic_word = n_word\n",
    "n_topic_word = 15\n",
    "\n",
    "topic_word_dict = {}\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31: ['system', 'year', 'max', 'good', 'make', 'nntp', 'host', 'thing', 'problem', 'time', 'work', 'god', 'window', 'file', 'university'],\n"
     ]
    }
   ],
   "source": [
    "for key in topic_word_dict:\n",
    "    print(f\"{key}: {topic_word_dict[key]},\")\n",
    "topic_words_list = list(topic_word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0530316422131872\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "\n",
    "class WhiteSpacePreprocessing():\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
    "                      \"Use WhiteSpacePreprocessingStopwords.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "    \n",
    "def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):  \n",
    "    num_k = class_num\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "  \n",
    "    for c1 in range(0, num_k):\n",
    "        for c2 in range(0, num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "  \n",
    "    match = linear_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "  \n",
    "    return res\n",
    "\n",
    "def get_document_topic(topic_words, preprocessed_documents_lemmatized):\n",
    "    topic_words_flatten = list(itertools.chain.from_iterable(topic_words))\n",
    "    if '' in topic_words_flatten:\n",
    "        topic_words_flatten.remove('')\n",
    "    topic_words_flatten = list(set(topic_words_flatten))\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary = topic_words_flatten)\n",
    "    vectorizer = vectorizer.fit(preprocessed_documents_lemmatized)\n",
    "    count_mat = vectorizer.transform(preprocessed_documents_lemmatized).toarray()\n",
    "    \n",
    "    count_mat_normalized = count_mat + 1e-4\n",
    "    count_mat_normalized = count_mat_normalized / count_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat = vectorizer.transform([' '.join(i) for i in topic_words]).toarray()\n",
    "    topic_mat_normalized = topic_mat + 1e-4\n",
    "    topic_mat_normalized = topic_mat_normalized / topic_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat_inverse = topic_mat_normalized @ topic_mat_normalized.transpose()\n",
    "    topic_mat_inverse = np.linalg.inv(topic_mat_inverse)\n",
    "    topic_mat_inverse = topic_mat_normalized.transpose() @ topic_mat_inverse\n",
    "    document_topic = count_mat_normalized @ topic_mat_inverse\n",
    "    return document_topic\n",
    "\n",
    "class TopicModelDataPreparationNoNumber(TopicModelDataPreparation):\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, wordlist=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b', vocabulary=wordlist)\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(train_contextualized_embeddings, train_bow_embeddings, self.id2token, encoded_labels)\n",
    "    \n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "qt = TopicModelDataPreparationNoNumber(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sp = WhiteSpacePreprocessing(textData.data, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_documents_lemmatized = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()]) for doc in preprocessed_documents]\n",
    "\n",
    "document_topic = get_document_topic(topic_words_list, preprocessed_documents_lemmatized)\n",
    "train_target_filtered = textData.targets.squeeze()[retained_indices]\n",
    "flat_predict = torch.tensor(np.argmax(document_topic, axis=1))\n",
    "flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "num_samples = flat_predict.shape[0]\n",
    "match = _hungarian_match(flat_predict, flat_target, num_samples, 20)    \n",
    "reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "for pred_i, target_i in match:\n",
    "    reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(reordered_preds, flat_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "[-2.44295]\n",
      "-2.44295\n",
      "[-0.0561]\n",
      "-0.0561\n",
      "[-0.02944]\n",
      "-0.02944\n",
      "[-1.38054]\n",
      "-1.38054\n",
      "[0.33068]\n",
      "0.33068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic_N': 1,\n",
       " 'umass_wiki': -2.44295,\n",
       " 'npmi_wiki': -0.0561,\n",
       " 'uci_wiki': -1.38054,\n",
       " 'CV_wiki': 0.33068,\n",
       " 'cp_wiki': -0.02944,\n",
       " 'sim_w2v': 0.11448081420570776,\n",
       " 'diversity': 1.0,\n",
       " 'filename': 'results/240518_092208.txt'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "results = get_topic_qualities(topic_words_list, args.palmetto_dir, reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                              filename=f'results/{now}.txt')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "\n",
    "n_word = args.n_word\n",
    "n_topic_word = n_word\n",
    "\n",
    "words_to_idx = {k: v for v, k in enumerate(words)}\n",
    "topic_word_dict = {}\n",
    "topic_score_dict = {}\n",
    "total_score_cat = []\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    total_score_cat.append(total_score)\n",
    "    # topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    # topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]\n",
    "    topic_score_dict[docs_per_class.cluster_label.iloc[label]] = [total_score[words_to_idx[top_word]] for top_word in topic_word_list[:int(n_topic_word)]]\n",
    "total_score_cat = np.stack(total_score_cat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "topic_word_set = list(itertools.chain.from_iterable(pd.DataFrame.from_dict(topic_word_dict).values))\n",
    "word_candidates = remove_dup(topic_word_set)[:n_word]\n",
    "n_word = len(word_candidates)\n",
    "n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('our_word_candidates_10000.pkl', 'wb') as f:\n",
    "    pickle.dump(word_candidates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11696\\342046661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_candidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweight_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_score_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11696\\342046661.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_candidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweight_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_score_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "weight_candidates = {}\n",
    "for candidate in word_candidates:\n",
    "    weight_candidates[candidate] = [total_score_cat[label, words_to_idx[candidate]] for label in range(n_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cand_to_idx = {k: v for v, k in enumerate(list(weight_candidates.keys()))}\n",
    "weight_cand_matrix = np.array(list(weight_candidates.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, basesim_matrix, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        sim_weight, sim_indices = basesim_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2TestDataset(Dataset):\n",
    "    def __init__(self, encoder, ds, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embedding_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [00:04<00:00, 2677.24it/s]\n",
      "100%|██████████| 11314/11314 [00:56<00:00, 201.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, basesim_matrix, word_candidates, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_cands = torch.tensor(weight_cand_matrix.max(axis=1)).cuda(gpu_ids[0]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7532/7532 [00:03<00:00, 1923.28it/s]\n",
      "100%|██████████| 7532/7532 [00:02<00:00, 2711.94it/s]\n",
      "100%|██████████| 7532/7532 [00:37<00:00, 201.45it/s]\n"
     ]
    }
   ],
   "source": [
    "testds = BertDataset(bert=bert_name, text_list=textData.test_data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds2 = Stage2TestDataset(model.encoder, testds, word_candidates, lemmatize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.43686 - dist: 0.03966 - cons: -0.38214\n",
      "Epoch-1 / recon: 2.33547 - dist: 0.02696 - cons: -0.43764\n",
      "Epoch-2 / recon: 2.28356 - dist: 0.02287 - cons: -0.45926\n",
      "Epoch-3 / recon: 2.25368 - dist: 0.02088 - cons: -0.47212\n",
      "Epoch-4 / recon: 2.23464 - dist: 0.01971 - cons: -0.48005\n",
      "Epoch-5 / recon: 2.22149 - dist: 0.01898 - cons: -0.48574\n",
      "Epoch-6 / recon: 2.21182 - dist: 0.01851 - cons: -0.49027\n",
      "Epoch-7 / recon: 2.20444 - dist: 0.01809 - cons: -0.49365\n",
      "Epoch-8 / recon: 2.19860 - dist: 0.01782 - cons: -0.49630\n",
      "Epoch-9 / recon: 2.19384 - dist: 0.01763 - cons: -0.49855\n",
      "Epoch-10 / recon: 2.18992 - dist: 0.01746 - cons: -0.50058\n",
      "Epoch-11 / recon: 2.18665 - dist: 0.01735 - cons: -0.50233\n",
      "Epoch-12 / recon: 2.18385 - dist: 0.01722 - cons: -0.50381\n",
      "Epoch-13 / recon: 2.18144 - dist: 0.01714 - cons: -0.50522\n",
      "Epoch-14 / recon: 2.17932 - dist: 0.01705 - cons: -0.50644\n",
      "Epoch-15 / recon: 2.17748 - dist: 0.01698 - cons: -0.50757\n",
      "Epoch-16 / recon: 2.17583 - dist: 0.01691 - cons: -0.50852\n",
      "Epoch-17 / recon: 2.17436 - dist: 0.01686 - cons: -0.50935\n",
      "Epoch-18 / recon: 2.17305 - dist: 0.01683 - cons: -0.51023\n",
      "Epoch-19 / recon: 2.17187 - dist: 0.01679 - cons: -0.51095\n",
      "Epoch-20 / recon: 2.17080 - dist: 0.01675 - cons: -0.51168\n",
      "Epoch-21 / recon: 2.16982 - dist: 0.01675 - cons: -0.51237\n",
      "Epoch-22 / recon: 2.16891 - dist: 0.01672 - cons: -0.51297\n",
      "Epoch-23 / recon: 2.16809 - dist: 0.01670 - cons: -0.51357\n",
      "Epoch-24 / recon: 2.16732 - dist: 0.01669 - cons: -0.51420\n",
      "Epoch-25 / recon: 2.16661 - dist: 0.01667 - cons: -0.51471\n",
      "Epoch-26 / recon: 2.16596 - dist: 0.01665 - cons: -0.51524\n",
      "Epoch-27 / recon: 2.16536 - dist: 0.01663 - cons: -0.51570\n",
      "Epoch-28 / recon: 2.16479 - dist: 0.01662 - cons: -0.51615\n",
      "Epoch-29 / recon: 2.16425 - dist: 0.01662 - cons: -0.51661\n",
      "Epoch-30 / recon: 2.16375 - dist: 0.01660 - cons: -0.51703\n",
      "Epoch-31 / recon: 2.16327 - dist: 0.01659 - cons: -0.51746\n",
      "Epoch-32 / recon: 2.16284 - dist: 0.01658 - cons: -0.51781\n",
      "Epoch-33 / recon: 2.16241 - dist: 0.01657 - cons: -0.51815\n",
      "Epoch-34 / recon: 2.16202 - dist: 0.01656 - cons: -0.51848\n",
      "Epoch-35 / recon: 2.16164 - dist: 0.01654 - cons: -0.51876\n",
      "Epoch-36 / recon: 2.16129 - dist: 0.01653 - cons: -0.51909\n",
      "Epoch-37 / recon: 2.16095 - dist: 0.01653 - cons: -0.51940\n",
      "Epoch-38 / recon: 2.16062 - dist: 0.01652 - cons: -0.51971\n",
      "Epoch-39 / recon: 2.16031 - dist: 0.01650 - cons: -0.51997\n",
      "Epoch-40 / recon: 2.16003 - dist: 0.01650 - cons: -0.52023\n",
      "Epoch-41 / recon: 2.15976 - dist: 0.01650 - cons: -0.52046\n",
      "Epoch-42 / recon: 2.15949 - dist: 0.01650 - cons: -0.52070\n",
      "Epoch-43 / recon: 2.15924 - dist: 0.01651 - cons: -0.52096\n",
      "Epoch-44 / recon: 2.15899 - dist: 0.01650 - cons: -0.52119\n",
      "Epoch-45 / recon: 2.15876 - dist: 0.01650 - cons: -0.52142\n",
      "Epoch-46 / recon: 2.15855 - dist: 0.01650 - cons: -0.52166\n",
      "Epoch-47 / recon: 2.15833 - dist: 0.01650 - cons: -0.52188\n",
      "Epoch-48 / recon: 2.15811 - dist: 0.01650 - cons: -0.52210\n",
      "Epoch-49 / recon: 2.15791 - dist: 0.01650 - cons: -0.52234\n",
      "------- Evaluation results -------\n",
      "topic-0 ['drive', 'scsi', 'disk', 'organization', 'line', 'floppy', 'ide', 'hard', 'controller', 'mac', 'rom', 'problem', 'university', 'boot', 'host']\n",
      "topic-1 ['car', 'subject', 'line', 'organization', 'engine', 'article', 'writes', 'oil', 'dealer', 'posting', 'like', 'one', 'good', 'nntp', 'host']\n",
      "topic-2 ['subject', 'morality', 'writes', 'objective', 'line', 'organization', 'article', 'homosexual', 'people', 'keith', 'moral', 'gay', 'say', 'cramer', 'think']\n",
      "topic-3 ['key', 'clipper', 'chip', 'encryption', 'escrow', 'crypto', 'government', 'nsa', 'secret', 'system', 'line', 'algorithm', 'organization', 'secure', 'phone']\n",
      "topic-4 ['subject', 'line', 'modem', 'organization', 'detector', 'university', 'radar', 'circuit', 'audio', 'host', 'phone', 'posting', 'nntp', 'fax', 'use']\n",
      "topic-5 ['subject', 'line', 'organization', 'game', 'university', 'player', 'team', 'hockey', 'posting', 'host', 'nntp', 'baseball', 'article', 'nhl', 'writes']\n",
      "topic-6 ['subject', 'line', 'organization', 'university', 'posting', 'nntp', 'host', 'thanks', 'orbit', 'mail', 'moon', 'article', 'distribution', 'writes', 'one']\n",
      "topic-7 ['subject', 'line', 'sale', 'organization', 'graphic', 'posting', 'window', 'host', 'software', 'card', 'nntp', 'mail', 'university', 'thanks', 'package']\n",
      "topic-8 ['subject', 'driver', 'line', 'organization', 'msg', 'window', 'ftp', 'port', 'card', 'nntp', 'host', 'posting', 'keyboard', 'site', 'university']\n",
      "topic-9 ['gun', 'subject', 'firearm', 'weapon', 'people', 'line', 'organization', 'writes', 'right', 'article', 'handgun', 'fbi', 'batf', 'law', 'criminal']\n",
      "topic-10 ['max', 'god', 'atheist', 'christian', 'people', 'islam', 'one', 'jesus', 'believe', 'islamic', 'say', 'belief', 'church', 'faith', 'atheism']\n",
      "topic-11 ['bike', 'subject', 'dod', 'line', 'organization', 'motorcycle', 'article', 'dog', 'writes', 'ride', 'posting', 'nntp', 'host', 'helmet', 'rider']\n",
      "topic-12 ['israeli', 'israel', 'armenian', 'jew', 'arab', 'subject', 'turkish', 'armenia', 'palestinian', 'people', 'greek', 'writes', 'article', 'line', 'policy']\n",
      "topic-13 ['subject', 'god', 'christian', 'jesus', 'bible', 'line', 'church', 'christ', 'organization', 'writes', 'one', 'religion', 'article', 'people', 'christianity']\n",
      "topic-14 ['space', 'subject', 'organization', 'line', 'clinton', 'jewish', 'government', 'article', 'writes', 'president', 'nasa', 'launch', 'shuttle', 'tax', 'posting']\n",
      "topic-15 ['game', 'team', 'season', 'win', 'hockey', 'player', 'year', 'playoff', 'fan', 'play', 'detroit', 'wing', 'line', 'devil', 'pitching']\n",
      "topic-16 ['window', 'subject', 'file', 'line', 'organization', 'program', 'manager', 'use', 'application', 'host', 'problem', 'nntp', 'posting', 'university', 'server']\n",
      "topic-17 ['card', 'line', 'bus', 'chip', 'organization', 'ram', 'simms', 'motherboard', 'board', 'speed', 'video', 'memory', 'slot', 'eisa', 'vram']\n",
      "topic-18 ['subject', 'gordon', 'bank', 'organization', 'article', 'line', 'writes', 'doctor', 'disease', 'patient', 'food', 'medical', 'pittsburgh', 'one', 'chastity']\n",
      "topic-19 ['subject', 'font', 'monitor', 'window', 'image', 'printer', 'widget', 'line', 'color', 'motif', 'graphic', 'gif', 'mouse', 'organization', 'vga']\n",
      "./\n",
      "[-2.66497, -3.26815, -2.4043, -2.67092, -4.7997, -3.84636, -3.08821, -3.74033, -4.01606, -2.77793, -2.38409, -4.44936, -1.81784, -1.54714, -2.82267, -2.67084, -3.31201, -4.55378, -2.49093, -3.72492]\n",
      "-3.1525255\n",
      "[0.05516, -0.08059, -0.01903, 0.01387, -0.08338, -0.059, -0.08848, -0.07212, -0.13225, 0.01444, 0.09434, -0.07572, 0.05181, 0.06432, -0.04937, 0.05864, -0.05368, -0.00599, -0.04172, -0.02375]\n",
      "-0.021625\n",
      "[0.31068, -0.1293, -0.02947, 0.2453, -0.05658, -0.01461, -0.09981, 0.00664, -0.12768, 0.07712, 0.43859, -0.16594, 0.24872, 0.20867, -0.01523, 0.36038, 0.12207, 0.28896, -0.08486, 0.24754]\n",
      "0.0915595\n",
      "[0.17609, -2.14401, -0.92908, -0.62044, -2.5834, -2.00107, -2.24242, -2.10576, -3.53427, -0.26498, 0.84338, -2.39409, 0.45632, 0.65022, -1.67316, 0.11042, -1.83908, -1.2308, -1.18515, -1.48989]\n",
      "-1.2000584999999995\n",
      "[0.41784, 0.32592, 0.31918, 0.37613, 0.40086, 0.41827, 0.33934, 0.37959, 0.38606, 0.37852, 0.39801, 0.4215, 0.36825, 0.38269, 0.3178, 0.43141, 0.35437, 0.40871, 0.29888, 0.38929]\n",
      "0.37563099999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 199.11it/s]\n",
      "59it [00:00, 280.96it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 267.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 20, 'umass_wiki': -3.1525255, 'npmi_wiki': -0.021625, 'uci_wiki': -1.2000584999999995, 'CV_wiki': 0.37563099999999994, 'cp_wiki': 0.0915595, 'sim_w2v': 0.13078458368007323, 'diversity': 0.5833333333333334, 'filename': 'results/240517_170500.txt', 'acc': 0.511019649495486, 'macro-F1': 0.4789935479199131, 'Purity': 0.5077004779607011, 'NMI': 0.5214715428181115, 'label_match': 0.5007070885628425}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.43808 - dist: 0.04732 - cons: -0.35132\n",
      "Epoch-1 / recon: 2.34121 - dist: 0.03673 - cons: -0.38693\n",
      "Epoch-2 / recon: 2.29146 - dist: 0.03206 - cons: -0.40453\n",
      "Epoch-3 / recon: 2.26234 - dist: 0.02869 - cons: -0.41860\n",
      "Epoch-4 / recon: 2.24314 - dist: 0.02647 - cons: -0.42878\n",
      "Epoch-5 / recon: 2.22972 - dist: 0.02496 - cons: -0.43675\n",
      "Epoch-6 / recon: 2.21980 - dist: 0.02386 - cons: -0.44337\n",
      "Epoch-7 / recon: 2.21214 - dist: 0.02303 - cons: -0.44917\n",
      "Epoch-8 / recon: 2.20619 - dist: 0.02234 - cons: -0.45422\n",
      "Epoch-9 / recon: 2.20118 - dist: 0.02176 - cons: -0.45889\n",
      "Epoch-10 / recon: 2.19697 - dist: 0.02130 - cons: -0.46292\n",
      "Epoch-11 / recon: 2.19337 - dist: 0.02088 - cons: -0.46637\n",
      "Epoch-12 / recon: 2.19026 - dist: 0.02056 - cons: -0.46951\n",
      "Epoch-13 / recon: 2.18754 - dist: 0.02028 - cons: -0.47230\n",
      "Epoch-14 / recon: 2.18517 - dist: 0.02003 - cons: -0.47480\n",
      "Epoch-15 / recon: 2.18310 - dist: 0.01983 - cons: -0.47696\n",
      "Epoch-16 / recon: 2.18125 - dist: 0.01968 - cons: -0.47895\n",
      "Epoch-17 / recon: 2.17960 - dist: 0.01953 - cons: -0.48065\n",
      "Epoch-18 / recon: 2.17811 - dist: 0.01938 - cons: -0.48223\n",
      "Epoch-19 / recon: 2.17679 - dist: 0.01923 - cons: -0.48368\n",
      "Epoch-20 / recon: 2.17555 - dist: 0.01911 - cons: -0.48501\n",
      "Epoch-21 / recon: 2.17443 - dist: 0.01901 - cons: -0.48627\n",
      "Epoch-22 / recon: 2.17340 - dist: 0.01893 - cons: -0.48746\n",
      "Epoch-23 / recon: 2.17246 - dist: 0.01885 - cons: -0.48848\n",
      "Epoch-24 / recon: 2.17160 - dist: 0.01878 - cons: -0.48946\n",
      "Epoch-25 / recon: 2.17079 - dist: 0.01871 - cons: -0.49038\n",
      "Epoch-26 / recon: 2.17004 - dist: 0.01863 - cons: -0.49129\n",
      "Epoch-27 / recon: 2.16934 - dist: 0.01858 - cons: -0.49210\n",
      "Epoch-28 / recon: 2.16868 - dist: 0.01852 - cons: -0.49290\n",
      "Epoch-29 / recon: 2.16806 - dist: 0.01846 - cons: -0.49370\n",
      "Epoch-30 / recon: 2.16749 - dist: 0.01841 - cons: -0.49444\n",
      "Epoch-31 / recon: 2.16695 - dist: 0.01837 - cons: -0.49517\n",
      "Epoch-32 / recon: 2.16645 - dist: 0.01833 - cons: -0.49583\n",
      "Epoch-33 / recon: 2.16596 - dist: 0.01828 - cons: -0.49642\n",
      "Epoch-34 / recon: 2.16550 - dist: 0.01825 - cons: -0.49705\n",
      "Epoch-35 / recon: 2.16504 - dist: 0.01821 - cons: -0.49765\n",
      "Epoch-36 / recon: 2.16460 - dist: 0.01817 - cons: -0.49823\n",
      "Epoch-37 / recon: 2.16419 - dist: 0.01815 - cons: -0.49877\n",
      "Epoch-38 / recon: 2.16382 - dist: 0.01813 - cons: -0.49928\n",
      "Epoch-39 / recon: 2.16345 - dist: 0.01810 - cons: -0.49982\n",
      "Epoch-40 / recon: 2.16309 - dist: 0.01808 - cons: -0.50034\n",
      "Epoch-41 / recon: 2.16276 - dist: 0.01805 - cons: -0.50079\n",
      "Epoch-42 / recon: 2.16243 - dist: 0.01803 - cons: -0.50125\n",
      "Epoch-43 / recon: 2.16213 - dist: 0.01801 - cons: -0.50168\n",
      "Epoch-44 / recon: 2.16183 - dist: 0.01800 - cons: -0.50214\n",
      "Epoch-45 / recon: 2.16154 - dist: 0.01798 - cons: -0.50255\n",
      "Epoch-46 / recon: 2.16127 - dist: 0.01797 - cons: -0.50293\n",
      "Epoch-47 / recon: 2.16102 - dist: 0.01796 - cons: -0.50330\n",
      "Epoch-48 / recon: 2.16077 - dist: 0.01794 - cons: -0.50367\n",
      "Epoch-49 / recon: 2.16052 - dist: 0.01792 - cons: -0.50406\n",
      "------- Evaluation results -------\n",
      "topic-0 ['israeli', 'israel', 'armenian', 'jew', 'arab', 'subject', 'turkish', 'armenia', 'palestinian', 'writes', 'people', 'article', 'greek', 'jewish', 'line']\n",
      "topic-1 ['subject', 'line', 'organization', 'game', 'university', 'baseball', 'jewish', 'posting', 'host', 'nntp', 'article', 'ticket', 'player', 'thanks', 'writes']\n",
      "topic-2 ['max', 'god', 'christian', 'jesus', 'church', 'bible', 'christ', 'faith', 'sin', 'christianity', 'one', 'people', 'religion', 'scripture', 'say']\n",
      "topic-3 ['subject', 'gordon', 'bank', 'article', 'organization', 'line', 'writes', 'disease', 'doctor', 'drug', 'patient', 'food', 'medical', 'chastity', 'skepticism']\n",
      "topic-4 ['subject', 'line', 'organization', 'sale', 'graphic', 'file', 'window', 'posting', 'host', 'nntp', 'software', 'university', 'thanks', 'mail', 'package']\n",
      "topic-5 ['gun', 'subject', 'firearm', 'weapon', 'people', 'line', 'organization', 'right', 'law', 'writes', 'article', 'handgun', 'criminal', 'one', 'like']\n",
      "topic-6 ['card', 'line', 'organization', 'bus', 'chip', 'ram', 'video', 'simms', 'motherboard', 'board', 'memory', 'speed', 'slot', 'eisa', 'vram']\n",
      "topic-7 ['subject', 'line', 'organization', 'clinton', 'government', 'article', 'writes', 'fbi', 'posting', 'president', 'nntp', 'host', 'tax', 'people', 'waco']\n",
      "topic-8 ['key', 'clipper', 'chip', 'encryption', 'escrow', 'crypto', 'nsa', 'government', 'line', 'secret', 'system', 'algorithm', 'organization', 'secure', 'phone']\n",
      "topic-9 ['drive', 'scsi', 'subject', 'disk', 'line', 'organization', 'floppy', 'hard', 'ide', 'controller', 'mac', 'rom', 'problem', 'university', 'boot']\n",
      "topic-10 ['team', 'player', 'game', 'subject', 'year', 'hockey', 'play', 'nhl', 'season', 'writes', 'league', 'line', 'organization', 'article', 'win']\n",
      "topic-11 ['space', 'subject', 'moon', 'line', 'organization', 'nasa', 'article', 'writes', 'orbit', 'launch', 'lunar', 'pat', 'shuttle', 'one', 'henry']\n",
      "topic-12 ['window', 'driver', 'file', 'msg', 'line', 'organization', 'manager', 'keyboard', 'port', 'ftp', 'program', 'use', 'version', 'card', 'mouse']\n",
      "topic-13 ['subject', 'atheist', 'morality', 'god', 'writes', 'objective', 'people', 'line', 'homosexual', 'organization', 'article', 'moral', 'keith', 'law', 'islamic']\n",
      "topic-14 ['subject', 'line', 'modem', 'organization', 'university', 'circuit', 'detector', 'radar', 'host', 'phone', 'posting', 'nntp', 'audio', 'use', 'one']\n",
      "topic-15 ['game', 'team', 'line', 'organization', 'win', 'season', 'university', 'writes', 'fan', 'detroit', 'playoff', 'hockey', 'devil', 'year', 'espn']\n",
      "topic-16 ['subject', 'line', 'organization', 'university', 'posting', 'nntp', 'host', 'thanks', 'mail', 'distribution', 'please', 'reply', 'article', 'science', 'internet']\n",
      "topic-17 ['bike', 'subject', 'dod', 'line', 'motorcycle', 'organization', 'article', 'dog', 'writes', 'ride', 'posting', 'bmw', 'nntp', 'host', 'helmet']\n",
      "topic-18 ['subject', 'font', 'monitor', 'image', 'window', 'color', 'widget', 'printer', 'line', 'motif', 'graphic', 'organization', 'gif', 'vga', 'mouse']\n",
      "topic-19 ['car', 'subject', 'line', 'organization', 'engine', 'article', 'writes', 'oil', 'dealer', 'posting', 'one', 'like', 'price', 'good', 'new']\n",
      "./\n",
      "[-1.82601, -3.91319, -1.87839, -2.86814, -3.73491, -1.59801, -4.68877, -4.05574, -2.63201, -2.91008, -1.60341, -2.33728, -2.90835, -2.08744, -4.09787, -2.20505, -3.48958, -4.7643, -3.78873, -1.79451]\n",
      "-2.9590885000000005\n",
      "[0.06439, -0.0827, 0.12609, -0.05337, -0.07419, 0.02964, -0.00599, -0.07716, 0.01387, 0.03458, 0.0536, 0.00854, -0.03068, 0.00894, -0.0812, 0.04175, -0.07483, -0.09116, -0.02375, -0.03703]\n",
      "-0.012532999999999999\n",
      "[0.26095, -0.09565, 0.48886, -0.02609, 0.03859, 0.09121, 0.28913, -0.09533, 0.24482, 0.26293, 0.14219, 0.00763, 0.11592, 0.09154, -0.05555, 0.18844, -0.00163, -0.18966, 0.24752, -0.0486]\n",
      "0.09786100000000002\n",
      "[0.58272, -2.17348, 1.23708, -1.77668, -2.14258, 0.33135, -1.2308, -2.18973, -0.62044, -0.32491, 0.31164, -0.30321, -1.2164, -0.29809, -2.35664, 0.37633, -2.10071, -2.72812, -1.48989, -1.03999]\n",
      "-0.9576275000000001\n",
      "[0.3968, 0.3687, 0.46976, 0.30888, 0.37638, 0.30415, 0.40871, 0.35356, 0.37613, 0.42583, 0.42432, 0.34856, 0.31346, 0.29378, 0.36916, 0.36376, 0.3553, 0.40739, 0.38929, 0.27329]\n",
      "0.3663605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 194.31it/s]\n",
      "59it [00:00, 268.18it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 295.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 20, 'umass_wiki': -2.9590885000000005, 'npmi_wiki': -0.012532999999999999, 'uci_wiki': -0.9576275000000001, 'CV_wiki': 0.3663605, 'cp_wiki': 0.09786100000000002, 'sim_w2v': 0.13125200410931456, 'diversity': 0.59, 'filename': 'results/240517_170922.txt', 'acc': 0.5045140732873075, 'macro-F1': 0.4862578630156074, 'Purity': 0.5144715878916623, 'NMI': 0.5205239386579498, 'label_match': 0.530051263920806}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.43675 - dist: 0.03977 - cons: -0.36888\n",
      "Epoch-1 / recon: 2.33802 - dist: 0.02916 - cons: -0.41194\n",
      "Epoch-2 / recon: 2.28706 - dist: 0.02529 - cons: -0.43212\n",
      "Epoch-3 / recon: 2.25737 - dist: 0.02331 - cons: -0.44485\n",
      "Epoch-4 / recon: 2.23827 - dist: 0.02227 - cons: -0.45305\n",
      "Epoch-5 / recon: 2.22499 - dist: 0.02153 - cons: -0.45919\n",
      "Epoch-6 / recon: 2.21529 - dist: 0.02100 - cons: -0.46394\n",
      "Epoch-7 / recon: 2.20781 - dist: 0.02054 - cons: -0.46746\n",
      "Epoch-8 / recon: 2.20183 - dist: 0.02026 - cons: -0.47032\n",
      "Epoch-9 / recon: 2.19697 - dist: 0.02000 - cons: -0.47260\n",
      "Epoch-10 / recon: 2.19297 - dist: 0.01973 - cons: -0.47466\n",
      "Epoch-11 / recon: 2.18981 - dist: 0.01943 - cons: -0.47693\n",
      "Epoch-12 / recon: 2.18698 - dist: 0.01915 - cons: -0.47935\n",
      "Epoch-13 / recon: 2.18447 - dist: 0.01893 - cons: -0.48164\n",
      "Epoch-14 / recon: 2.18225 - dist: 0.01874 - cons: -0.48389\n",
      "Epoch-15 / recon: 2.18032 - dist: 0.01855 - cons: -0.48583\n",
      "Epoch-16 / recon: 2.17858 - dist: 0.01840 - cons: -0.48762\n",
      "Epoch-17 / recon: 2.17701 - dist: 0.01825 - cons: -0.48928\n",
      "Epoch-18 / recon: 2.17560 - dist: 0.01814 - cons: -0.49081\n",
      "Epoch-19 / recon: 2.17433 - dist: 0.01801 - cons: -0.49215\n",
      "Epoch-20 / recon: 2.17315 - dist: 0.01791 - cons: -0.49348\n",
      "Epoch-21 / recon: 2.17212 - dist: 0.01782 - cons: -0.49468\n",
      "Epoch-22 / recon: 2.17113 - dist: 0.01775 - cons: -0.49580\n",
      "Epoch-23 / recon: 2.17024 - dist: 0.01768 - cons: -0.49684\n",
      "Epoch-24 / recon: 2.16941 - dist: 0.01762 - cons: -0.49784\n",
      "Epoch-25 / recon: 2.16865 - dist: 0.01755 - cons: -0.49877\n",
      "Epoch-26 / recon: 2.16794 - dist: 0.01749 - cons: -0.49969\n",
      "Epoch-27 / recon: 2.16728 - dist: 0.01743 - cons: -0.50052\n",
      "Epoch-28 / recon: 2.16666 - dist: 0.01737 - cons: -0.50125\n",
      "Epoch-29 / recon: 2.16608 - dist: 0.01732 - cons: -0.50201\n",
      "Epoch-30 / recon: 2.16553 - dist: 0.01729 - cons: -0.50275\n",
      "Epoch-31 / recon: 2.16502 - dist: 0.01725 - cons: -0.50340\n",
      "Epoch-32 / recon: 2.16456 - dist: 0.01720 - cons: -0.50400\n",
      "Epoch-33 / recon: 2.16411 - dist: 0.01716 - cons: -0.50456\n",
      "Epoch-34 / recon: 2.16368 - dist: 0.01714 - cons: -0.50516\n",
      "Epoch-35 / recon: 2.16329 - dist: 0.01712 - cons: -0.50569\n",
      "Epoch-36 / recon: 2.16289 - dist: 0.01710 - cons: -0.50624\n",
      "Epoch-37 / recon: 2.16254 - dist: 0.01709 - cons: -0.50676\n",
      "Epoch-38 / recon: 2.16220 - dist: 0.01707 - cons: -0.50726\n",
      "Epoch-39 / recon: 2.16187 - dist: 0.01704 - cons: -0.50770\n",
      "Epoch-40 / recon: 2.16155 - dist: 0.01702 - cons: -0.50815\n",
      "Epoch-41 / recon: 2.16125 - dist: 0.01701 - cons: -0.50856\n",
      "Epoch-42 / recon: 2.16096 - dist: 0.01700 - cons: -0.50900\n",
      "Epoch-43 / recon: 2.16069 - dist: 0.01699 - cons: -0.50940\n",
      "Epoch-44 / recon: 2.16042 - dist: 0.01698 - cons: -0.50977\n",
      "Epoch-45 / recon: 2.16017 - dist: 0.01697 - cons: -0.51013\n",
      "Epoch-46 / recon: 2.15993 - dist: 0.01695 - cons: -0.51049\n",
      "Epoch-47 / recon: 2.15970 - dist: 0.01695 - cons: -0.51085\n",
      "Epoch-48 / recon: 2.15948 - dist: 0.01693 - cons: -0.51117\n",
      "Epoch-49 / recon: 2.15926 - dist: 0.01692 - cons: -0.51151\n",
      "------- Evaluation results -------\n",
      "topic-0 ['space', 'subject', 'moon', 'line', 'organization', 'nasa', 'article', 'writes', 'launch', 'orbit', 'lunar', 'pat', 'shuttle', 'one', 'satellite']\n",
      "topic-1 ['subject', 'clinton', 'line', 'government', 'organization', 'article', 'writes', 'jewish', 'president', 'fbi', 'tax', 'waco', 'posting', 'people', 'nntp']\n",
      "topic-2 ['subject', 'line', 'organization', 'sale', 'file', 'graphic', 'window', 'posting', 'host', 'nntp', 'software', 'university', 'thanks', 'mail', 'card']\n",
      "topic-3 ['max', 'god', 'christian', 'jesus', 'church', 'bible', 'christ', 'faith', 'christianity', 'sin', 'one', 'people', 'religion', 'scripture', 'say']\n",
      "topic-4 ['key', 'clipper', 'chip', 'encryption', 'escrow', 'government', 'crypto', 'nsa', 'line', 'secret', 'algorithm', 'system', 'organization', 'secure', 'phone']\n",
      "topic-5 ['subject', 'card', 'line', 'organization', 'video', 'motherboard', 'drive', 'chip', 'board', 'apple', 'university', 'battery', 'system', 'mac', 'posting']\n",
      "topic-6 ['subject', 'line', 'font', 'window', 'color', 'graphic', 'organization', 'widget', 'motif', 'mouse', 'file', 'gif', 'thanks', 'host', 'xterm']\n",
      "topic-7 ['subject', 'game', 'line', 'organization', 'player', 'team', 'university', 'hockey', 'baseball', 'article', 'nhl', 'writes', 'posting', 'host', 'nntp']\n",
      "topic-8 ['subject', 'atheist', 'morality', 'god', 'objective', 'writes', 'people', 'homosexual', 'line', 'moral', 'keith', 'organization', 'article', 'islamic', 'law']\n",
      "topic-9 ['subject', 'line', 'organization', 'university', 'posting', 'nntp', 'host', 'mail', 'thanks', 'please', 'distribution', 'reply', 'internet', 'address', 'science']\n",
      "topic-10 ['subject', 'gordon', 'bank', 'article', 'organization', 'line', 'writes', 'disease', 'doctor', 'drug', 'patient', 'food', 'medical', 'chastity', 'skepticism']\n",
      "topic-11 ['window', 'driver', 'file', 'msg', 'line', 'organization', 'manager', 'ftp', 'keyboard', 'port', 'program', 'use', 'version', 'card', 'mouse']\n",
      "topic-12 ['car', 'subject', 'line', 'organization', 'article', 'writes', 'engine', 'oil', 'dealer', 'posting', 'like', 'one', 'saturn', 'price', 'good']\n",
      "topic-13 ['drive', 'scsi', 'disk', 'ide', 'controller', 'organization', 'bus', 'line', 'floppy', 'hard', 'simms', 'memory', 'ram', 'mac', 'problem']\n",
      "topic-14 ['bike', 'subject', 'dod', 'line', 'organization', 'motorcycle', 'article', 'bmw', 'writes', 'posting', 'dog', 'ride', 'nntp', 'host', 'helmet']\n",
      "topic-15 ['game', 'team', 'win', 'season', 'hockey', 'year', 'playoff', 'player', 'fan', 'play', 'detroit', 'line', 'devil', 'organization', 'wing']\n",
      "topic-16 ['israeli', 'israel', 'jew', 'subject', 'arab', 'writes', 'article', 'organization', 'palestinian', 'line', 'jewish', 'people', 'policy', 'peace', 'one']\n",
      "topic-17 ['monitor', 'line', 'printer', 'organization', 'image', 'vga', 'university', 'video', 'posting', 'screen', 'nntp', 'host', 'one', 'problem', 'article']\n",
      "topic-18 ['subject', 'line', 'organization', 'modem', 'university', 'circuit', 'posting', 'host', 'detector', 'phone', 'radar', 'nntp', 'fax', 'audio', 'one']\n",
      "topic-19 ['gun', 'armenian', 'subject', 'weapon', 'firearm', 'people', 'turkish', 'armenia', 'handgun', 'line', 'law', 'right', 'criminal', 'one', 'writes']\n",
      "./\n",
      "[-2.38689, -4.55058, -3.72612, -1.88013, -2.71483, -3.33823, -4.8302, -4.09883, -1.95698, -3.54029, -2.86814, -2.8432, -2.45067, -2.93418, -4.67953, -2.25472, -1.42298, -3.60277, -4.76197, -1.99663]\n",
      "-3.1418935\n",
      "[0.03403, -0.07096, -0.08009, 0.12609, 0.01387, -0.02393, -0.0711, -0.059, 0.00894, -0.07006, -0.05337, -0.03068, -0.05288, 0.05479, -0.09116, 0.07137, 0.06514, -0.08059, -0.09554, 0.00127]\n",
      "-0.020193000000000003\n",
      "[0.09233, -0.09589, 0.02558, 0.48887, 0.24488, 0.07968, 0.03206, -0.04294, 0.10085, -0.0055, -0.02609, 0.11581, -0.07084, 0.38353, -0.18972, 0.32584, 0.1815, 0.00457, -0.08903, 0.07361]\n",
      "0.081455\n",
      "[-1e-05, -2.11053, -2.21995, 1.23708, -0.62044, -0.98442, -2.31373, -2.00107, -0.29809, -2.03324, -1.77668, -1.2164, -1.53505, -0.09694, -2.72812, 0.6087, 0.7074, -2.28388, -2.78349, -0.48742]\n",
      "-1.1468140000000002\n",
      "[0.36964, 0.36065, 0.36545, 0.46976, 0.37613, 0.36068, 0.38764, 0.41827, 0.29378, 0.36319, 0.30888, 0.31346, 0.28168, 0.41269, 0.40739, 0.40897, 0.38081, 0.36693, 0.39825, 0.34572]\n",
      "0.36949850000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 200.47it/s]\n",
      "59it [00:00, 296.48it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 275.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 20, 'umass_wiki': -3.1418935, 'npmi_wiki': -0.020193000000000003, 'uci_wiki': -1.1468140000000002, 'CV_wiki': 0.36949850000000006, 'cp_wiki': 0.081455, 'sim_w2v': 0.12837664890615214, 'diversity': 0.5833333333333334, 'filename': 'results/240517_171349.txt', 'acc': 0.5104885820499203, 'macro-F1': 0.5002090664630988, 'Purity': 0.5142060541688794, 'NMI': 0.531561800346321, 'label_match': 0.5317305992575571}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.43578 - dist: 0.03914 - cons: -0.38100\n",
      "Epoch-1 / recon: 2.33550 - dist: 0.02798 - cons: -0.42785\n",
      "Epoch-2 / recon: 2.28429 - dist: 0.02457 - cons: -0.44632\n",
      "Epoch-3 / recon: 2.25480 - dist: 0.02288 - cons: -0.45725\n",
      "Epoch-4 / recon: 2.23600 - dist: 0.02172 - cons: -0.46440\n",
      "Epoch-5 / recon: 2.22299 - dist: 0.02095 - cons: -0.46977\n",
      "Epoch-6 / recon: 2.21342 - dist: 0.02050 - cons: -0.47375\n",
      "Epoch-7 / recon: 2.20601 - dist: 0.02020 - cons: -0.47700\n",
      "Epoch-8 / recon: 2.20013 - dist: 0.01987 - cons: -0.47973\n",
      "Epoch-9 / recon: 2.19532 - dist: 0.01968 - cons: -0.48195\n",
      "Epoch-10 / recon: 2.19133 - dist: 0.01946 - cons: -0.48389\n",
      "Epoch-11 / recon: 2.18797 - dist: 0.01926 - cons: -0.48562\n",
      "Epoch-12 / recon: 2.18509 - dist: 0.01913 - cons: -0.48719\n",
      "Epoch-13 / recon: 2.18257 - dist: 0.01902 - cons: -0.48860\n",
      "Epoch-14 / recon: 2.18038 - dist: 0.01893 - cons: -0.48982\n",
      "Epoch-15 / recon: 2.17842 - dist: 0.01888 - cons: -0.49097\n",
      "Epoch-16 / recon: 2.17669 - dist: 0.01882 - cons: -0.49203\n",
      "Epoch-17 / recon: 2.17512 - dist: 0.01876 - cons: -0.49300\n",
      "Epoch-18 / recon: 2.17371 - dist: 0.01871 - cons: -0.49386\n",
      "Epoch-19 / recon: 2.17242 - dist: 0.01865 - cons: -0.49464\n",
      "Epoch-20 / recon: 2.17124 - dist: 0.01861 - cons: -0.49539\n",
      "Epoch-21 / recon: 2.17016 - dist: 0.01856 - cons: -0.49612\n",
      "Epoch-22 / recon: 2.16916 - dist: 0.01853 - cons: -0.49673\n",
      "Epoch-23 / recon: 2.16825 - dist: 0.01849 - cons: -0.49730\n",
      "Epoch-24 / recon: 2.16740 - dist: 0.01845 - cons: -0.49790\n",
      "Epoch-25 / recon: 2.16661 - dist: 0.01843 - cons: -0.49841\n",
      "Epoch-26 / recon: 2.16588 - dist: 0.01840 - cons: -0.49890\n",
      "Epoch-27 / recon: 2.16519 - dist: 0.01838 - cons: -0.49940\n",
      "Epoch-28 / recon: 2.16454 - dist: 0.01836 - cons: -0.49987\n",
      "Epoch-29 / recon: 2.16393 - dist: 0.01834 - cons: -0.50028\n",
      "Epoch-30 / recon: 2.16336 - dist: 0.01832 - cons: -0.50070\n",
      "Epoch-31 / recon: 2.16281 - dist: 0.01831 - cons: -0.50108\n",
      "Epoch-32 / recon: 2.16229 - dist: 0.01831 - cons: -0.50146\n",
      "Epoch-33 / recon: 2.16180 - dist: 0.01830 - cons: -0.50183\n",
      "Epoch-34 / recon: 2.16133 - dist: 0.01830 - cons: -0.50219\n",
      "Epoch-35 / recon: 2.16089 - dist: 0.01828 - cons: -0.50253\n",
      "Epoch-36 / recon: 2.16047 - dist: 0.01826 - cons: -0.50284\n",
      "Epoch-37 / recon: 2.16008 - dist: 0.01826 - cons: -0.50314\n",
      "Epoch-38 / recon: 2.15969 - dist: 0.01826 - cons: -0.50343\n",
      "Epoch-39 / recon: 2.15933 - dist: 0.01826 - cons: -0.50372\n",
      "Epoch-40 / recon: 2.15898 - dist: 0.01824 - cons: -0.50400\n",
      "Epoch-41 / recon: 2.15865 - dist: 0.01825 - cons: -0.50425\n",
      "Epoch-42 / recon: 2.15833 - dist: 0.01825 - cons: -0.50452\n",
      "Epoch-43 / recon: 2.15801 - dist: 0.01824 - cons: -0.50474\n",
      "Epoch-44 / recon: 2.15771 - dist: 0.01823 - cons: -0.50500\n",
      "Epoch-45 / recon: 2.15742 - dist: 0.01823 - cons: -0.50526\n",
      "Epoch-46 / recon: 2.15715 - dist: 0.01822 - cons: -0.50550\n",
      "Epoch-47 / recon: 2.15689 - dist: 0.01821 - cons: -0.50571\n",
      "Epoch-48 / recon: 2.15664 - dist: 0.01822 - cons: -0.50594\n",
      "Epoch-49 / recon: 2.15639 - dist: 0.01821 - cons: -0.50615\n",
      "------- Evaluation results -------\n",
      "topic-0 ['bike', 'car', 'subject', 'dod', 'motorcycle', 'engine', 'line', 'organization', 'oil', 'honda', 'article', 'bmw', 'writes', 'helmet', 'ride']\n",
      "topic-1 ['line', 'organization', 'subject', 'sale', 'university', 'posting', 'host', 'nntp', 'circuit', 'phone', 'one', 'distribution', 'sound', 'number', 'offer']\n",
      "topic-2 ['window', 'file', 'line', 'organization', 'subject', 'program', 'use', 'manager', 'host', 'university', 'nntp', 'posting', 'problem', 'application', 'know']\n",
      "topic-3 ['gordon', 'bank', 'subject', 'article', 'organization', 'line', 'writes', 'disease', 'doctor', 'drug', 'patient', 'food', 'one', 'medical', 'chastity']\n",
      "topic-4 ['israeli', 'israel', 'jew', 'arab', 'writes', 'subject', 'organization', 'article', 'line', 'palestinian', 'one', 'people', 'university', 'jewish', 'policy']\n",
      "topic-5 ['atheist', 'morality', 'writes', 'line', 'subject', 'organization', 'objective', 'article', 'people', 'god', 'one', 'say', 'keith', 'moral', 'law']\n",
      "topic-6 ['line', 'organization', 'subject', 'window', 'font', 'widget', 'color', 'motif', 'mouse', 'host', 'graphic', 'posting', 'nntp', 'point', 'gif']\n",
      "topic-7 ['monitor', 'subject', 'line', 'organization', 'printer', 'image', 'university', 'vga', 'posting', 'video', 'one', 'nntp', 'host', 'article', 'problem']\n",
      "topic-8 ['key', 'clipper', 'chip', 'encryption', 'escrow', 'line', 'organization', 'modem', 'government', 'crypto', 'nsa', 'system', 'one', 'writes', 'secret']\n",
      "topic-9 ['driver', 'subject', 'line', 'organization', 'msg', 'window', 'nntp', 'host', 'posting', 'ftp', 'port', 'card', 'keyboard', 'university', 'article']\n",
      "topic-10 ['space', 'subject', 'line', 'moon', 'organization', 'article', 'writes', 'orbit', 'nasa', 'launch', 'one', 'lunar', 'posting', 'pat', 'nntp']\n",
      "topic-11 ['line', 'subject', 'organization', 'university', 'posting', 'nntp', 'host', 'mail', 'thanks', 'reply', 'distribution', 'please', 'article', 'internet', 'one']\n",
      "topic-12 ['card', 'line', 'organization', 'bus', 'chip', 'ram', 'simms', 'motherboard', 'board', 'video', 'memory', 'speed', 'slot', 'university', 'eisa']\n",
      "topic-13 ['subject', 'line', 'organization', 'clinton', 'article', 'writes', 'jewish', 'game', 'government', 'president', 'posting', 'university', 'host', 'fbi', 'nntp']\n",
      "topic-14 ['graphic', 'god', 'christian', 'sale', 'bible', 'encryption', 'religion', 'jesus', 'christianity', 'hockey', 'window', 'algorithm', 'package', 'team', 'modem']\n",
      "topic-15 ['game', 'line', 'team', 'organization', 'subject', 'win', 'writes', 'university', 'article', 'season', 'fan', 'posting', 'nntp', 'host', 'year']\n",
      "topic-16 ['team', 'player', 'year', 'game', 'hockey', 'line', 'nhl', 'organization', 'writes', 'play', 'article', 'season', 'think', 'league', 'like']\n",
      "topic-17 ['max', 'god', 'jesus', 'christian', 'church', 'one', 'bible', 'christ', 'line', 'people', 'writes', 'say', 'organization', 'faith', 'sin']\n",
      "topic-18 ['gun', 'armenian', 'people', 'weapon', 'firearm', 'subject', 'turkish', 'line', 'armenia', 'one', 'writes', 'article', 'right', 'organization', 'like']\n",
      "topic-19 ['drive', 'scsi', 'disk', 'organization', 'line', 'hard', 'floppy', 'ide', 'problem', 'controller', 'mac', 'university', 'rom', 'host', 'nntp']\n",
      "./\n",
      "[-2.68899, -3.05941, -2.94356, -2.33832, -1.4382, -1.65906, -5.26377, -3.8147, -2.76116, -3.87739, -4.3672, -3.28972, -3.58637, -3.84432, -3.88189, -3.17796, -1.64288, -1.76996, -1.75967, -4.28736]\n",
      "-3.0725945\n",
      "[-0.01009, -0.0769, -0.07422, -0.02957, 0.05247, 0.00365, -0.10655, -0.09018, -0.01879, -0.13855, -0.0577, -0.07308, -0.01639, -0.07931, -0.12165, -0.07862, 0.04338, 0.05873, -0.00407, 0.01095]\n",
      "-0.04032450000000001\n",
      "[0.07729, -0.04667, 0.0424, -0.02314, 0.13625, 0.01045, -0.04373, -0.02243, 0.12517, -0.13466, -0.14208, 0.00045, 0.20298, -0.14293, -0.16575, -0.11988, 0.09407, 0.18711, 0.0004, 0.14782]\n",
      "0.009155999999999997\n",
      "[-0.98491, -2.06554, -2.07954, -1.04389, 0.56353, -0.26375, -3.13983, -2.46672, -1.1858, -3.67449, -1.9923, -2.00447, -1.19705, -2.05515, -3.77442, -2.17342, 0.24684, 0.49642, -0.33331, -0.83958]\n",
      "-1.4983690000000003\n",
      "[0.37877, 0.32825, 0.34093, 0.30288, 0.37254, 0.28045, 0.41447, 0.37094, 0.35562, 0.38616, 0.40815, 0.34259, 0.39236, 0.35566, 0.49054, 0.37887, 0.39855, 0.3743, 0.3018, 0.45223]\n",
      "0.37130300000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 183.13it/s]\n",
      "59it [00:00, 302.56it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 274.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 20, 'umass_wiki': -3.0725945, 'npmi_wiki': -0.04032450000000001, 'uci_wiki': -1.4983690000000003, 'CV_wiki': 0.37130300000000005, 'cp_wiki': 0.009155999999999997, 'sim_w2v': 0.12148387445962326, 'diversity': 0.5166666666666667, 'filename': 'results/240517_171814.txt', 'acc': 0.4629580456718003, 'macro-F1': 0.44323185927424386, 'Purity': 0.4698619224641529, 'NMI': 0.5181998443043795, 'label_match': 0.48258794414000356}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 2.43727 - dist: 0.03480 - cons: -0.40264\n",
      "Epoch-1 / recon: 2.33745 - dist: 0.02474 - cons: -0.44628\n",
      "Epoch-2 / recon: 2.28568 - dist: 0.02141 - cons: -0.46605\n",
      "Epoch-3 / recon: 2.25581 - dist: 0.01986 - cons: -0.47736\n",
      "Epoch-4 / recon: 2.23659 - dist: 0.01896 - cons: -0.48486\n",
      "Epoch-5 / recon: 2.22319 - dist: 0.01836 - cons: -0.49050\n",
      "Epoch-6 / recon: 2.21331 - dist: 0.01791 - cons: -0.49492\n",
      "Epoch-7 / recon: 2.20572 - dist: 0.01765 - cons: -0.49842\n",
      "Epoch-8 / recon: 2.19970 - dist: 0.01740 - cons: -0.50128\n",
      "Epoch-9 / recon: 2.19478 - dist: 0.01718 - cons: -0.50367\n",
      "Epoch-10 / recon: 2.19071 - dist: 0.01699 - cons: -0.50572\n",
      "Epoch-11 / recon: 2.18728 - dist: 0.01684 - cons: -0.50753\n",
      "Epoch-12 / recon: 2.18434 - dist: 0.01678 - cons: -0.50915\n",
      "Epoch-13 / recon: 2.18179 - dist: 0.01669 - cons: -0.51064\n",
      "Epoch-14 / recon: 2.17958 - dist: 0.01665 - cons: -0.51196\n",
      "Epoch-15 / recon: 2.17762 - dist: 0.01660 - cons: -0.51316\n",
      "Epoch-16 / recon: 2.17590 - dist: 0.01656 - cons: -0.51418\n",
      "Epoch-17 / recon: 2.17434 - dist: 0.01650 - cons: -0.51511\n",
      "Epoch-18 / recon: 2.17293 - dist: 0.01649 - cons: -0.51599\n",
      "Epoch-19 / recon: 2.17168 - dist: 0.01646 - cons: -0.51681\n",
      "Epoch-20 / recon: 2.17054 - dist: 0.01646 - cons: -0.51757\n",
      "Epoch-21 / recon: 2.16950 - dist: 0.01643 - cons: -0.51836\n",
      "Epoch-22 / recon: 2.16853 - dist: 0.01642 - cons: -0.51905\n",
      "Epoch-23 / recon: 2.16765 - dist: 0.01641 - cons: -0.51969\n",
      "Epoch-24 / recon: 2.16683 - dist: 0.01641 - cons: -0.52027\n",
      "Epoch-25 / recon: 2.16608 - dist: 0.01639 - cons: -0.52080\n",
      "Epoch-26 / recon: 2.16538 - dist: 0.01639 - cons: -0.52136\n",
      "Epoch-27 / recon: 2.16472 - dist: 0.01637 - cons: -0.52192\n",
      "Epoch-28 / recon: 2.16412 - dist: 0.01637 - cons: -0.52241\n",
      "Epoch-29 / recon: 2.16355 - dist: 0.01638 - cons: -0.52289\n",
      "Epoch-30 / recon: 2.16302 - dist: 0.01637 - cons: -0.52339\n",
      "Epoch-31 / recon: 2.16253 - dist: 0.01636 - cons: -0.52384\n",
      "Epoch-32 / recon: 2.16206 - dist: 0.01635 - cons: -0.52426\n",
      "Epoch-33 / recon: 2.16161 - dist: 0.01636 - cons: -0.52465\n",
      "Epoch-34 / recon: 2.16120 - dist: 0.01635 - cons: -0.52503\n",
      "Epoch-35 / recon: 2.16081 - dist: 0.01635 - cons: -0.52539\n",
      "Epoch-36 / recon: 2.16044 - dist: 0.01636 - cons: -0.52574\n",
      "Epoch-37 / recon: 2.16009 - dist: 0.01635 - cons: -0.52607\n",
      "Epoch-38 / recon: 2.15975 - dist: 0.01635 - cons: -0.52639\n",
      "Epoch-39 / recon: 2.15943 - dist: 0.01636 - cons: -0.52671\n",
      "Epoch-40 / recon: 2.15912 - dist: 0.01636 - cons: -0.52700\n",
      "Epoch-41 / recon: 2.15882 - dist: 0.01636 - cons: -0.52728\n",
      "Epoch-42 / recon: 2.15855 - dist: 0.01637 - cons: -0.52757\n",
      "Epoch-43 / recon: 2.15828 - dist: 0.01636 - cons: -0.52783\n",
      "Epoch-44 / recon: 2.15802 - dist: 0.01637 - cons: -0.52809\n",
      "Epoch-45 / recon: 2.15778 - dist: 0.01637 - cons: -0.52834\n",
      "Epoch-46 / recon: 2.15755 - dist: 0.01637 - cons: -0.52859\n",
      "Epoch-47 / recon: 2.15733 - dist: 0.01637 - cons: -0.52882\n",
      "Epoch-48 / recon: 2.15711 - dist: 0.01638 - cons: -0.52906\n",
      "Epoch-49 / recon: 2.15690 - dist: 0.01637 - cons: -0.52927\n",
      "------- Evaluation results -------\n",
      "topic-0 ['drive', 'scsi', 'disk', 'organization', 'line', 'floppy', 'hard', 'ide', 'controller', 'mac', 'rom', 'problem', 'university', 'boot', 'host']\n",
      "topic-1 ['subject', 'clinton', 'line', 'organization', 'government', 'article', 'writes', 'jewish', 'fbi', 'president', 'tax', 'waco', 'people', 'posting', 'nntp']\n",
      "topic-2 ['car', 'subject', 'line', 'organization', 'article', 'engine', 'writes', 'oil', 'dealer', 'posting', 'like', 'one', 'good', 'price', 'saturn']\n",
      "topic-3 ['card', 'line', 'bus', 'chip', 'organization', 'ram', 'simms', 'video', 'motherboard', 'board', 'memory', 'slot', 'speed', 'eisa', 'isa']\n",
      "topic-4 ['subject', 'line', 'organization', 'university', 'posting', 'nntp', 'host', 'mail', 'thanks', 'please', 'reply', 'distribution', 'internet', 'address', 'science']\n",
      "topic-5 ['max', 'god', 'jesus', 'christian', 'church', 'atheist', 'bible', 'religion', 'people', 'christ', 'belief', 'islam', 'faith', 'sin', 'one']\n",
      "topic-6 ['gun', 'armenian', 'turkish', 'subject', 'weapon', 'people', 'firearm', 'armenia', 'greek', 'turkey', 'handgun', 'line', 'criminal', 'right', 'argic']\n",
      "topic-7 ['space', 'subject', 'moon', 'line', 'organization', 'nasa', 'article', 'launch', 'writes', 'orbit', 'lunar', 'pat', 'shuttle', 'satellite', 'henry']\n",
      "topic-8 ['team', 'game', 'win', 'season', 'hockey', 'player', 'playoff', 'year', 'fan', 'play', 'detroit', 'devil', 'pitching', 'line', 'cup']\n",
      "topic-9 ['key', 'clipper', 'encryption', 'chip', 'escrow', 'modem', 'crypto', 'government', 'nsa', 'line', 'secret', 'algorithm', 'organization', 'secure', 'system']\n",
      "topic-10 ['bike', 'subject', 'dod', 'motorcycle', 'line', 'organization', 'dog', 'article', 'writes', 'ride', 'posting', 'bmw', 'nntp', 'host', 'helmet']\n",
      "topic-11 ['subject', 'monitor', 'line', 'printer', 'organization', 'image', 'vga', 'university', 'video', 'posting', 'screen', 'nntp', 'host', 'problem', 'one']\n",
      "topic-12 ['subject', 'line', 'organization', 'game', 'player', 'university', 'team', 'hockey', 'baseball', 'article', 'posting', 'host', 'writes', 'nntp', 'nhl']\n",
      "topic-13 ['subject', 'line', 'sale', 'organization', 'circuit', 'phone', 'university', 'offer', 'posting', 'host', 'nntp', 'amp', 'distribution', 'shipping', 'sound']\n",
      "topic-14 ['window', 'subject', 'file', 'line', 'organization', 'program', 'manager', 'use', 'application', 'host', 'university', 'nntp', 'posting', 'version', 'running']\n",
      "topic-15 ['subject', 'driver', 'line', 'organization', 'msg', 'window', 'ftp', 'port', 'card', 'nntp', 'keyboard', 'host', 'posting', 'site', 'file']\n",
      "topic-16 ['subject', 'morality', 'writes', 'objective', 'line', 'organization', 'article', 'people', 'homosexual', 'keith', 'moral', 'atheist', 'gay', 'cramer', 'law']\n",
      "topic-17 ['subject', 'line', 'font', 'window', 'widget', 'color', 'organization', 'graphic', 'motif', 'mouse', 'file', 'gif', 'xterm', 'thanks', 'host']\n",
      "topic-18 ['subject', 'gordon', 'bank', 'organization', 'article', 'line', 'disease', 'writes', 'doctor', 'patient', 'food', 'medical', 'chastity', 'skepticism', 'pittsburgh']\n",
      "topic-19 ['israeli', 'subject', 'israel', 'arab', 'christian', 'jew', 'writes', 'line', 'jesus', 'organization', 'palestinian', 'god', 'article', 'bible', 'one']\n",
      "./\n",
      "[-2.70253, -4.56963, -2.53675, -3.96361, -3.52827, -2.06636, -5.5098, -2.60736, -2.43256, -2.94498, -4.7208, -3.80162, -4.05614, -4.90077, -4.66957, -4.11984, -2.78196, -4.59278, -2.98583, -1.77472]\n",
      "-3.5632940000000004\n",
      "[0.05516, -0.07096, -0.05288, 0.01249, -0.07006, 0.12209, -0.03491, 0.02304, 0.07073, -0.00997, -0.09116, -0.08247, -0.059, -0.11271, -0.10942, -0.11778, -0.02711, -0.0711, -0.06516, 0.05045]\n",
      "-0.03203649999999999\n",
      "[0.31079, -0.09617, -0.06095, 0.30837, -0.00549, 0.5097, -0.06062, 0.06622, 0.41674, 0.21643, -0.18964, -0.02174, -0.03328, -0.11099, -0.0598, -0.05597, -0.02065, 0.03194, -0.09026, 0.14566]\n",
      "0.06001449999999999\n",
      "[0.17609, -2.11053, -1.53505, -0.76379, -2.03324, 1.15428, -1.57594, -0.22272, 0.20472, -1.24456, -2.72812, -2.36084, -2.00107, -3.07504, -3.10822, -3.3377, -1.16446, -2.31373, -1.90937, 0.5258]\n",
      "-1.4711744999999996\n",
      "[0.41784, 0.36065, 0.28168, 0.39456, 0.36319, 0.4534, 0.36446, 0.3717, 0.45468, 0.39109, 0.40739, 0.38188, 0.41827, 0.37, 0.40851, 0.38209, 0.33112, 0.38764, 0.30528, 0.36718]\n",
      "0.38063050000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:00, 191.40it/s]\n",
      "59it [00:00, 271.89it/s]\n",
      "100%|██████████| 89/89 [00:00<00:00, 291.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 20, 'umass_wiki': -3.5632940000000004, 'npmi_wiki': -0.03203649999999999, 'uci_wiki': -1.4711744999999996, 'CV_wiki': 0.38063050000000004, 'cp_wiki': 0.06001449999999999, 'sim_w2v': 0.13061233760068758, 'diversity': 0.6133333333333333, 'filename': 'results/240517_172312.txt', 'acc': 0.484599044078598, 'macro-F1': 0.45900530629575814, 'Purity': 0.49256505576208176, 'NMI': 0.5226597176784031, 'label_match': 0.49398974721583877}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_classification, evaluate_clustering\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    testloader = DataLoader(testds2, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * ((1-org_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * ((1-pos_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic, pos_topic), dim=0), dirichlet_alpha_2)\n",
    "            \n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(15, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "    now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "                                  reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                                  filename=f'results/{now}.txt')\n",
    "    train_theta = []\n",
    "    test_theta = []\n",
    "    for batch_idx, batch in tqdm(enumerate(trainloader)):\n",
    "        org_input, _, org_bow, _ = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        train_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    train_theta = np.concatenate(train_theta, axis=0)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(testloader)): \n",
    "        org_input, org_bow = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        test_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    test_theta = np.concatenate(test_theta, axis=0)\n",
    "    \n",
    "    classification_res = evaluate_classification(train_theta, test_theta, textData.targets, textData.test_targets)\n",
    "    clustering_res = evaluate_clustering(test_theta, textData.test_targets)\n",
    "    \n",
    "    results.update(classification_res)\n",
    "    results.update(clustering_res)\n",
    "    \n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(textData.data) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_N  umass_wiki  npmi_wiki  uci_wiki   CV_wiki   cp_wiki   sim_w2v  \\\n",
      "0       20   -3.152525  -0.021625 -1.200058  0.375631  0.091560  0.130785   \n",
      "1       20   -2.959089  -0.012533 -0.957628  0.366360  0.097861  0.131252   \n",
      "2       20   -3.141894  -0.020193 -1.146814  0.369499  0.081455  0.128377   \n",
      "3       20   -3.072595  -0.040325 -1.498369  0.371303  0.009156  0.121484   \n",
      "4       20   -3.563294  -0.032036 -1.471174  0.380631  0.060014  0.130612   \n",
      "\n",
      "   diversity                   filename       acc  macro-F1    Purity  \\\n",
      "0   0.583333  results/240517_170500.txt  0.511020  0.478994  0.507700   \n",
      "1   0.590000  results/240517_170922.txt  0.504514  0.486258  0.514472   \n",
      "2   0.583333  results/240517_171349.txt  0.510489  0.500209  0.514206   \n",
      "3   0.516667  results/240517_171814.txt  0.462958  0.443232  0.469862   \n",
      "4   0.613333  results/240517_172312.txt  0.484599  0.459005  0.492565   \n",
      "\n",
      "        NMI  label_match  \n",
      "0  0.521472     0.500707  \n",
      "1  0.520524     0.530051  \n",
      "2  0.531562     0.531731  \n",
      "3  0.518200     0.482588  \n",
      "4  0.522660     0.493990  \n",
      "mean\n",
      "topic_N        20.000000\n",
      "umass_wiki     -3.177879\n",
      "npmi_wiki      -0.025342\n",
      "uci_wiki       -1.254809\n",
      "CV_wiki         0.372685\n",
      "cp_wiki         0.068009\n",
      "sim_w2v         0.128502\n",
      "diversity       0.577333\n",
      "acc             0.494716\n",
      "macro-F1        0.473540\n",
      "Purity          0.499761\n",
      "NMI             0.522883\n",
      "label_match     0.507813\n",
      "dtype: float64\n",
      "std\n",
      "topic_N        0.000000\n",
      "umass_wiki     0.228821\n",
      "npmi_wiki      0.010882\n",
      "uci_wiki       0.228644\n",
      "CV_wiki        0.005565\n",
      "cp_wiki        0.035896\n",
      "sim_w2v        0.004077\n",
      "diversity      0.036086\n",
      "acc            0.020746\n",
      "macro-F1       0.022542\n",
      "Purity         0.018933\n",
      "NMI            0.005120\n",
      "label_match    0.022048\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)\n",
    "print('mean')\n",
    "print(results_df.mean())\n",
    "print('std')\n",
    "print(results_df.std())\n",
    "\n",
    "if args.result_file is not None:\n",
    "    result_filename = f'results/{args.result_file}'\n",
    "else:\n",
    "    result_filename = f'results/{now}.tsv'\n",
    "\n",
    "results_df.to_csv(result_filename, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
