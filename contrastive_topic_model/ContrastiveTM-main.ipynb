{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Arguments\n",
    "args_text = '--base-model sentence-transformers/all-MiniLM-L6-v2 '+\\\n",
    "            '--dataset yahoo --n-word 5000 --epochs-1 200 --epochs-2 50 ' + \\\n",
    "            '--bsz 32 --stage-2-lr 2e-2 --stage-2-repeat 5 --coeff-1-dist 50 '+ \\\n",
    "            '--n-cluster 50 ' + \\\n",
    "            '--stage-1-ckpt trained_model/imdb_model_all-MiniLM-L6-v2_stage1_50t_5000w_199e.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this log_lamb_rs, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PDT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtools.optim import RangerLars\n",
    "import gensim.downloader\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils import AverageMeter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from pytorch_transformers import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import OPTICS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import gensim.downloader\n",
    "from scipy.linalg import qr\n",
    "from data import *\n",
    "from model import ContBertTopicExtractorAE\n",
    "from evaluation import get_topic_qualities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Contrastive topic modeling')\n",
    "    parser.add_argument('--epochs-1', default=100, type=int,\n",
    "                        help='Number of training epochs for Stage 1')\n",
    "    parser.add_argument('--epochs-2', default=10, type=int,\n",
    "                        help='Number of training epochs for Stage 2')\n",
    "    parser.add_argument('--bsz', type=int, default=64,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--dataset', default='news', type=str,\n",
    "                        choices=['news', 'twitter', 'wiki', 'nips', 'stackoverflow', 'reuters', 'r52', 'imdb', 'agnews', 'yahoo'],\n",
    "                        help='Name of the dataset')\n",
    "    parser.add_argument('--n-cluster', default=20, type=int,\n",
    "                        help='Number of clusters')\n",
    "    parser.add_argument('--n-topic', type=int,\n",
    "                        help='Number of topics. If not specified, use same value as --n-cluster')\n",
    "    parser.add_argument('--n-word', default=2000, type=int,\n",
    "                        help='Number of words in vocabulary')\n",
    "    \n",
    "    parser.add_argument('--base-model', type=str,\n",
    "                        help='Name of base model in huggingface library.')\n",
    "    \n",
    "    parser.add_argument('--gpus', default=[0,1], type=int, nargs='+',\n",
    "                        help='List of GPU numbers to use. Use 0 by default')\n",
    "    \n",
    "    parser.add_argument('--coeff-1-sim', default=1.0, type=float,\n",
    "                        help='Coefficient for NN dot product similarity loss (Phase 1)')\n",
    "    parser.add_argument('--coeff-1-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for NN SWD distribution loss (Phase 1)')\n",
    "    parser.add_argument('--dirichlet-alpha-1', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 1). Use 1/n_topic by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-1-ckpt', type=str,\n",
    "                        help='Name of torch checkpoint file Stage 1. If this argument is given, skip Stage 1.')\n",
    "    \n",
    "    parser.add_argument('--coeff-2-recon', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE reconstruction loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-regul', default=1.0, type=float,\n",
    "                        help='Coefficient for VAE KLD regularization loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-cons', default=1.0, type=float,\n",
    "                        help='Coefficient for CL consistency loss (Phase 2)')\n",
    "    parser.add_argument('--coeff-2-dist', default=1.0, type=float,\n",
    "                        help='Coefficient for CL SWD distribution matching loss (Phase 2)')\n",
    "    parser.add_argument('--dirichlet-alpha-2', type=float,\n",
    "                        help='Parameter for Dirichlet distribution (Phase 2). Use same value as dirichlet-alpha-1 by default.')\n",
    "    \n",
    "    parser.add_argument('--stage-2-lr', default=2e-1, type=float,\n",
    "                        help='Learning rate of phase 2')\n",
    "    parser.add_argument('--stage-2-repeat', default=5, type=int,\n",
    "                        help='Repetition count of phase 2')\n",
    "    \n",
    "    parser.add_argument('--result-file', type=str,\n",
    "                        help='File name for result summary')\n",
    "    parser.add_argument('--palmetto-dir', type=str, default='./',\n",
    "                        help='Directory where palmetto JAR and the Wikipedia index are. For evaluation')\n",
    "    \n",
    "    \n",
    "    # Check if the code is run in Jupyter notebook\n",
    "    is_in_jupyter = False\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            is_in_jupyter = True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            is_in_jupyter = False  # Terminal running IPython\n",
    "        else:\n",
    "            is_in_jupyter = False  # Other type (?)\n",
    "    except NameError:\n",
    "        is_in_jupyter = False\n",
    "    \n",
    "    if is_in_jupyter:\n",
    "        return parser.parse_args(args=args_text.split())\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "def data_load(dataset_name):\n",
    "    should_measure_hungarian = False\n",
    "    if dataset_name == 'news':\n",
    "        textData = newsData()\n",
    "        should_measure_hungarian = True\n",
    "    elif dataset_name == 'imdb':\n",
    "        textData = IMDBData()\n",
    "    elif dataset_name == 'agnews':\n",
    "        textData = AGNewsData()\n",
    "    elif dataset_name == 'yahoo':\n",
    "        textData = YahooData()\n",
    "    elif dataset_name == 'twitter':\n",
    "        textData = twitterData('/home/data/topicmodel/twitter_covid19.tsv')\n",
    "    elif dataset_name == 'wiki':\n",
    "        textData = wikiData('/home/data/topicmodel/smplAbstracts/')\n",
    "    elif dataset_name == 'nips':\n",
    "        textData = nipsData('/home/data/topicmodel/papers.csv')\n",
    "    elif dataset_name == 'stackoverflow':\n",
    "        textData = stackoverflowData('/home/data/topicmodel/stack_overflow.csv')\n",
    "    elif dataset_name == 'reuters':\n",
    "        textData = reutersData('/home/data/topicmodel/reuters-21578.txt')\n",
    "    elif dataset_name == 'r52':\n",
    "        textData = r52Data('/home/data/topicmodel/r52/')\n",
    "        should_measure_hungarian = True\n",
    "    return textData, should_measure_hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = _parse_args()\n",
    "bsz = args.bsz\n",
    "epochs_1 = args.epochs_1\n",
    "epochs_2 = args.epochs_2\n",
    "\n",
    "n_cluster = args.n_cluster\n",
    "n_topic = args.n_topic if (args.n_topic is not None) else n_cluster\n",
    "args.n_topic = n_topic\n",
    "\n",
    "textData, should_measure_hungarian = data_load(args.dataset)\n",
    "\n",
    "ema_alpha = 0.99\n",
    "n_word = args.n_word\n",
    "if args.dirichlet_alpha_1 is None:\n",
    "    dirichlet_alpha_1 = 1 / n_cluster\n",
    "else:\n",
    "    dirichlet_alpha_1 = args.dirichlet_alpha_1\n",
    "if args.dirichlet_alpha_2 is None:\n",
    "    dirichlet_alpha_2 = dirichlet_alpha_1\n",
    "else:\n",
    "    dirichlet_alpha_2 = args.dirichlet_alpha_2\n",
    "    \n",
    "bert_name = args.base_model\n",
    "bert_name_short = bert_name.split('/')[-1]\n",
    "gpu_ids = args.gpus\n",
    "\n",
    "skip_stage_1 = (args.stage_1_ckpt is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2662.38it/s]\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009000539779663086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d2afe064204c518e7155a28dafcc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainds = BertDataset(bert=bert_name, text_list=textData.data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "basesim_path = f\"./save/{args.dataset}_{bert_name_short}_basesim_matrix_full.pkl\"\n",
    "if os.path.isfile(basesim_path) == False:\n",
    "    model = SentenceTransformer(bert_name.split('/')[-1], device='cuda')\n",
    "    base_result_list = []\n",
    "    for text in tqdm_notebook(trainds.nonempty_text):\n",
    "        base_result_list.append(model.encode(text))\n",
    "        \n",
    "    base_result_embedding = np.stack(base_result_list)\n",
    "    basereduced_norm = F.normalize(torch.tensor(base_result_embedding), dim=-1)\n",
    "    basesim_matrix = torch.mm(basereduced_norm, basereduced_norm.t())\n",
    "    ind = np.diag_indices(basesim_matrix.shape[0])\n",
    "    basesim_matrix[ind[0], ind[1]] = torch.ones(basesim_matrix.shape[0]) * -1\n",
    "    torch.save(basesim_matrix, basesim_path)\n",
    "else:\n",
    "    basesim_matrix = torch.load(basesim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Discovery neighborhood pairs and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hidden, alpha=1.0):\n",
    "    device = hidden.device\n",
    "    hidden_dim = hidden.shape[-1]\n",
    "    rand_w = torch.Tensor(np.eye(hidden_dim, dtype=np.float64)).to(device)\n",
    "    loss_dist_match = get_swd_loss(hidden, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t) ** 2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=768)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n",
    "    model.cuda(gpu_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = args.bsz = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    losses = AverageMeter()\n",
    "    closses = AverageMeter() \n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter() \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "    finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "\n",
    "    optimizer = RangerLars(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    global_step = 0\n",
    "    memory_queue = F.softmax(torch.randn(512, n_cluster).cuda(gpu_ids[0]), dim=1)\n",
    "    for epoch in range(epochs_1):\n",
    "        model.train()\n",
    "        #ema_model.train()\n",
    "        tbar = tqdm_notebook(trainloader)\n",
    "        for batch_idx, batch in enumerate(tbar):       \n",
    "            org_input, pos_input, _, _ = batch\n",
    "            org_input_ids = org_input['input_ids'].cuda(gpu_ids[0])\n",
    "            org_attention_mask = org_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            pos_input_ids = pos_input['input_ids'].cuda(gpu_ids[0])\n",
    "            pos_attention_mask = pos_input['attention_mask'].cuda(gpu_ids[0])\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            all_input_ids = torch.cat((org_input_ids, pos_input_ids), dim=0)\n",
    "            all_attention_masks = torch.cat((org_attention_mask, pos_attention_mask), dim=0)\n",
    "            all_topics, _ = model(all_input_ids, all_attention_masks, return_topic=True)\n",
    "\n",
    "            orig_topic, pos_topic = torch.split(all_topics, len(all_topics) // 2)\n",
    "            pos_sim = torch.sum(orig_topic * pos_topic, dim=-1)\n",
    "\n",
    "            # consistency loss\n",
    "            consistency_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution matching loss\n",
    "            memory_queue = torch.cat((memory_queue.detach(), all_topics), dim=0)[all_topics.size(0):]\n",
    "            distmatch_loss = dist_match_loss(memory_queue, dirichlet_alpha_1)\n",
    "            loss = args.coeff_1_sim * consistency_loss + \\\n",
    "                   10 * distmatch_loss\n",
    "\n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(consistency_loss.item(), bsz)\n",
    "            dlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            tbar.set_description(\"Epoch-{} / consistency: {:.5f} - dist: {:.5f}\".format(epoch, \n",
    "                                                                                        closses.avg, \n",
    "                                                                                        dlosses.avg), refresh=True)\n",
    "            tbar.refresh()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            global_step += 1\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_stage_1:\n",
    "    model_stage1_name = f'./trained_model/{args.dataset}_model_{bert_name_short}_stage1_{args.n_topic}t_{args.n_word}w_{epoch}e.ckpt'\n",
    "    torch.save(model.module.state_dict(), model_stage1_name)\n",
    "else:\n",
    "    model_stage1_name = args.stage_1_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: extract vocab set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_embedding_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del ema_model\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContBertTopicExtractorAE(N_topic=n_cluster, N_word=n_word, bert=bert_name, bert_dim=768)\n",
    "model.cuda(gpu_ids[0])\n",
    "\n",
    "model.load_state_dict(torch.load(model_stage1_name), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003999948501586914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 40,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd80bdd51d89443484b5941dc2ba6d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_basesim_matrix = copy.deepcopy(basesim_matrix)\n",
    "finetuneds = FinetuneDataset(trainds, temp_basesim_matrix, ratio=1, k=1)\n",
    "memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "result_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm_notebook(memoryloader)):        \n",
    "        org_input, _, _, _ = batch\n",
    "        org_input_ids = org_input['input_ids'].to(gpu_ids[0])\n",
    "        org_attention_mask = org_input['attention_mask'].to(gpu_ids[0])\n",
    "        topic, embed = model(org_input_ids, org_attention_mask, return_topic = True)\n",
    "        result_list.append(topic)\n",
    "result_embedding = torch.cat(result_list)\n",
    "_, result_topic = torch.max(result_embedding, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': trainds.preprocess_ctm(trainds.nonempty_text), \n",
    "     'cluster_label': result_topic.cpu().numpy()}\n",
    "cluster_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_class = cluster_df.groupby(['cluster_label'], as_index=False).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "ctfidf_vectorizer = CTFIDFVectorizer()\n",
    "count = count_vectorizer.fit_transform(docs_per_class.text)\n",
    "ctfidf = ctfidf_vectorizer.fit_transform(count, n_samples=len(cluster_df)).toarray()\n",
    "words = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transport to gensim\n",
    "(gensim_corpus, gensim_dict) = vect2gensim(count_vectorizer, count)\n",
    "vocab_list = set(gensim_dict.token2id.keys())\n",
    "stopwords = set(line.strip() for line in open('stopwords_en.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "resolution_score = (ctfidf - np.min(ctfidf, axis=1, keepdims=True)) / (np.max(ctfidf, axis=1, keepdims=True) - np.min(ctfidf, axis=1, keepdims=True))\n",
    "\n",
    "n_word = args.n_word\n",
    "# n_topic_word = n_word / len(docs_per_class.cluster_label.index)\n",
    "n_topic_word = n_word\n",
    "n_topic_word = 15\n",
    "\n",
    "topic_word_dict = {}\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['game', 'league', 'team', 'win', 'season', 'sox', 'red', 'sport', 'player', 'coach'],\n",
      "2: ['china', 'president', 'bush', 'election', 'russian', 'russia', 'government', 'afghan', 'presidential', 'talk'],\n",
      "3: ['tiger', 'dhaka', 'bank', 'swift', 'nec', 'argentine', 'trojan', 'part', 'security', 'death'],\n",
      "4: ['attack', 'terror', 'intel', 'bomb', 'blast', 'suspect', 'gunfire', 'people', 'terrorist', 'security'],\n",
      "5: ['sony', 'kodak', 'patent', 'digital', 'studio', 'format', 'technology', 'sale', 'camera', 'company'],\n",
      "6: ['dvd', 'google', 'amazon', 'search', 'studio', 'rental', 'online', 'offering', 'engine', 'format'],\n",
      "7: ['google', 'sharon', 'gaza', 'gay', 'ipo', 'share', 'dell', 'range', 'search', 'ariel'],\n",
      "8: ['intel', 'bomb', 'chip', 'pentium', 'offense', 'dual', 'attack', 'latest', 'hit', 'afp'],\n",
      "10: ['holiday', 'christmas', 'shopping', 'season', 'thanksgiving', 'online', 'retailer', 'santa', 'sale', 'november'],\n",
      "11: ['company', 'microsoft', 'ibm', 'software', 'corp', 'security', 'stock', 'service', 'technology', 'system'],\n",
      "12: ['dell', 'blackberry', 'ibm', 'sony', 'jaguar', 'aol', 'href', 'http', 'ticker', 'company'],\n",
      "13: ['museum', 'phishing', 'buyer', 'mae', 'fannie', 'bank', 'electricity', 'attack', 'stunned', 'slight'],\n",
      "14: ['iraq', 'iraqi', 'baghdad', 'war', 'killed', 'troop', 'reuters', 'palestinian', 'nuclear', 'iran'],\n",
      "15: ['oil', 'price', 'ford', 'stock', 'car', 'mexico', 'crude', 'high', 'friday', 'motor'],\n",
      "16: ['phishing', 'church', 'pope', 'security', 'catholic', 'religious', 'spam', 'attack', 'hole', 'girl'],\n",
      "17: ['woman', 'men', 'site', 'researcher', 'yahoo', 'system', 'sell', 'leading', 'wireless', 'drink'],\n",
      "18: ['space', 'nasa', 'moon', 'bird', 'saturn', 'shuttle', 'planet', 'scientist', 'titan', 'cassini'],\n",
      "20: ['season', 'oracle', 'euro', 'european', 'peoplesoft', 'time', 'year', 'italy', 'point', 'cup'],\n",
      "21: ['mortgage', 'wife', 'haiti', 'rate', 'test', 'application', 'pension', 'reuters', 'ibm', 'loss'],\n",
      "22: ['novell', 'book', 'greek', 'athens', 'greece', 'microsoft', 'thai', 'antitrust', 'google', 'dollar'],\n",
      "23: ['bank', 'prison', 'jail', 'investment', 'martha', 'stewart', 'year', 'prisoner', 'job', 'sentenced'],\n",
      "24: ['family', 'year', 'home', 'reuters', 'house', 'sale', 'federer', 'rate', 'top', 'yesterday'],\n",
      "25: ['oil', 'stock', 'price', 'reuters', 'year', 'africa', 'hurricane', 'million', 'tuesday', 'space'],\n",
      "26: ['scientist', 'nasa', 'solaris', 'arctic', 'linux', 'science', 'warming', 'system', 'human', 'computer'],\n",
      "27: ['japan', 'tokyo', 'japanese', 'german', 'reuters', 'germany', 'country', 'dollar', 'nikkei', 'stock'],\n",
      "28: ['ipod', 'apple', 'music', 'india', 'itunes', 'song', 'photo', 'indian', 'player', 'computer'],\n",
      "29: ['disney', 'eisner', 'walt', 'michael', 'ovitz', 'wal', 'mart', 'store', 'shareholder', 'director'],\n",
      "30: ['prince', 'mauresmo', 'thatcher', 'amelie', 'ireland', 'dell', 'coup', 'dame', 'notre', 'athens'],\n",
      "31: ['baby', 'apple', 'charlotte', 'regulator', 'imac', 'die', 'food', 'mae', 'fannie', 'genesis'],\n",
      "32: ['patent', 'profit', 'quarter', 'sale', 'company', 'earnings', 'business', 'share', 'consumer', 'file'],\n",
      "33: ['french', 'france', 'paris', 'italian', 'european', 'spain', 'spanish', 'dutch', 'italy', 'union'],\n",
      "34: ['google', 'yahoo', 'search', 'ibm', 'internet', 'microsoft', 'web', 'mail', 'user', 'service'],\n",
      "35: ['athens', 'profit', 'quarterly', 'food', 'advertising', 'online', 'lcd', 'font', 'sale', 'earnings'],\n",
      "36: ['museum', 'master', 'andre', 'lehman', 'harrington', 'stephen', 'abbey', 'edged', 'agassi', 'tied'],\n",
      "37: ['student', 'university', 'chicago', 'school', 'quarterly', 'profit', 'library', 'earnings', 'posted', 'quickinfo'],\n",
      "38: ['farm', 'kim', 'classic', 'hurricane', 'originally', 'contact', 'speaking', 'record', 'estimated', 'damaged'],\n",
      "39: ['drug', 'cisco', 'phone', 'pfizer', 'company', 'sale', 'target', 'stock', 'maker', 'police'],\n",
      "40: ['child', 'toy', 'language', 'safety', 'food', 'ice', 'dvd', 'junk', 'rental', 'knicks'],\n",
      "41: ['child', 'florida', 'block', 'law', 'judge', 'pornography', 'growth', 'porn', 'park', 'hurricane'],\n",
      "42: ['music', 'movie', 'file', 'industry', 'apple', 'reduced', 'fine', 'msn', 'tom', 'color'],\n",
      "43: ['dell', 'based', 'music', 'digital', 'girl', 'golden', 'font', 'sale', 'yahoo', 'million'],\n",
      "44: ['sex', 'porn', 'spam', 'abuse', 'virgin', 'internet', 'trial', 'hotel', 'rape', 'case'],\n",
      "45: ['music', 'digital', 'online', 'apple', 'player', 'virgin', 'service', 'peer', 'industry', 'ipod'],\n",
      "46: ['trial', 'enron', 'criminal', 'judge', 'court', 'murder', 'fraud', 'attorney', 'federal', 'spitzer'],\n",
      "47: ['son', 'father', 'prime', 'minister', 'company', 'chip', 'parent', 'blair', 'thatcher', 'chief'],\n",
      "48: ['black', 'hollinger', 'sears', 'phone', 'nvidia', 'gold', 'african', 'company', 'sale', 'imac'],\n",
      "49: ['fight', 'boxing', 'heavyweight', 'chip', 'amd', 'intel', 'rfid', 'boeing', 'processor', 'movie'],\n"
     ]
    }
   ],
   "source": [
    "for key in topic_word_dict:\n",
    "    print(f\"{key}: {topic_word_dict[key]},\")\n",
    "topic_words_list = list(topic_word_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2729\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "\n",
    "class WhiteSpacePreprocessing():\n",
    "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
    "        self.documents = documents\n",
    "        self.stopwords = set(stop_words.words(stopwords_language))\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        warnings.simplefilter('always', DeprecationWarning)\n",
    "        warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
    "                      \"Use WhiteSpacePreprocessingStopwords.\")\n",
    "\n",
    "    def preprocess(self):\n",
    "        preprocessed_docs_tmp = self.documents\n",
    "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=self.vocabulary_size)\n",
    "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
    "        temp_vocabulary = set(vectorizer.get_feature_names())\n",
    "\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "\n",
    "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
    "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
    "            if len(doc) > 0:\n",
    "                preprocessed_docs.append(doc)\n",
    "                unpreprocessed_docs.append(self.documents[i])\n",
    "                retained_indices.append(i)\n",
    "\n",
    "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
    "\n",
    "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices\n",
    "    \n",
    "def _hungarian_match(flat_preds, flat_targets, num_samples, class_num):  \n",
    "    num_k = class_num\n",
    "    num_correct = np.zeros((num_k, num_k))\n",
    "  \n",
    "    for c1 in range(0, num_k):\n",
    "        for c2 in range(0, num_k):\n",
    "            votes = int(((flat_preds == c1) * (flat_targets == c2)).sum())\n",
    "            num_correct[c1, c2] = votes\n",
    "  \n",
    "    match = linear_assignment(num_samples - num_correct)\n",
    "    match = np.array(list(zip(*match)))\n",
    "    res = []\n",
    "    for out_c, gt_c in match:\n",
    "        res.append((out_c, gt_c))\n",
    "  \n",
    "    return res\n",
    "\n",
    "def get_document_topic(topic_words, preprocessed_documents_lemmatized):\n",
    "    topic_words_flatten = list(itertools.chain.from_iterable(topic_words))\n",
    "    if '' in topic_words_flatten:\n",
    "        topic_words_flatten.remove('')\n",
    "    topic_words_flatten = list(set(topic_words_flatten))\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary = topic_words_flatten)\n",
    "    vectorizer = vectorizer.fit(preprocessed_documents_lemmatized)\n",
    "    count_mat = vectorizer.transform(preprocessed_documents_lemmatized).toarray()\n",
    "    \n",
    "    count_mat_normalized = count_mat + 1e-4\n",
    "    count_mat_normalized = count_mat_normalized / count_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat = vectorizer.transform([' '.join(i) for i in topic_words]).toarray()\n",
    "    topic_mat_normalized = topic_mat + 1e-4\n",
    "    topic_mat_normalized = topic_mat_normalized / topic_mat_normalized.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    topic_mat_inverse = topic_mat_normalized @ topic_mat_normalized.transpose()\n",
    "    topic_mat_inverse = np.linalg.inv(topic_mat_inverse)\n",
    "    topic_mat_inverse = topic_mat_normalized.transpose() @ topic_mat_inverse\n",
    "    document_topic = count_mat_normalized @ topic_mat_inverse\n",
    "    return document_topic\n",
    "\n",
    "class TopicModelDataPreparationNoNumber(TopicModelDataPreparation):\n",
    "    def fit(self, text_for_contextual, text_for_bow, labels=None, wordlist=None):\n",
    "        \"\"\"\n",
    "        This method fits the vectorizer and gets the embeddings from the contextual model\n",
    "        :param text_for_contextual: list of unpreprocessed documents to generate the contextualized embeddings\n",
    "        :param text_for_bow: list of preprocessed documents for creating the bag-of-words\n",
    "        :param labels: list of labels associated with each document (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.contextualized_model is None:\n",
    "            raise Exception(\"You should define a contextualized model if you want to create the embeddings\")\n",
    "\n",
    "        # TODO: this count vectorizer removes tokens that have len = 1, might be unexpected for the users\n",
    "        self.vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Z]{2,}\\b', vocabulary=wordlist)\n",
    "\n",
    "        train_bow_embeddings = self.vectorizer.fit_transform(text_for_bow)\n",
    "        train_contextualized_embeddings = bert_embeddings_from_list(text_for_contextual, self.contextualized_model)\n",
    "        self.vocab = self.vectorizer.get_feature_names()\n",
    "        self.id2token = {k: v for k, v in zip(range(0, len(self.vocab)), self.vocab)}\n",
    "\n",
    "        if labels:\n",
    "            self.label_encoder = OneHotEncoder()\n",
    "            encoded_labels = self.label_encoder.fit_transform(np.array([labels]).reshape(-1, 1))\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "\n",
    "        return CTMDataset(train_contextualized_embeddings, train_bow_embeddings, self.id2token, encoded_labels)\n",
    "    \n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "qt = TopicModelDataPreparationNoNumber(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sp = WhiteSpacePreprocessing(textData.data, stopwords_language='english')\n",
    "preprocessed_documents, unpreprocessed_corpus, vocab, retained_indices = sp.preprocess()\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_documents_lemmatized = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()]) for doc in preprocessed_documents]\n",
    "\n",
    "document_topic = get_document_topic(topic_words_list, preprocessed_documents_lemmatized)\n",
    "train_target_filtered = textData.targets.squeeze()[retained_indices]\n",
    "flat_predict = torch.tensor(np.argmax(document_topic, axis=1))\n",
    "flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "num_samples = flat_predict.shape[0]\n",
    "match = _hungarian_match(flat_predict, flat_target, num_samples, 20)    \n",
    "reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "for pred_i, target_i in match:\n",
    "    reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1313456783749812"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(reordered_preds, flat_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "[0.57759, 0.40263, 0.42702, 0.43382, 0.39785, 0.35325, 0.43599, 0.47734, 0.38248, 0.42291, 0.48933, 0.41943, 0.45991, 0.37065, 0.50068, 0.4133, 0.50158, 0.45876, 0.37076, 0.51911, 0.34519, 0.40916, 0.33607, 0.50174, 0.42234, 0.46855, 0.50648, 0.57108, 0.50911, 0.37846, 0.44886, 0.45811, 0.48907, 0.47214, 0.33032, 0.35725, 0.38744, 0.45854, 0.38762, 0.44086, 0.38436, 0.41892, 0.37053, 0.45788, 0.40261, 0.46482, 0.62787]\n",
      "0.43871851063829786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic_N': 47,\n",
       " 'CV_wiki': 0.43871851063829786,\n",
       " 'sim_w2v': 0.16512155871698558,\n",
       " 'diversity': 0.7425531914893617,\n",
       " 'filename': 'results/240510_165640.txt'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "results = get_topic_qualities(topic_words_list, args.palmetto_dir, reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                              filename=f'results/{now}.txt')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = [coherence_normalize(doc) for doc in trainds.nonempty_text]\n",
    "gensim_dict = Dictionary(normalized)\n",
    "\n",
    "n_word = args.n_word\n",
    "n_topic_word = n_word\n",
    "\n",
    "words_to_idx = {k: v for v, k in enumerate(words)}\n",
    "topic_word_dict = {}\n",
    "topic_score_dict = {}\n",
    "total_score_cat = []\n",
    "for label in docs_per_class.cluster_label.index:\n",
    "    total_score = resolution_score[label]\n",
    "    score_higest = total_score.argsort()\n",
    "    score_higest = score_higest[::-1]\n",
    "    topic_word_list = [words[index] for index in score_higest]\n",
    "    \n",
    "    total_score_cat.append(total_score)\n",
    "    topic_word_list = [word for word in topic_word_list if word not in stopwords]    \n",
    "    topic_word_list = [word for word in topic_word_list if word in gensim_dict.token2id]\n",
    "    topic_word_list = [word for word in topic_word_list if len(word) >= 3]    \n",
    "    topic_word_dict[docs_per_class.cluster_label.iloc[label]] = topic_word_list[:int(n_topic_word)]\n",
    "    topic_score_dict[docs_per_class.cluster_label.iloc[label]] = [total_score[words_to_idx[top_word]] for top_word in topic_word_list[:int(n_topic_word)]]\n",
    "total_score_cat = np.stack(total_score_cat, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3886"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "topic_words_list = list(topic_word_dict.values())\n",
    "topic_word_set = list(itertools.chain.from_iterable(pd.DataFrame.from_dict(topic_word_dict).values))\n",
    "word_candidates = remove_dup(topic_word_set)[:n_word]\n",
    "n_word = len(word_candidates)\n",
    "n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('our_word_candidates_10000.pkl', 'wb') as f:\n",
    "    pickle.dump(word_candidates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 47 is out of bounds for axis 0 with size 47",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3664\\342046661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_candidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweight_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_score_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3664\\342046661.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_candidates\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweight_candidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtotal_score_cat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 47 is out of bounds for axis 0 with size 47"
     ]
    }
   ],
   "source": [
    "weight_candidates = {}\n",
    "for candidate in word_candidates:\n",
    "    weight_candidates[candidate] = [total_score_cat[label, words_to_idx[candidate]] for label in range(n_cluster)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cand_to_idx = {k: v for v, k in enumerate(list(weight_candidates.keys()))}\n",
    "weight_cand_matrix = np.array(list(weight_candidates.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-formulate the bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_match_loss(hiddens, alpha=1.0):\n",
    "    device = hiddens.device\n",
    "    hidden_dim = hiddens.shape[-1]\n",
    "    H = np.random.randn(hidden_dim, hidden_dim)\n",
    "    Q, R = qr(H) \n",
    "    rand_w = torch.Tensor(Q).to(device)\n",
    "    loss_dist_match = get_swd_loss(hiddens, rand_w, alpha)\n",
    "    return loss_dist_match\n",
    "\n",
    "\n",
    "def js_div_loss(hidden1, hidden2):\n",
    "    m = 0.5 * (hidden1 + hidden2)\n",
    "    return kldiv(m.log(), hidden1) + kldiv(m.log(), hidden2)\n",
    "\n",
    "\n",
    "def get_swd_loss(states, rand_w, alpha=1.0):\n",
    "    device = states.device\n",
    "    states_shape = states.shape\n",
    "    states = torch.matmul(states, rand_w)\n",
    "    states_t, _ = torch.sort(states.t(), dim=1)\n",
    "\n",
    "    # Random vector with length from normal distribution\n",
    "    states_prior = torch.Tensor(np.random.dirichlet([alpha]*states_shape[1], states_shape[0])).to(device) # (bsz, dim)\n",
    "    states_prior = torch.matmul(states_prior, rand_w) # (dim, dim)\n",
    "    states_prior_t, _ = torch.sort(states_prior.t(), dim=1) # (dim, bsz)\n",
    "    return torch.mean(torch.sum((states_prior_t - states_t)**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2Dataset(Dataset):\n",
    "    def __init__(self, encoder, ds, basesim_matrix, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "            \n",
    "        sim_weight, sim_indices = basesim_matrix.topk(k=k, dim=-1)\n",
    "        zip_iterator = zip(np.arange(len(sim_weight)), sim_indices.squeeze().data.numpy())\n",
    "        self.pos_dict = dict(zip_iterator)\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = self.pos_dict[idx]\n",
    "        return self.embedding_list[idx], self.embedding_list[pos_idx], self.bow_list[idx], self.bow_list[pos_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2TestDataset(Dataset):\n",
    "    def __init__(self, encoder, ds, word_candidates, k=1, lemmatize=False):\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ds = ds\n",
    "        self.org_list = self.ds.org_list\n",
    "        self.nonempty_text = self.ds.nonempty_text\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords_list = set(english_stopwords)\n",
    "        self.vectorizer = CountVectorizer(vocabulary=word_candidates)\n",
    "        self.vectorizer.fit(self.preprocess_ctm(self.nonempty_text)) \n",
    "        self.bow_list = []\n",
    "        for sent in tqdm(self.nonempty_text):\n",
    "            self.bow_list.append(self.vectorize(sent))\n",
    "        \n",
    "        self.embedding_list = []\n",
    "        encoder_device = next(encoder.parameters()).device\n",
    "        for org_input in tqdm(self.org_list):\n",
    "            org_input_ids = org_input['input_ids'].to(encoder_device).reshape(1, -1)\n",
    "            org_attention_mask = org_input['attention_mask'].to(encoder_device).reshape(1, -1)\n",
    "            embedding = encoder(input_ids = org_input_ids, attention_mask = org_attention_mask)\n",
    "            self.embedding_list.append(embedding['pooler_output'].squeeze().detach().cpu())\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.org_list)\n",
    "        \n",
    "    def preprocess_ctm(self, documents):\n",
    "        preprocessed_docs_tmp = documents\n",
    "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [doc.translate(\n",
    "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
    "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords_list])\n",
    "                                 for doc in preprocessed_docs_tmp]\n",
    "        if self.lemmatize:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            preprocessed_docs_tmp = [' '.join([lemmatizer.lemmatize(w) for w in doc.split()])\n",
    "                                     for doc in preprocessed_docs_tmp]\n",
    "        return preprocessed_docs_tmp\n",
    "        \n",
    "    def vectorize(self, text):\n",
    "        text = self.preprocess_ctm([text])\n",
    "        vectorized_input = self.vectorizer.transform(text)\n",
    "        vectorized_input = vectorized_input.toarray().astype(np.float64)\n",
    "#         vectorized_input = (vectorized_input != 0).astype(np.float64)\n",
    "\n",
    "        # Get word distribution from BoW\n",
    "        if vectorized_input.sum() == 0:\n",
    "            vectorized_input += 1e-8\n",
    "        vectorized_input = vectorized_input / vectorized_input.sum(axis=1, keepdims=True)\n",
    "        assert abs(vectorized_input.sum() - vectorized_input.shape[0]) < 0.01\n",
    "        \n",
    "        vectorized_label = torch.tensor(vectorized_input, dtype=torch.float)\n",
    "        return vectorized_label[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embedding_list[idx], self.bow_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:10<00:00, 2310.51it/s]\n",
      "100%|██████████| 25000/25000 [02:13<00:00, 187.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuneds = Stage2Dataset(model.encoder, trainds, basesim_matrix, word_candidates, lemmatize=True)    \n",
    "\n",
    "kldiv = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "vocab_dict = finetuneds.vectorizer.vocabulary_\n",
    "vocab_dict_reverse = {i:v for v, i in vocab_dict.items()}\n",
    "print(n_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_hungarian_score(topic_dist, train_target):\n",
    "    dist = topic_dist\n",
    "    train_target_filtered = train_target\n",
    "    flat_predict = torch.tensor(np.argmax(dist, axis=1))\n",
    "    flat_target = torch.tensor(train_target_filtered).to(flat_predict.device)\n",
    "    num_samples = flat_predict.shape[0]\n",
    "    num_classes = dist.shape[1]\n",
    "    match = _hungarian_match(flat_predict, flat_target, num_samples, num_classes)    \n",
    "    reordered_preds = torch.zeros(num_samples).to(flat_predict.device)\n",
    "    for pred_i, target_i in match:\n",
    "        reordered_preds[flat_predict == pred_i] = int(target_i)\n",
    "    acc = int((reordered_preds == flat_target.float()).sum()) / float(num_samples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_cands = torch.tensor(weight_cand_matrix.max(axis=1)).cuda(gpu_ids[0]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:18<00:00, 1366.15it/s]\n",
      "100%|██████████| 25000/25000 [00:08<00:00, 2994.91it/s]\n",
      "100%|██████████| 25000/25000 [02:49<00:00, 147.59it/s]\n"
     ]
    }
   ],
   "source": [
    "testds = BertDataset(bert=bert_name, text_list=textData.test_data, N_word=n_word, vectorizer=None, lemmatize=True)\n",
    "testds2 = Stage2TestDataset(model.encoder, testds, word_candidates, lemmatize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.22128 - dist: 0.08091 - cons: -0.22642\n",
      "Epoch-1 / recon: 7.06781 - dist: 0.06777 - cons: -0.24033\n",
      "Epoch-2 / recon: 7.01073 - dist: 0.06271 - cons: -0.24752\n",
      "Epoch-3 / recon: 6.98040 - dist: 0.05935 - cons: -0.25301\n",
      "Epoch-4 / recon: 6.96100 - dist: 0.05704 - cons: -0.25784\n",
      "Epoch-5 / recon: 6.94740 - dist: 0.05506 - cons: -0.26184\n",
      "Epoch-6 / recon: 6.93740 - dist: 0.05365 - cons: -0.26469\n",
      "Epoch-7 / recon: 6.92962 - dist: 0.05253 - cons: -0.26701\n",
      "Epoch-8 / recon: 6.92329 - dist: 0.05158 - cons: -0.26923\n",
      "Epoch-9 / recon: 6.91808 - dist: 0.05064 - cons: -0.27140\n",
      "Epoch-10 / recon: 6.91360 - dist: 0.04983 - cons: -0.27324\n",
      "Epoch-11 / recon: 6.90974 - dist: 0.04906 - cons: -0.27478\n",
      "Epoch-12 / recon: 6.90641 - dist: 0.04838 - cons: -0.27598\n",
      "Epoch-13 / recon: 6.90345 - dist: 0.04777 - cons: -0.27725\n",
      "Epoch-14 / recon: 6.90080 - dist: 0.04722 - cons: -0.27836\n",
      "Epoch-15 / recon: 6.89845 - dist: 0.04672 - cons: -0.27933\n",
      "Epoch-16 / recon: 6.89636 - dist: 0.04625 - cons: -0.28013\n",
      "Epoch-17 / recon: 6.89441 - dist: 0.04580 - cons: -0.28104\n",
      "Epoch-18 / recon: 6.89264 - dist: 0.04540 - cons: -0.28179\n",
      "Epoch-19 / recon: 6.89100 - dist: 0.04502 - cons: -0.28247\n",
      "Epoch-20 / recon: 6.88947 - dist: 0.04467 - cons: -0.28311\n",
      "Epoch-21 / recon: 6.88805 - dist: 0.04436 - cons: -0.28365\n",
      "Epoch-22 / recon: 6.88674 - dist: 0.04407 - cons: -0.28413\n",
      "Epoch-23 / recon: 6.88547 - dist: 0.04371 - cons: -0.28480\n",
      "Epoch-24 / recon: 6.88430 - dist: 0.04333 - cons: -0.28541\n",
      "Epoch-25 / recon: 6.88319 - dist: 0.04297 - cons: -0.28599\n",
      "Epoch-26 / recon: 6.88214 - dist: 0.04260 - cons: -0.28665\n",
      "Epoch-27 / recon: 6.88114 - dist: 0.04227 - cons: -0.28720\n",
      "Epoch-28 / recon: 6.88018 - dist: 0.04193 - cons: -0.28766\n",
      "Epoch-29 / recon: 6.87926 - dist: 0.04162 - cons: -0.28819\n",
      "Epoch-30 / recon: 6.87835 - dist: 0.04132 - cons: -0.28875\n",
      "Epoch-31 / recon: 6.87749 - dist: 0.04103 - cons: -0.28918\n",
      "Epoch-32 / recon: 6.87664 - dist: 0.04074 - cons: -0.28957\n",
      "Epoch-33 / recon: 6.87581 - dist: 0.04047 - cons: -0.29004\n",
      "Epoch-34 / recon: 6.87502 - dist: 0.04022 - cons: -0.29040\n",
      "Epoch-35 / recon: 6.87426 - dist: 0.03997 - cons: -0.29077\n",
      "Epoch-36 / recon: 6.87351 - dist: 0.03973 - cons: -0.29115\n",
      "Epoch-37 / recon: 6.87278 - dist: 0.03949 - cons: -0.29153\n",
      "Epoch-38 / recon: 6.87206 - dist: 0.03925 - cons: -0.29192\n",
      "Epoch-39 / recon: 6.87134 - dist: 0.03899 - cons: -0.29231\n",
      "Epoch-40 / recon: 6.87062 - dist: 0.03875 - cons: -0.29272\n",
      "Epoch-41 / recon: 6.86992 - dist: 0.03850 - cons: -0.29318\n",
      "Epoch-42 / recon: 6.86923 - dist: 0.03827 - cons: -0.29360\n",
      "Epoch-43 / recon: 6.86853 - dist: 0.03804 - cons: -0.29401\n",
      "Epoch-44 / recon: 6.86783 - dist: 0.03782 - cons: -0.29439\n",
      "Epoch-45 / recon: 6.86716 - dist: 0.03761 - cons: -0.29477\n",
      "Epoch-46 / recon: 6.86650 - dist: 0.03741 - cons: -0.29511\n",
      "Epoch-47 / recon: 6.86585 - dist: 0.03720 - cons: -0.29550\n",
      "Epoch-48 / recon: 6.86522 - dist: 0.03702 - cons: -0.29586\n",
      "Epoch-49 / recon: 6.86459 - dist: 0.03683 - cons: -0.29623\n",
      "------- Evaluation results -------\n",
      "topic-0 ['movie', 'film', 'bad', 'funny', 'time', 'good', 'comedy', 'acting', 'worst', 'scene', 'watch', 'make', 'money', 'laugh', 'character']\n",
      "topic-1 ['noir', 'crime', 'white', 'criminal', 'flynn', 'prison', 'woman', 'fbi', 'husband', 'gangster', 'heist', 'trial', 'wife', 'detective', 'romantic']\n",
      "topic-2 ['movie', 'kid', 'film', 'child', 'show', 'time', 'good', 'year', 'great', 'story', 'make', 'watch', 'adult', 'watching', 'family']\n",
      "topic-3 ['horror', 'film', 'movie', 'good', 'make', 'story', 'time', 'character', 'house', 'scene', 'great', 'gore', 'director', 'creepy', 'plot']\n",
      "topic-4 ['movie', 'war', 'film', 'kung', 'fight', 'martial', 'scene', 'action', 'good', 'art', 'great', 'time', 'soldier', 'story', 'fighting']\n",
      "topic-5 ['game', 'movie', 'film', 'good', 'time', 'show', 'star', 'series', 'trek', 'fan', 'character', 'story', 'great', 'match', 'sport']\n",
      "topic-6 ['film', 'movie', 'character', 'bad', 'time', 'good', 'scene', 'story', 'make', 'people', 'thing', 'end', 'made', 'man', 'show']\n",
      "topic-7 ['zombie', 'movie', 'film', 'sci', 'alien', 'time', 'bad', 'make', 'good', 'dinosaur', 'animal', 'thing', 'people', 'cartoon', 'story']\n",
      "topic-8 ['movie', 'film', 'comedy', 'woman', 'good', 'character', 'funny', 'romantic', 'love', 'time', 'great', 'make', 'story', 'life', 'watch']\n",
      "topic-9 ['movie', 'film', 'bad', 'music', 'good', 'acting', 'make', 'thing', 'watch', 'story', 'worst', 'scene', 'great', 'time', 'made']\n",
      "topic-10 ['black', 'white', 'car', 'bad', 'movie', 'worst', 'terrible', 'awful', 'race', 'film', 'horror', 'funny', 'character', 'batman', 'unintentionally']\n",
      "topic-11 ['soundtrack', 'musical', 'funny', 'laugh', 'bollywood', 'movie', 'comedy', 'music', 'laughed', 'humor', 'joke', 'humour', 'song', 'cue', 'funniest']\n",
      "topic-12 ['slasher', 'killer', 'cop', 'killed', 'harry', 'prison', 'teacher', 'murder', 'criminal', 'kid', 'dirty', 'excessive', 'police', 'uncle', 'killing']\n",
      "topic-13 ['horror', 'killer', 'gore', 'bad', 'slasher', 'awful', 'scary', 'waste', 'worst', 'terrible', 'budget', 'crap', 'war', 'acting', 'death']\n",
      "topic-14 ['game', 'trek', 'season', 'baseball', 'episode', 'series', 'match', 'carter', 'anime', 'seagal', 'mario', 'sport', 'navy', 'kenneth', 'sci']\n",
      "topic-15 ['martial', 'villain', 'spectacular', 'fight', 'cop', 'noir', 'gangster', 'action', 'criminal', 'brother', 'mountain', 'hero', 'crime', 'police', 'battle']\n",
      "topic-16 ['war', 'sidney', 'daughter', 'civil', 'emma', 'broadway', 'emily', 'jane', 'stewart', 'catholic', 'gay', 'mother', 'sexuality', 'mgm', 'marry']\n",
      "topic-17 ['zombie', 'porn', 'horror', 'porno', 'gore', 'monster', 'sex', 'scientist', 'nudity', 'scary', 'trained', 'sci', 'scare', 'eaten', 'creature']\n",
      "topic-18 ['movie', 'film', 'wife', 'good', 'love', 'character', 'story', 'mother', 'daughter', 'husband', 'time', 'great', 'life', 'woman', 'make']\n",
      "topic-19 ['movie', 'film', 'bad', 'scene', 'good', 'acting', 'worst', 'time', 'story', 'watch', 'make', 'end', 'plot', 'people', 'great']\n",
      "topic-20 ['horror', 'killer', 'gore', 'slasher', 'murder', 'crime', 'death', 'cop', 'scary', 'killing', 'waste', 'bad', 'blood', 'creepy', 'awful']\n",
      "topic-21 ['music', 'song', 'soundtrack', 'band', 'movie', 'musical', 'film', 'good', 'bollywood', 'actor', 'acting', 'score', 'bad', 'awful', 'story']\n",
      "topic-22 ['movie', 'film', 'sex', 'gay', 'scene', 'character', 'good', 'story', 'make', 'time', 'bad', 'woman', 'acting', 'people', 'sexual']\n",
      "topic-23 ['movie', 'good', 'film', 'bad', 'time', 'funny', 'watch', 'acting', 'worst', 'story', 'great', 'make', 'plot', 'watching', 'made']\n",
      "topic-24 ['book', 'movie', 'film', 'christmas', 'read', 'story', 'good', 'character', 'santa', 'time', 'version', 'great', 'make', 'made', 'bad']\n",
      "topic-25 ['music', 'band', 'song', 'soundtrack', 'musical', 'bollywood', 'rock', 'kid', 'singer', 'terrible', 'scary', 'awful', 'waste', 'worst', 'movie']\n",
      "topic-26 ['movie', 'film', 'good', 'killer', 'time', 'bad', 'character', 'scene', 'slasher', 'people', 'plot', 'make', 'great', 'acting', 'made']\n",
      "topic-27 ['documentary', 'country', 'american', 'political', 'film', 'world', 'people', 'politics', 'nation', 'war', 'society', 'london', 'powerful', 'movie', 'british']\n",
      "topic-28 ['movie', 'film', 'father', 'family', 'son', 'life', 'story', 'good', 'character', 'brother', 'time', 'child', 'love', 'mother', 'man']\n",
      "topic-29 ['film', 'murder', 'movie', 'character', 'killer', 'good', 'scene', 'time', 'story', 'crime', 'make', 'show', 'man', 'detective', 'great']\n",
      "topic-30 ['worst', 'car', 'stupid', 'movie', 'bad', 'asleep', 'crap', 'worthless', 'cover', 'drove', 'store', 'sell', 'rent', 'insane', 'mirror']\n",
      "topic-31 ['movie', 'film', 'batman', 'black', 'white', 'eddie', 'good', 'people', 'time', 'superman', 'funny', 'show', 'make', 'car', 'murphy']\n",
      "topic-32 ['musical', 'movie', 'film', 'song', 'dance', 'great', 'number', 'time', 'dancing', 'show', 'good', 'music', 'sinatra', 'singing', 'kelly']\n",
      "topic-33 ['movie', 'film', 'character', 'good', 'cop', 'time', 'story', 'make', 'great', 'action', 'plot', 'police', 'gangster', 'made', 'bad']\n",
      "topic-34 ['film', 'movie', 'character', 'story', 'time', 'life', 'love', 'make', 'scene', 'woman', 'show', 'man', 'good', 'year', 'work']\n",
      "topic-35 ['film', 'movie', 'good', 'character', 'acting', 'director', 'story', 'bad', 'make', 'time', 'actor', 'great', 'script', 'plot', 'made']\n",
      "topic-36 ['movie', 'film', 'music', 'band', 'rock', 'good', 'time', 'song', 'great', 'story', 'indian', 'watch', 'scene', 'fan', 'make']\n",
      "topic-37 ['movie', 'film', 'story', 'japanese', 'time', 'character', 'good', 'people', 'french', 'russian', 'hitler', 'german', 'great', 'scene', 'actor']\n",
      "topic-38 ['character', 'film', 'bad', 'good', 'plot', 'scene', 'role', 'main', 'movie', 'woman', 'cop', 'story', 'thing', 'make', 'acting']\n",
      "topic-39 ['movie', 'film', 'time', 'good', 'great', 'love', 'song', 'make', 'story', 'music', 'scene', 'character', 'people', 'watch', 'girl']\n",
      "topic-40 ['movie', 'film', 'bad', 'horror', 'good', 'acting', 'time', 'watch', 'worst', 'plot', 'awful', 'make', 'actor', 'scary', 'made']\n",
      "topic-41 ['cop', 'villain', 'fight', 'charisma', 'action', 'cool', 'bad', 'disturbing', 'gun', 'menacing', 'dog', 'police', 'scene', 'pace', 'brother']\n",
      "topic-42 ['film', 'character', 'review', 'horror', 'script', 'terrible', 'director', 'camera', 'bad', 'dialogue', 'writer', 'scary', 'writing', 'badly', 'disney']\n",
      "topic-43 ['movie', 'film', 'disney', 'animation', 'character', 'cartoon', 'good', 'story', 'animated', 'time', 'great', 'school', 'make', 'student', 'year']\n",
      "topic-44 ['movie', 'film', 'scary', 'horror', 'story', 'good', 'people', 'time', 'ghost', 'make', 'christian', 'jesus', 'great', 'scared', 'made']\n",
      "topic-45 ['show', 'episode', 'season', 'series', 'character', 'good', 'great', 'watch', 'funny', 'time', 'love', 'back', 'people', 'make', 'family']\n",
      "topic-46 ['movie', 'vampire', 'film', 'werewolf', 'bad', 'good', 'scene', 'horror', 'character', 'time', 'make', 'monster', 'story', 'thing', 'dracula']\n",
      "topic-47 ['movie', 'film', 'western', 'soldier', 'good', 'time', 'story', 'war', 'scene', 'great', 'character', 'action', 'bad', 'military', 'make']\n",
      "topic-48 ['movie', 'film', 'good', 'character', 'bad', 'time', 'make', 'story', 'great', 'actor', 'scene', 'made', 'thing', 'people', 'plot']\n",
      "topic-49 ['family', 'daughter', 'kid', 'mother', 'child', 'parent', 'pregnant', 'mom', 'son', 'funny', 'adult', 'baby', 'father', 'occur', 'sister']\n",
      "./\n",
      "[0.46201, 0.35964, 0.28406, 0.33122, 0.31613, 0.33301, 0.36705, 0.30369, 0.33171, 0.47067, 0.3934, 0.36879, 0.34412, 0.44266, 0.38298, 0.32737, 0.31132, 0.53433, 0.32851, 0.42031, 0.40813, 0.44677, 0.40474, 0.4371, 0.36663, 0.4811, 0.43197, 0.28706, 0.34305, 0.33626, 0.4218, 0.30575, 0.45095, 0.38456, 0.31444, 0.46425, 0.33406, 0.31568, 0.40476, 0.32767, 0.48062, 0.26879, 0.32489, 0.41047, 0.37777, 0.30343, 0.37403, 0.30463, 0.38935, 0.3865]\n",
      "0.3740038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 372.62it/s]\n",
      "196it [00:00, 547.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'CV_wiki': 0.3740038, 'sim_w2v': 0.1900792576881946, 'diversity': 0.31466666666666665, 'filename': 'results/240510_161129.txt', 'acc': 0.5028, 'macro-F1': 0.5002894670751754, 'Purity': 0.58096, 'NMI': 0.008206916704606689}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.22208 - dist: 0.08663 - cons: -0.21496\n",
      "Epoch-1 / recon: 7.06839 - dist: 0.07394 - cons: -0.23816\n",
      "Epoch-2 / recon: 7.01209 - dist: 0.06934 - cons: -0.24720\n",
      "Epoch-3 / recon: 6.98304 - dist: 0.06690 - cons: -0.25261\n",
      "Epoch-4 / recon: 6.96497 - dist: 0.06500 - cons: -0.25579\n",
      "Epoch-5 / recon: 6.95242 - dist: 0.06325 - cons: -0.25882\n",
      "Epoch-6 / recon: 6.94343 - dist: 0.06188 - cons: -0.26166\n",
      "Epoch-7 / recon: 6.93643 - dist: 0.06082 - cons: -0.26369\n",
      "Epoch-8 / recon: 6.93057 - dist: 0.05966 - cons: -0.26540\n",
      "Epoch-9 / recon: 6.92562 - dist: 0.05839 - cons: -0.26721\n",
      "Epoch-10 / recon: 6.92120 - dist: 0.05714 - cons: -0.26874\n",
      "Epoch-11 / recon: 6.91735 - dist: 0.05593 - cons: -0.27040\n",
      "Epoch-12 / recon: 6.91396 - dist: 0.05485 - cons: -0.27198\n",
      "Epoch-13 / recon: 6.91100 - dist: 0.05381 - cons: -0.27333\n",
      "Epoch-14 / recon: 6.90832 - dist: 0.05285 - cons: -0.27452\n",
      "Epoch-15 / recon: 6.90586 - dist: 0.05199 - cons: -0.27579\n",
      "Epoch-16 / recon: 6.90361 - dist: 0.05119 - cons: -0.27675\n",
      "Epoch-17 / recon: 6.90157 - dist: 0.05047 - cons: -0.27761\n",
      "Epoch-18 / recon: 6.89973 - dist: 0.04981 - cons: -0.27847\n",
      "Epoch-19 / recon: 6.89802 - dist: 0.04920 - cons: -0.27927\n",
      "Epoch-20 / recon: 6.89647 - dist: 0.04863 - cons: -0.27998\n",
      "Epoch-21 / recon: 6.89496 - dist: 0.04808 - cons: -0.28072\n",
      "Epoch-22 / recon: 6.89354 - dist: 0.04754 - cons: -0.28133\n",
      "Epoch-23 / recon: 6.89220 - dist: 0.04700 - cons: -0.28203\n",
      "Epoch-24 / recon: 6.89092 - dist: 0.04650 - cons: -0.28257\n",
      "Epoch-25 / recon: 6.88965 - dist: 0.04602 - cons: -0.28319\n",
      "Epoch-26 / recon: 6.88843 - dist: 0.04554 - cons: -0.28381\n",
      "Epoch-27 / recon: 6.88725 - dist: 0.04510 - cons: -0.28434\n",
      "Epoch-28 / recon: 6.88612 - dist: 0.04469 - cons: -0.28487\n",
      "Epoch-29 / recon: 6.88505 - dist: 0.04427 - cons: -0.28541\n",
      "Epoch-30 / recon: 6.88401 - dist: 0.04385 - cons: -0.28595\n",
      "Epoch-31 / recon: 6.88297 - dist: 0.04345 - cons: -0.28650\n",
      "Epoch-32 / recon: 6.88199 - dist: 0.04307 - cons: -0.28699\n",
      "Epoch-33 / recon: 6.88104 - dist: 0.04272 - cons: -0.28750\n",
      "Epoch-34 / recon: 6.88012 - dist: 0.04238 - cons: -0.28800\n",
      "Epoch-35 / recon: 6.87924 - dist: 0.04206 - cons: -0.28843\n",
      "Epoch-36 / recon: 6.87839 - dist: 0.04176 - cons: -0.28887\n",
      "Epoch-37 / recon: 6.87759 - dist: 0.04147 - cons: -0.28924\n",
      "Epoch-38 / recon: 6.87681 - dist: 0.04121 - cons: -0.28957\n",
      "Epoch-39 / recon: 6.87607 - dist: 0.04094 - cons: -0.28990\n",
      "Epoch-40 / recon: 6.87534 - dist: 0.04069 - cons: -0.29028\n",
      "Epoch-41 / recon: 6.87464 - dist: 0.04043 - cons: -0.29060\n",
      "Epoch-42 / recon: 6.87396 - dist: 0.04020 - cons: -0.29091\n",
      "Epoch-43 / recon: 6.87332 - dist: 0.03996 - cons: -0.29117\n",
      "Epoch-44 / recon: 6.87267 - dist: 0.03974 - cons: -0.29149\n",
      "Epoch-45 / recon: 6.87204 - dist: 0.03952 - cons: -0.29184\n",
      "Epoch-46 / recon: 6.87138 - dist: 0.03930 - cons: -0.29219\n",
      "Epoch-47 / recon: 6.87073 - dist: 0.03909 - cons: -0.29256\n",
      "Epoch-48 / recon: 6.87010 - dist: 0.03889 - cons: -0.29288\n",
      "Epoch-49 / recon: 6.86947 - dist: 0.03870 - cons: -0.29318\n",
      "------- Evaluation results -------\n",
      "topic-0 ['band', 'music', 'song', 'bollywood', 'musical', 'soundtrack', 'film', 'rock', 'woody', 'character', 'performance', 'dancing', 'movie', 'actor', 'great']\n",
      "topic-1 ['film', 'horror', 'vampire', 'sex', 'scene', 'war', 'character', 'movie', 'woman', 'make', 'good', 'white', 'gay', 'werewolf', 'story']\n",
      "topic-2 ['movie', 'bad', 'film', 'good', 'time', 'acting', 'plot', 'car', 'scene', 'worst', 'awful', 'character', 'make', 'story', 'watch']\n",
      "topic-3 ['film', 'movie', 'story', 'french', 'good', 'time', 'character', 'italian', 'great', 'people', 'scene', 'life', 'actor', 'make', 'love']\n",
      "topic-4 ['film', 'horror', 'movie', 'good', 'character', 'story', 'make', 'time', 'house', 'scene', 'great', 'gore', 'creepy', 'bad', 'director']\n",
      "topic-5 ['movie', 'film', 'family', 'father', 'son', 'story', 'life', 'time', 'good', 'brother', 'character', 'child', 'love', 'baby', 'man']\n",
      "topic-6 ['broadway', 'bollywood', 'musical', 'prince', 'dance', 'song', 'singing', 'emma', 'daughter', 'firm', 'dancing', 'sing', 'sister', 'sung', 'unexpectedly']\n",
      "topic-7 ['bad', 'worst', 'awful', 'terrible', 'zombie', 'sucked', 'crap', 'waste', 'sci', 'kid', 'asleep', 'movie', 'disney', 'avoid', 'action']\n",
      "topic-8 ['prison', 'mother', 'daughter', 'murder', 'noir', 'gangster', 'cop', 'crime', 'criminal', 'husband', 'father', 'married', 'wife', 'woman', 'killer']\n",
      "topic-9 ['film', 'movie', 'musical', 'music', 'dance', 'song', 'band', 'rock', 'great', 'number', 'good', 'time', 'show', 'dancing', 'make']\n",
      "topic-10 ['movie', 'film', 'bad', 'music', 'good', 'horror', 'acting', 'scene', 'time', 'make', 'worst', 'watch', 'thing', 'made', 'story']\n",
      "topic-11 ['show', 'episode', 'season', 'series', 'character', 'good', 'funny', 'watch', 'great', 'time', 'love', 'people', 'back', 'watching', 'make']\n",
      "topic-12 ['movie', 'kid', 'child', 'film', 'show', 'good', 'time', 'year', 'great', 'watch', 'story', 'adult', 'make', 'character', 'thing']\n",
      "topic-13 ['zombie', 'slasher', 'gore', 'horror', 'excessive', 'scare', 'killer', 'student', 'instantly', 'christmas', 'nightmare', 'spooky', 'scary', 'vampire', 'creepy']\n",
      "topic-14 ['movie', 'good', 'funny', 'film', 'time', 'bad', 'watch', 'great', 'acting', 'story', 'make', 'comedy', 'laugh', 'plot', 'made']\n",
      "topic-15 ['movie', 'zombie', 'film', 'time', 'good', 'sci', 'alien', 'make', 'bad', 'people', 'story', 'trek', 'animal', 'cartoon', 'thing']\n",
      "topic-16 ['horror', 'bad', 'vampire', 'movie', 'kid', 'scene', 'film', 'war', 'worst', 'effect', 'sex', 'sci', 'awful', 'acting', 'plot']\n",
      "topic-17 ['book', 'movie', 'christmas', 'film', 'read', 'story', 'character', 'santa', 'good', 'time', 'version', 'made', 'make', 'great', 'bad']\n",
      "topic-18 ['film', 'movie', 'character', 'funny', 'bad', 'actor', 'people', 'great', 'good', 'acting', 'plot', 'time', 'watch', 'story', 'make']\n",
      "topic-19 ['war', 'battle', 'film', 'soldier', 'political', 'military', 'american', 'movie', 'army', 'documentary', 'world', 'time', 'people', 'english', 'real']\n",
      "topic-20 ['horror', 'sci', 'zombie', 'vampire', 'bad', 'monster', 'war', 'effect', 'film', 'scene', 'animal', 'movie', 'kid', 'awful', 'budget']\n",
      "topic-21 ['family', 'father', 'son', 'daughter', 'mother', 'husband', 'parent', 'life', 'sister', 'baby', 'wife', 'brother', 'child', 'authority', 'love']\n",
      "topic-22 ['funny', 'joke', 'band', 'laugh', 'movie', 'comedy', 'humor', 'music', 'funnier', 'song', 'hilarious', 'rock', 'soundtrack', 'laughing', 'book']\n",
      "topic-23 ['movie', 'film', 'killer', 'good', 'time', 'character', 'slasher', 'bad', 'scene', 'people', 'guy', 'end', 'make', 'kill', 'plot']\n",
      "topic-24 ['woman', 'husband', 'wife', 'war', 'romantic', 'mother', 'married', 'men', 'soldier', 'french', 'white', 'daughter', 'italian', 'young', 'noir']\n",
      "topic-25 ['film', 'movie', 'school', 'bad', 'time', 'student', 'character', 'good', 'scene', 'make', 'acting', 'high', 'people', 'plot', 'watching']\n",
      "topic-26 ['band', 'music', 'musical', 'rock', 'song', 'bollywood', 'soundtrack', 'dancing', 'singer', 'musician', 'dance', 'woody', 'punk', 'singing', 'concert']\n",
      "topic-27 ['movie', 'film', 'wife', 'good', 'mother', 'story', 'love', 'husband', 'daughter', 'life', 'character', 'great', 'woman', 'time', 'make']\n",
      "topic-28 ['movie', 'game', 'film', 'good', 'kung', 'fight', 'martial', 'great', 'scene', 'time', 'character', 'action', 'art', 'story', 'fan']\n",
      "topic-29 ['bad', 'terrible', 'worst', 'awful', 'movie', 'plot', 'waste', 'acting', 'funny', 'crap', 'sci', 'film', 'sucked', 'joke', 'script']\n",
      "topic-30 ['film', 'movie', 'character', 'time', 'bad', 'good', 'scene', 'make', 'thing', 'story', 'man', 'made', 'people', 'end', 'guy']\n",
      "topic-31 ['movie', 'film', 'bad', 'worst', 'good', 'acting', 'watch', 'time', 'made', 'make', 'thing', 'terrible', 'scene', 'plot', 'effect']\n",
      "topic-32 ['film', 'movie', 'good', 'director', 'character', 'acting', 'time', 'story', 'bad', 'actor', 'script', 'make', 'great', 'performance', 'plot']\n",
      "topic-33 ['film', 'murder', 'movie', 'killer', 'good', 'scene', 'character', 'crime', 'time', 'story', 'man', 'make', 'detective', 'police', 'great']\n",
      "topic-34 ['movie', 'film', 'comedy', 'good', 'romantic', 'love', 'funny', 'woman', 'character', 'time', 'story', 'scene', 'watch', 'great', 'make']\n",
      "topic-35 ['movie', 'film', 'sex', 'gay', 'scene', 'character', 'good', 'bad', 'story', 'time', 'acting', 'plot', 'make', 'porn', 'sexual']\n",
      "topic-36 ['movie', 'film', 'time', 'love', 'good', 'song', 'great', 'character', 'make', 'story', 'scene', 'music', 'watch', 'girl', 'acting']\n",
      "topic-37 ['kid', 'fooled', 'nostalgia', 'minimal', 'disney', 'cynical', 'cry', 'inane', 'explains', 'movie', 'scary', 'school', 'decided', 'remembered', 'joke']\n",
      "topic-38 ['movie', 'film', 'scary', 'horror', 'good', 'time', 'story', 'make', 'christian', 'people', 'ghost', 'jesus', 'scared', 'scene', 'god']\n",
      "topic-39 ['trek', 'episode', 'stewart', 'catholic', 'season', 'gentleman', 'war', 'clark', 'western', 'jesus', 'series', 'captain', 'suffered', 'christ', 'flynn']\n",
      "topic-40 ['movie', 'film', 'character', 'good', 'time', 'story', 'great', 'make', 'black', 'comedy', 'actor', 'action', 'scene', 'white', 'bad']\n",
      "topic-41 ['war', 'movie', 'film', 'soldier', 'time', 'good', 'story', 'western', 'military', 'great', 'character', 'people', 'scene', 'action', 'army']\n",
      "topic-42 ['disney', 'movie', 'film', 'animation', 'story', 'cartoon', 'character', 'animated', 'time', 'good', 'great', 'make', 'king', 'lion', 'made']\n",
      "topic-43 ['movie', 'film', 'music', 'good', 'time', 'story', 'song', 'great', 'watch', 'indian', 'scene', 'make', 'character', 'love', 'acting']\n",
      "topic-44 ['movie', 'film', 'japanese', 'story', 'good', 'people', 'russian', 'german', 'time', 'hitler', 'made', 'american', 'character', 'chinese', 'political']\n",
      "topic-45 ['movie', 'vampire', 'film', 'scene', 'bad', 'make', 'time', 'good', 'hunter', 'character', 'story', 'plot', 'monster', 'werewolf', 'horror']\n",
      "topic-46 ['film', 'war', 'battle', 'movie', 'bad', 'vampire', 'sci', 'soldier', 'acting', 'fight', 'action', 'scene', 'time', 'game', 'director']\n",
      "topic-47 ['film', 'movie', 'character', 'make', 'story', 'time', 'life', 'love', 'people', 'scene', 'good', 'show', 'work', 'man', 'thing']\n",
      "topic-48 ['zombie', 'horror', 'scary', 'kid', 'killer', 'slasher', 'gore', 'disney', 'worst', 'awful', 'scare', 'bad', 'sci', 'waste', 'school']\n",
      "topic-49 ['movie', 'film', 'batman', 'eddie', 'black', 'good', 'time', 'white', 'superman', 'character', 'people', 'make', 'murphy', 'show', 'series']\n",
      "./\n",
      "[0.46843, 0.33733, 0.44598, 0.31693, 0.32192, 0.33013, 0.45712, 0.46644, 0.36291, 0.43227, 0.48677, 0.29478, 0.30132, 0.40909, 0.44542, 0.30545, 0.45334, 0.36663, 0.40436, 0.32094, 0.33979, 0.42223, 0.42917, 0.31967, 0.30107, 0.38527, 0.53753, 0.32851, 0.31587, 0.49804, 0.37461, 0.48021, 0.41771, 0.34656, 0.33966, 0.41505, 0.41644, 0.44667, 0.34067, 0.30517, 0.32726, 0.3177, 0.44974, 0.40974, 0.37489, 0.36056, 0.3879, 0.31311, 0.44299, 0.32027]\n",
      "0.3838324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 401.64it/s]\n",
      "196it [00:00, 579.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'CV_wiki': 0.3838324, 'sim_w2v': 0.1974983770854452, 'diversity': 0.26666666666666666, 'filename': 'results/240510_161651.txt', 'acc': 0.49592, 'macro-F1': 0.491215398222163, 'Purity': 0.5828, 'NMI': 0.008682843844335339}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.22172 - dist: 0.08060 - cons: -0.21695\n",
      "Epoch-1 / recon: 7.06786 - dist: 0.06930 - cons: -0.23674\n",
      "Epoch-2 / recon: 7.01181 - dist: 0.06499 - cons: -0.24456\n",
      "Epoch-3 / recon: 6.98303 - dist: 0.06275 - cons: -0.24917\n",
      "Epoch-4 / recon: 6.96558 - dist: 0.06131 - cons: -0.25185\n",
      "Epoch-5 / recon: 6.95378 - dist: 0.06030 - cons: -0.25393\n",
      "Epoch-6 / recon: 6.94526 - dist: 0.05959 - cons: -0.25542\n",
      "Epoch-7 / recon: 6.93876 - dist: 0.05898 - cons: -0.25674\n",
      "Epoch-8 / recon: 6.93355 - dist: 0.05848 - cons: -0.25781\n",
      "Epoch-9 / recon: 6.92940 - dist: 0.05804 - cons: -0.25869\n",
      "Epoch-10 / recon: 6.92577 - dist: 0.05746 - cons: -0.25956\n",
      "Epoch-11 / recon: 6.92253 - dist: 0.05693 - cons: -0.26035\n",
      "Epoch-12 / recon: 6.91954 - dist: 0.05644 - cons: -0.26117\n",
      "Epoch-13 / recon: 6.91686 - dist: 0.05597 - cons: -0.26196\n",
      "Epoch-14 / recon: 6.91437 - dist: 0.05531 - cons: -0.26284\n",
      "Epoch-15 / recon: 6.91214 - dist: 0.05461 - cons: -0.26359\n",
      "Epoch-16 / recon: 6.91004 - dist: 0.05395 - cons: -0.26431\n",
      "Epoch-17 / recon: 6.90815 - dist: 0.05337 - cons: -0.26493\n",
      "Epoch-18 / recon: 6.90637 - dist: 0.05285 - cons: -0.26567\n",
      "Epoch-19 / recon: 6.90474 - dist: 0.05236 - cons: -0.26624\n",
      "Epoch-20 / recon: 6.90322 - dist: 0.05188 - cons: -0.26680\n",
      "Epoch-21 / recon: 6.90181 - dist: 0.05140 - cons: -0.26743\n",
      "Epoch-22 / recon: 6.90047 - dist: 0.05093 - cons: -0.26800\n",
      "Epoch-23 / recon: 6.89919 - dist: 0.05050 - cons: -0.26863\n",
      "Epoch-24 / recon: 6.89796 - dist: 0.05011 - cons: -0.26913\n",
      "Epoch-25 / recon: 6.89680 - dist: 0.04969 - cons: -0.26969\n",
      "Epoch-26 / recon: 6.89568 - dist: 0.04927 - cons: -0.27030\n",
      "Epoch-27 / recon: 6.89458 - dist: 0.04888 - cons: -0.27090\n",
      "Epoch-28 / recon: 6.89351 - dist: 0.04851 - cons: -0.27142\n",
      "Epoch-29 / recon: 6.89243 - dist: 0.04815 - cons: -0.27197\n",
      "Epoch-30 / recon: 6.89140 - dist: 0.04778 - cons: -0.27258\n",
      "Epoch-31 / recon: 6.89036 - dist: 0.04739 - cons: -0.27334\n",
      "Epoch-32 / recon: 6.88935 - dist: 0.04701 - cons: -0.27407\n",
      "Epoch-33 / recon: 6.88837 - dist: 0.04665 - cons: -0.27477\n",
      "Epoch-34 / recon: 6.88743 - dist: 0.04631 - cons: -0.27546\n",
      "Epoch-35 / recon: 6.88651 - dist: 0.04598 - cons: -0.27615\n",
      "Epoch-36 / recon: 6.88561 - dist: 0.04567 - cons: -0.27680\n",
      "Epoch-37 / recon: 6.88473 - dist: 0.04536 - cons: -0.27738\n",
      "Epoch-38 / recon: 6.88388 - dist: 0.04507 - cons: -0.27799\n",
      "Epoch-39 / recon: 6.88302 - dist: 0.04476 - cons: -0.27861\n",
      "Epoch-40 / recon: 6.88220 - dist: 0.04444 - cons: -0.27920\n",
      "Epoch-41 / recon: 6.88139 - dist: 0.04412 - cons: -0.27977\n",
      "Epoch-42 / recon: 6.88060 - dist: 0.04382 - cons: -0.28035\n",
      "Epoch-43 / recon: 6.87982 - dist: 0.04352 - cons: -0.28092\n",
      "Epoch-44 / recon: 6.87905 - dist: 0.04324 - cons: -0.28151\n",
      "Epoch-45 / recon: 6.87829 - dist: 0.04297 - cons: -0.28203\n",
      "Epoch-46 / recon: 6.87755 - dist: 0.04271 - cons: -0.28256\n",
      "Epoch-47 / recon: 6.87681 - dist: 0.04246 - cons: -0.28306\n",
      "Epoch-48 / recon: 6.87606 - dist: 0.04219 - cons: -0.28354\n",
      "Epoch-49 / recon: 6.87534 - dist: 0.04193 - cons: -0.28401\n",
      "------- Evaluation results -------\n",
      "topic-0 ['movie', 'film', 'fight', 'action', 'good', 'scene', 'kung', 'martial', 'great', 'character', 'art', 'time', 'story', 'bad', 'fighting']\n",
      "topic-1 ['horror', 'film', 'movie', 'good', 'story', 'house', 'make', 'character', 'time', 'gore', 'scene', 'creepy', 'great', 'director', 'scare']\n",
      "topic-2 ['movie', 'film', 'music', 'band', 'rock', 'song', 'good', 'time', 'great', 'indian', 'watch', 'story', 'make', 'bollywood', 'scene']\n",
      "topic-3 ['show', 'episode', 'season', 'series', 'character', 'great', 'good', 'watch', 'funny', 'time', 'love', 'back', 'watching', 'story', 'make']\n",
      "topic-4 ['movie', 'kid', 'child', 'film', 'show', 'time', 'good', 'year', 'great', 'watch', 'story', 'adult', 'parent', 'make', 'watching']\n",
      "topic-5 ['movie', 'scary', 'film', 'horror', 'good', 'killer', 'bad', 'worst', 'time', 'ghost', 'watch', 'scared', 'slasher', 'acting', 'halloween']\n",
      "topic-6 ['sex', 'porn', 'nudity', 'horror', 'porno', 'woman', 'scene', 'nude', 'naked', 'hot', 'sexual', 'gore', 'rape', 'girl', 'movie']\n",
      "topic-7 ['game', 'baseball', 'jesus', 'christian', 'anime', 'sport', 'match', 'christ', 'catholic', 'football', 'church', 'joseph', 'bible', 'mario', 'religious']\n",
      "topic-8 ['horror', 'scary', 'gore', 'killer', 'school', 'bad', 'zombie', 'slasher', 'worst', 'effect', 'movie', 'killing', 'scare', 'crappy', 'war']\n",
      "topic-9 ['sex', 'school', 'student', 'movie', 'sexual', 'kid', 'film', 'scene', 'gay', 'bad', 'rape', 'guy', 'college', 'boy', 'girl']\n",
      "topic-10 ['movie', 'film', 'horror', 'scary', 'time', 'good', 'make', 'ghost', 'story', 'scared', 'bad', 'watch', 'made', 'scene', 'acting']\n",
      "topic-11 ['music', 'song', 'soundtrack', 'musical', 'movie', 'bad', 'kid', 'band', 'worst', 'film', 'great', 'waste', 'end', 'terrible', 'awful']\n",
      "topic-12 ['movie', 'film', 'comedy', 'romantic', 'character', 'woman', 'love', 'funny', 'good', 'time', 'story', 'great', 'scene', 'make', 'romance']\n",
      "topic-13 ['movie', 'film', 'bad', 'music', 'good', 'time', 'acting', 'watch', 'scene', 'story', 'make', 'made', 'great', 'thing', 'actor']\n",
      "topic-14 ['film', 'murder', 'movie', 'killer', 'character', 'good', 'crime', 'time', 'scene', 'story', 'man', 'make', 'police', 'performance', 'prison']\n",
      "topic-15 ['movie', 'disney', 'film', 'animation', 'character', 'animated', 'cartoon', 'good', 'story', 'time', 'school', 'make', 'great', 'student', 'show']\n",
      "topic-16 ['movie', 'film', 'time', 'trek', 'eddie', 'show', 'people', 'good', 'story', 'star', 'car', 'episode', 'make', 'funny', 'murphy']\n",
      "topic-17 ['film', 'movie', 'character', 'time', 'story', 'good', 'scene', 'show', 'make', 'thing', 'people', 'made', 'bad', 'man', 'book']\n",
      "topic-18 ['horror', 'gore', 'music', 'soundtrack', 'killer', 'song', 'slasher', 'monster', 'scary', 'zombie', 'band', 'creature', 'student', 'scare', 'bad']\n",
      "topic-19 ['movie', 'film', 'good', 'funny', 'time', 'bad', 'watch', 'worst', 'great', 'acting', 'laugh', 'plot', 'story', 'made', 'make']\n",
      "topic-20 ['sister', 'family', 'brother', 'father', 'marry', 'roy', 'marie', 'france', 'french', 'daughter', 'stewart', 'western', 'dan', 'flynn', 'grandfather']\n",
      "topic-21 ['sex', 'gay', 'woman', 'sexual', 'porn', 'male', 'rape', 'nudity', 'nude', 'naked', 'lesbian', 'female', 'erotic', 'scene', 'boyfriend']\n",
      "topic-22 ['character', 'challenging', 'film', 'sexual', 'role', 'gay', 'romance', 'prison', 'action', 'criminal', 'plot', 'painfully', 'job', 'relate', 'great']\n",
      "topic-23 ['movie', 'film', 'awful', 'bad', 'utter', 'actor', 'deliberately', 'classic', 'fate', 'effect', 'story', 'special', 'category', 'made', 'zombie']\n",
      "topic-24 ['movie', 'film', 'time', 'good', 'great', 'song', 'love', 'make', 'music', 'character', 'story', 'scene', 'people', 'made', 'life']\n",
      "topic-25 ['zombie', 'movie', 'film', 'sci', 'alien', 'make', 'time', 'good', 'bad', 'dinosaur', 'animal', 'people', 'thing', 'cartoon', 'cat']\n",
      "topic-26 ['funny', 'comedy', 'joke', 'laugh', 'humor', 'family', 'christmas', 'kid', 'laughed', 'movie', 'funnier', 'hilarious', 'comedian', 'laughter', 'humour']\n",
      "topic-27 ['vampire', 'movie', 'film', 'werewolf', 'bad', 'scene', 'good', 'horror', 'monster', 'time', 'make', 'character', 'hunter', 'dracula', 'plot']\n",
      "topic-28 ['movie', 'film', 'wife', 'mother', 'love', 'good', 'daughter', 'story', 'character', 'husband', 'time', 'great', 'life', 'woman', 'young']\n",
      "topic-29 ['movie', 'film', 'killer', 'good', 'bad', 'slasher', 'time', 'character', 'acting', 'scene', 'plot', 'end', 'death', 'people', 'make']\n",
      "topic-30 ['film', 'movie', 'good', 'story', 'actor', 'character', 'french', 'director', 'acting', 'time', 'great', 'bad', 'make', 'script', 'italian']\n",
      "topic-31 ['film', 'kid', 'movie', 'school', 'student', 'making', 'scene', 'time', 'written', 'high', 'script', 'character', 'acting', 'flying', 'people']\n",
      "topic-32 ['music', 'movie', 'book', 'worst', 'bad', 'awful', 'song', 'soundtrack', 'terrible', 'good', 'watch', 'acting', 'thing', 'waste', 'made']\n",
      "topic-33 ['war', 'film', 'movie', 'soldier', 'western', 'time', 'good', 'military', 'story', 'army', 'world', 'american', 'action', 'great', 'year']\n",
      "topic-34 ['musical', 'film', 'movie', 'song', 'dance', 'number', 'dancing', 'great', 'time', 'music', 'story', 'singing', 'show', 'character', 'good']\n",
      "topic-35 ['movie', 'film', 'batman', 'black', 'good', 'character', 'white', 'superman', 'time', 'story', 'people', 'great', 'make', 'cop', 'guy']\n",
      "topic-36 ['book', 'movie', 'christmas', 'film', 'read', 'story', 'santa', 'family', 'good', 'character', 'time', 'version', 'great', 'made', 'make']\n",
      "topic-37 ['film', 'bad', 'movie', 'awful', 'worst', 'acting', 'character', 'plot', 'make', 'scene', 'end', 'story', 'terrible', 'good', 'people']\n",
      "topic-38 ['movie', 'bad', 'car', 'good', 'watch', 'film', 'worst', 'time', 'story', 'actor', 'make', 'thing', 'terrible', 'comedy', 'made']\n",
      "topic-39 ['book', 'movie', 'worst', 'watch', 'funny', 'bad', 'awful', 'terrible', 'good', 'laugh', 'acting', 'money', 'made', 'thing', 'comedy']\n",
      "topic-40 ['film', 'character', 'performance', 'great', 'production', 'action', 'written', 'director', 'role', 'bad', 'gay', 'script', 'end', 'plot', 'writing']\n",
      "topic-41 ['film', 'movie', 'time', 'character', 'story', 'people', 'good', 'make', 'year', 'great', 'made', 'watch', 'actor', 'acting', 'thing']\n",
      "topic-42 ['film', 'movie', 'story', 'russian', 'japanese', 'german', 'people', 'hitler', 'good', 'time', 'american', 'made', 'character', 'show', 'political']\n",
      "topic-43 ['horror', 'white', 'cop', 'gore', 'animal', 'prison', 'cat', 'police', 'criminal', 'black', 'gangster', 'creature', 'villain', 'monster', 'car']\n",
      "topic-44 ['sidney', 'philip', 'intensity', 'carter', 'arthur', 'davis', 'parker', 'exceptional', 'african', 'kenneth', 'mel', 'white', 'mgm', 'freeman', 'storytelling']\n",
      "topic-45 ['movie', 'film', 'sex', 'scene', 'gay', 'character', 'good', 'time', 'bad', 'make', 'story', 'plot', 'woman', 'sexual', 'acting']\n",
      "topic-46 ['book', 'movie', 'worst', 'awful', 'gift', 'watch', 'bad', 'waste', 'acting', 'money', 'good', 'terrible', 'boring', 'recommend', 'scary']\n",
      "topic-47 ['movie', 'film', 'father', 'son', 'good', 'story', 'life', 'family', 'time', 'mother', 'character', 'child', 'year', 'scene', 'daughter']\n",
      "topic-48 ['war', 'horror', 'army', 'soldier', 'terrorist', 'battle', 'military', 'villain', 'student', 'gore', 'death', 'cop', 'killer', 'political', 'action']\n",
      "topic-49 ['film', 'movie', 'character', 'time', 'love', 'life', 'woman', 'make', 'story', 'scene', 'man', 'year', 'people', 'show', 'thing']\n",
      "./\n",
      "[0.3179, 0.32472, 0.34096, 0.30098, 0.28608, 0.4621, 0.41135, 0.39907, 0.45193, 0.34891, 0.44343, 0.4308, 0.34663, 0.44745, 0.33771, 0.41355, 0.32559, 0.37913, 0.37084, 0.4621, 0.33598, 0.45769, 0.27543, 0.38233, 0.38249, 0.30477, 0.38274, 0.37748, 0.33496, 0.39555, 0.40787, 0.43907, 0.49199, 0.3067, 0.46238, 0.31919, 0.36699, 0.43675, 0.44161, 0.48513, 0.29154, 0.43321, 0.36571, 0.32131, 0.35001, 0.41119, 0.45284, 0.34807, 0.32138, 0.31409]\n",
      "0.37995360000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 371.91it/s]\n",
      "196it [00:00, 541.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'CV_wiki': 0.37995360000000006, 'sim_w2v': 0.19267273767480858, 'diversity': 0.30533333333333335, 'filename': 'results/240510_162219.txt', 'acc': 0.50188, 'macro-F1': 0.5015273006568528, 'Purity': 0.57464, 'NMI': 0.007711063285679446}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.22045 - dist: 0.08469 - cons: -0.21424\n",
      "Epoch-1 / recon: 7.06704 - dist: 0.07374 - cons: -0.23351\n",
      "Epoch-2 / recon: 7.00990 - dist: 0.06742 - cons: -0.24415\n",
      "Epoch-3 / recon: 6.98032 - dist: 0.06420 - cons: -0.24956\n",
      "Epoch-4 / recon: 6.96198 - dist: 0.06216 - cons: -0.25376\n",
      "Epoch-5 / recon: 6.94965 - dist: 0.06077 - cons: -0.25671\n",
      "Epoch-6 / recon: 6.94066 - dist: 0.05976 - cons: -0.25910\n",
      "Epoch-7 / recon: 6.93389 - dist: 0.05901 - cons: -0.26069\n",
      "Epoch-8 / recon: 6.92868 - dist: 0.05841 - cons: -0.26198\n",
      "Epoch-9 / recon: 6.92453 - dist: 0.05797 - cons: -0.26299\n",
      "Epoch-10 / recon: 6.92101 - dist: 0.05753 - cons: -0.26400\n",
      "Epoch-11 / recon: 6.91803 - dist: 0.05712 - cons: -0.26482\n",
      "Epoch-12 / recon: 6.91540 - dist: 0.05674 - cons: -0.26535\n",
      "Epoch-13 / recon: 6.91294 - dist: 0.05619 - cons: -0.26624\n",
      "Epoch-14 / recon: 6.91054 - dist: 0.05546 - cons: -0.26731\n",
      "Epoch-15 / recon: 6.90819 - dist: 0.05472 - cons: -0.26840\n",
      "Epoch-16 / recon: 6.90605 - dist: 0.05402 - cons: -0.26958\n",
      "Epoch-17 / recon: 6.90397 - dist: 0.05331 - cons: -0.27066\n",
      "Epoch-18 / recon: 6.90201 - dist: 0.05263 - cons: -0.27172\n",
      "Epoch-19 / recon: 6.90017 - dist: 0.05196 - cons: -0.27285\n",
      "Epoch-20 / recon: 6.89842 - dist: 0.05125 - cons: -0.27411\n",
      "Epoch-21 / recon: 6.89678 - dist: 0.05056 - cons: -0.27521\n",
      "Epoch-22 / recon: 6.89524 - dist: 0.04989 - cons: -0.27620\n",
      "Epoch-23 / recon: 6.89379 - dist: 0.04926 - cons: -0.27727\n",
      "Epoch-24 / recon: 6.89240 - dist: 0.04866 - cons: -0.27830\n",
      "Epoch-25 / recon: 6.89111 - dist: 0.04811 - cons: -0.27928\n",
      "Epoch-26 / recon: 6.88984 - dist: 0.04757 - cons: -0.28028\n",
      "Epoch-27 / recon: 6.88862 - dist: 0.04702 - cons: -0.28129\n",
      "Epoch-28 / recon: 6.88745 - dist: 0.04650 - cons: -0.28220\n",
      "Epoch-29 / recon: 6.88634 - dist: 0.04600 - cons: -0.28296\n",
      "Epoch-30 / recon: 6.88528 - dist: 0.04553 - cons: -0.28369\n",
      "Epoch-31 / recon: 6.88424 - dist: 0.04509 - cons: -0.28447\n",
      "Epoch-32 / recon: 6.88324 - dist: 0.04466 - cons: -0.28518\n",
      "Epoch-33 / recon: 6.88228 - dist: 0.04426 - cons: -0.28586\n",
      "Epoch-34 / recon: 6.88135 - dist: 0.04386 - cons: -0.28649\n",
      "Epoch-35 / recon: 6.88042 - dist: 0.04349 - cons: -0.28715\n",
      "Epoch-36 / recon: 6.87951 - dist: 0.04314 - cons: -0.28774\n",
      "Epoch-37 / recon: 6.87863 - dist: 0.04279 - cons: -0.28836\n",
      "Epoch-38 / recon: 6.87776 - dist: 0.04246 - cons: -0.28895\n",
      "Epoch-39 / recon: 6.87691 - dist: 0.04216 - cons: -0.28945\n",
      "Epoch-40 / recon: 6.87606 - dist: 0.04184 - cons: -0.29003\n",
      "Epoch-41 / recon: 6.87524 - dist: 0.04153 - cons: -0.29057\n",
      "Epoch-42 / recon: 6.87442 - dist: 0.04122 - cons: -0.29115\n",
      "Epoch-43 / recon: 6.87364 - dist: 0.04092 - cons: -0.29169\n",
      "Epoch-44 / recon: 6.87286 - dist: 0.04064 - cons: -0.29220\n",
      "Epoch-45 / recon: 6.87211 - dist: 0.04038 - cons: -0.29265\n",
      "Epoch-46 / recon: 6.87136 - dist: 0.04011 - cons: -0.29310\n",
      "Epoch-47 / recon: 6.87064 - dist: 0.03986 - cons: -0.29357\n",
      "Epoch-48 / recon: 6.86991 - dist: 0.03962 - cons: -0.29405\n",
      "Epoch-49 / recon: 6.86920 - dist: 0.03939 - cons: -0.29454\n",
      "------- Evaluation results -------\n",
      "topic-0 ['cartoon', 'family', 'cat', 'jerry', 'animal', 'daughter', 'dinosaur', 'mother', 'aunt', 'mouse', 'tom', 'rabbit', 'animated', 'sister', 'dog']\n",
      "topic-1 ['war', 'kid', 'disney', 'political', 'school', 'japanese', 'documentary', 'student', 'film', 'people', 'year', 'german', 'child', 'animation', 'politics']\n",
      "topic-2 ['movie', 'film', 'killer', 'good', 'slasher', 'time', 'character', 'bad', 'scene', 'great', 'people', 'end', 'make', 'plot', 'story']\n",
      "topic-3 ['music', 'musical', 'soundtrack', 'song', 'band', 'kid', 'bollywood', 'movie', 'rock', 'singer', 'sing', 'great', 'funny', 'dancing', 'dance']\n",
      "topic-4 ['film', 'murder', 'movie', 'killer', 'good', 'scene', 'character', 'crime', 'story', 'man', 'time', 'make', 'detective', 'police', 'great']\n",
      "topic-5 ['kid', 'zombie', 'vampire', 'werewolf', 'child', 'show', 'episode', 'war', 'monster', 'animal', 'sci', 'alien', 'scary', 'army', 'soldier']\n",
      "topic-6 ['show', 'episode', 'season', 'series', 'character', 'good', 'funny', 'great', 'watch', 'time', 'love', 'watching', 'back', 'make', 'people']\n",
      "topic-7 ['horror', 'zombie', 'vampire', 'gore', 'werewolf', 'monster', 'scary', 'sci', 'alien', 'creature', 'scientist', 'slasher', 'animal', 'dracula', 'dead']\n",
      "topic-8 ['movie', 'kid', 'child', 'film', 'show', 'time', 'good', 'year', 'great', 'watch', 'story', 'thing', 'make', 'adult', 'family']\n",
      "topic-9 ['crime', 'killer', 'cop', 'criminal', 'prison', 'murder', 'police', 'villain', 'battle', 'terrorist', 'action', 'death', 'gun', 'army', 'enemy']\n",
      "topic-10 ['movie', 'good', 'bad', 'film', 'time', 'funny', 'watch', 'great', 'worst', 'acting', 'story', 'made', 'plot', 'laugh', 'make']\n",
      "topic-11 ['movie', 'film', 'time', 'good', 'great', 'character', 'love', 'song', 'make', 'story', 'music', 'watch', 'scene', 'people', 'girl']\n",
      "topic-12 ['movie', 'film', 'bad', 'good', 'music', 'time', 'acting', 'watch', 'thing', 'scene', 'made', 'worst', 'make', 'story', 'great']\n",
      "topic-13 ['movie', 'bad', 'film', 'good', 'time', 'worst', 'scene', 'plot', 'awful', 'make', 'acting', 'character', 'story', 'made', 'watch']\n",
      "topic-14 ['movie', 'film', 'horror', 'scary', 'good', 'make', 'time', 'bad', 'ghost', 'watch', 'story', 'scared', 'acting', 'scene', 'made']\n",
      "topic-15 ['movie', 'film', 'comedy', 'romantic', 'funny', 'woman', 'good', 'character', 'love', 'time', 'great', 'make', 'story', 'actor', 'watch']\n",
      "topic-16 ['movie', 'film', 'black', 'white', 'good', 'character', 'time', 'people', 'cop', 'story', 'make', 'great', 'bad', 'guy', 'scene']\n",
      "topic-17 ['film', 'movie', 'good', 'character', 'acting', 'story', 'bad', 'time', 'director', 'actor', 'make', 'script', 'great', 'plot', 'performance']\n",
      "topic-18 ['movie', 'film', 'japanese', 'story', 'good', 'russian', 'german', 'time', 'people', 'hitler', 'american', 'made', 'character', 'great', 'life']\n",
      "topic-19 ['gay', 'comedy', 'black', 'funny', 'character', 'sexy', 'white', 'ben', 'joke', 'walken', 'guy', 'gangster', 'funnier', 'woman', 'married']\n",
      "topic-20 ['movie', 'film', 'music', 'band', 'rock', 'good', 'song', 'time', 'great', 'story', 'watch', 'indian', 'make', 'scene', 'fan']\n",
      "topic-21 ['funny', 'movie', 'worst', 'bad', 'joke', 'soundtrack', 'terrible', 'waste', 'awful', 'comedy', 'laugh', 'boring', 'band', 'music', 'wasted']\n",
      "topic-22 ['film', 'movie', 'french', 'vampire', 'people', 'werewolf', 'christian', 'character', 'war', 'political', 'life', 'time', 'story', 'zombie', 'great']\n",
      "topic-23 ['porn', 'funny', 'laugh', 'sentence', 'movie', 'joke', 'comedy', 'worst', 'lift', 'sneak', 'gift', 'advise', 'stupid', 'waste', 'laughed']\n",
      "topic-24 ['musical', 'bollywood', 'song', 'broadway', 'singing', 'soundtrack', 'singer', 'music', 'india', 'indian', 'dance', 'daughter', 'romantic', 'mgm', 'employed']\n",
      "topic-25 ['movie', 'game', 'film', 'martial', 'good', 'fight', 'kung', 'scene', 'action', 'great', 'art', 'time', 'match', 'character', 'story']\n",
      "topic-26 ['disney', 'movie', 'film', 'animation', 'story', 'cartoon', 'animated', 'character', 'time', 'good', 'great', 'make', 'king', 'original', 'lion']\n",
      "topic-27 ['war', 'movie', 'film', 'soldier', 'good', 'western', 'military', 'time', 'army', 'story', 'made', 'character', 'world', 'great', 'action']\n",
      "topic-28 ['horror', 'bad', 'gore', 'worst', 'killer', 'awful', 'slasher', 'budget', 'zombie', 'terrible', 'waste', 'asleep', 'crap', 'acting', 'boring']\n",
      "topic-29 ['movie', 'film', 'school', 'bad', 'student', 'good', 'time', 'make', 'high', 'character', 'acting', 'made', 'people', 'kid', 'scene']\n",
      "topic-30 ['film', 'movie', 'character', 'time', 'bad', 'scene', 'good', 'story', 'make', 'thing', 'people', 'man', 'end', 'made', 'life']\n",
      "topic-31 ['movie', 'zombie', 'film', 'bad', 'good', 'sci', 'time', 'make', 'alien', 'people', 'thing', 'acting', 'gore', 'animal', 'story']\n",
      "topic-32 ['film', 'horror', 'movie', 'good', 'story', 'make', 'character', 'house', 'scene', 'time', 'gore', 'director', 'creepy', 'plot', 'bad']\n",
      "topic-33 ['movie', 'film', 'batman', 'time', 'show', 'trek', 'good', 'eddie', 'series', 'episode', 'superman', 'make', 'story', 'car', 'people']\n",
      "topic-34 ['vampire', 'zombie', 'prison', 'criminal', 'werewolf', 'sci', 'cop', 'crime', 'police', 'alien', 'scientist', 'gore', 'dracula', 'murder', 'animal']\n",
      "topic-35 ['sex', 'zombie', 'vampire', 'gore', 'werewolf', 'porn', 'woman', 'killer', 'cop', 'horror', 'murder', 'guy', 'rape', 'prison', 'sci']\n",
      "topic-36 ['movie', 'film', 'father', 'son', 'good', 'story', 'time', 'life', 'character', 'family', 'mother', 'child', 'scene', 'role', 'man']\n",
      "topic-37 ['music', 'soundtrack', 'band', 'song', 'musical', 'bollywood', 'bad', 'movie', 'rock', 'singer', 'worst', 'awful', 'good', 'editing', 'film']\n",
      "topic-38 ['horror', 'kid', 'scary', 'child', 'father', 'zombie', 'mother', 'gore', 'son', 'vampire', 'monster', 'family', 'slasher', 'creepy', 'episode']\n",
      "topic-39 ['film', 'movie', 'character', 'time', 'story', 'life', 'make', 'love', 'scene', 'man', 'woman', 'good', 'show', 'year', 'people']\n",
      "topic-40 ['movie', 'family', 'film', 'brother', 'life', 'sister', 'story', 'character', 'good', 'time', 'great', 'love', 'people', 'make', 'made']\n",
      "topic-41 ['film', 'movie', 'story', 'character', 'french', 'time', 'italian', 'great', 'good', 'people', 'love', 'actor', 'life', 'make', 'acting']\n",
      "topic-42 ['book', 'movie', 'film', 'christmas', 'read', 'story', 'santa', 'good', 'character', 'time', 'version', 'made', 'make', 'bad', 'great']\n",
      "topic-43 ['film', 'bad', 'movie', 'worst', 'horror', 'impression', 'gift', 'deliberately', 'actor', 'worse', 'watch', 'acting', 'awful', 'category', 'good']\n",
      "topic-44 ['film', 'musical', 'movie', 'dance', 'song', 'number', 'dancing', 'great', 'story', 'kelly', 'time', 'music', 'show', 'sinatra', 'character']\n",
      "topic-45 ['movie', 'film', 'kid', 'child', 'funny', 'family', 'time', 'love', 'make', 'year', 'watch', 'good', 'parent', 'great', 'humor']\n",
      "topic-46 ['gentleman', 'emma', 'sidney', 'african', 'intensity', 'stewart', 'flynn', 'adaptation', 'carter', 'kenneth', 'jane', 'roy', 'catholic', 'philip', 'prejudice']\n",
      "topic-47 ['movie', 'vampire', 'film', 'bad', 'scene', 'werewolf', 'good', 'time', 'make', 'horror', 'monster', 'plot', 'character', 'story', 'hunter']\n",
      "topic-48 ['movie', 'film', 'wife', 'good', 'story', 'character', 'time', 'great', 'love', 'mother', 'husband', 'daughter', 'life', 'woman', 'scene']\n",
      "topic-49 ['movie', 'film', 'sex', 'gay', 'scene', 'good', 'time', 'bad', 'story', 'character', 'make', 'people', 'christian', 'acting', 'jesus']\n",
      "./\n",
      "[0.39695, 0.2925, 0.31903, 0.47524, 0.34656, 0.35917, 0.29478, 0.46232, 0.29484, 0.36652, 0.4621, 0.32767, 0.47067, 0.48554, 0.44343, 0.34574, 0.30746, 0.41771, 0.35363, 0.30944, 0.33406, 0.4509, 0.31335, 0.38733, 0.40992, 0.31572, 0.40573, 0.36653, 0.50711, 0.43093, 0.36558, 0.38162, 0.33371, 0.33379, 0.38754, 0.4024, 0.33708, 0.46937, 0.37605, 0.31301, 0.36636, 0.4004, 0.36663, 0.45634, 0.4111, 0.3064, 0.29353, 0.36056, 0.33575, 0.39058]\n",
      "0.37681360000000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 242.27it/s]\n",
      "196it [00:00, 422.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'CV_wiki': 0.37681360000000014, 'sim_w2v': 0.19536991442876636, 'diversity': 0.28, 'filename': 'results/240510_162735.txt', 'acc': 0.49824, 'macro-F1': 0.495168605797673, 'Purity': 0.585, 'NMI': 0.009253889604798282}\n",
      "\n",
      "Coeff   / regul: 1.00000 - recon: 1.00000 - c: 1.00000 - dist: 1.00000 \n",
      "Epoch-0 / recon: 7.21887 - dist: 0.07692 - cons: -0.21753\n",
      "Epoch-1 / recon: 7.06095 - dist: 0.06208 - cons: -0.24599\n",
      "Epoch-2 / recon: 7.00279 - dist: 0.05630 - cons: -0.25859\n",
      "Epoch-3 / recon: 6.97290 - dist: 0.05343 - cons: -0.26538\n",
      "Epoch-4 / recon: 6.95448 - dist: 0.05161 - cons: -0.27028\n",
      "Epoch-5 / recon: 6.94223 - dist: 0.05037 - cons: -0.27350\n",
      "Epoch-6 / recon: 6.93332 - dist: 0.04944 - cons: -0.27614\n",
      "Epoch-7 / recon: 6.92645 - dist: 0.04847 - cons: -0.27827\n",
      "Epoch-8 / recon: 6.92099 - dist: 0.04759 - cons: -0.28023\n",
      "Epoch-9 / recon: 6.91653 - dist: 0.04692 - cons: -0.28170\n",
      "Epoch-10 / recon: 6.91279 - dist: 0.04628 - cons: -0.28302\n",
      "Epoch-11 / recon: 6.90952 - dist: 0.04569 - cons: -0.28425\n",
      "Epoch-12 / recon: 6.90666 - dist: 0.04505 - cons: -0.28545\n",
      "Epoch-13 / recon: 6.90412 - dist: 0.04446 - cons: -0.28670\n",
      "Epoch-14 / recon: 6.90181 - dist: 0.04393 - cons: -0.28771\n",
      "Epoch-15 / recon: 6.89964 - dist: 0.04341 - cons: -0.28857\n",
      "Epoch-16 / recon: 6.89767 - dist: 0.04293 - cons: -0.28941\n",
      "Epoch-17 / recon: 6.89589 - dist: 0.04248 - cons: -0.29015\n",
      "Epoch-18 / recon: 6.89422 - dist: 0.04207 - cons: -0.29085\n",
      "Epoch-19 / recon: 6.89263 - dist: 0.04164 - cons: -0.29153\n",
      "Epoch-20 / recon: 6.89110 - dist: 0.04121 - cons: -0.29223\n",
      "Epoch-21 / recon: 6.88967 - dist: 0.04081 - cons: -0.29280\n",
      "Epoch-22 / recon: 6.88834 - dist: 0.04046 - cons: -0.29330\n",
      "Epoch-23 / recon: 6.88706 - dist: 0.04009 - cons: -0.29381\n",
      "Epoch-24 / recon: 6.88586 - dist: 0.03973 - cons: -0.29434\n",
      "Epoch-25 / recon: 6.88472 - dist: 0.03941 - cons: -0.29493\n",
      "Epoch-26 / recon: 6.88365 - dist: 0.03909 - cons: -0.29538\n",
      "Epoch-27 / recon: 6.88261 - dist: 0.03878 - cons: -0.29577\n",
      "Epoch-28 / recon: 6.88160 - dist: 0.03846 - cons: -0.29627\n",
      "Epoch-29 / recon: 6.88062 - dist: 0.03816 - cons: -0.29669\n",
      "Epoch-30 / recon: 6.87968 - dist: 0.03787 - cons: -0.29707\n",
      "Epoch-31 / recon: 6.87875 - dist: 0.03760 - cons: -0.29745\n",
      "Epoch-32 / recon: 6.87785 - dist: 0.03733 - cons: -0.29780\n",
      "Epoch-33 / recon: 6.87697 - dist: 0.03707 - cons: -0.29808\n",
      "Epoch-34 / recon: 6.87607 - dist: 0.03681 - cons: -0.29837\n",
      "Epoch-35 / recon: 6.87519 - dist: 0.03654 - cons: -0.29872\n",
      "Epoch-36 / recon: 6.87435 - dist: 0.03629 - cons: -0.29895\n",
      "Epoch-37 / recon: 6.87352 - dist: 0.03605 - cons: -0.29916\n",
      "Epoch-38 / recon: 6.87271 - dist: 0.03582 - cons: -0.29937\n",
      "Epoch-39 / recon: 6.87188 - dist: 0.03557 - cons: -0.29963\n",
      "Epoch-40 / recon: 6.87109 - dist: 0.03533 - cons: -0.29986\n",
      "Epoch-41 / recon: 6.87030 - dist: 0.03511 - cons: -0.30008\n",
      "Epoch-42 / recon: 6.86957 - dist: 0.03489 - cons: -0.30033\n",
      "Epoch-43 / recon: 6.86885 - dist: 0.03468 - cons: -0.30052\n",
      "Epoch-44 / recon: 6.86813 - dist: 0.03448 - cons: -0.30068\n",
      "Epoch-45 / recon: 6.86742 - dist: 0.03429 - cons: -0.30090\n",
      "Epoch-46 / recon: 6.86674 - dist: 0.03411 - cons: -0.30111\n",
      "Epoch-47 / recon: 6.86608 - dist: 0.03393 - cons: -0.30129\n",
      "Epoch-48 / recon: 6.86540 - dist: 0.03376 - cons: -0.30148\n",
      "Epoch-49 / recon: 6.86472 - dist: 0.03358 - cons: -0.30167\n",
      "------- Evaluation results -------\n",
      "topic-0 ['movie', 'good', 'time', 'film', 'bad', 'funny', 'watch', 'acting', 'worst', 'story', 'great', 'watching', 'plot', 'laugh', 'made']\n",
      "topic-1 ['stewart', 'western', 'daughter', 'father', 'trek', 'son', 'mary', 'james', 'barbara', 'dad', 'wayne', 'mother', 'emma', 'roy', 'hudson']\n",
      "topic-2 ['movie', 'film', 'batman', 'black', 'eddie', 'white', 'good', 'time', 'people', 'superman', 'make', 'car', 'character', 'murphy', 'show']\n",
      "topic-3 ['episode', 'season', 'show', 'series', 'abc', 'trek', 'sarah', 'program', 'pilot', 'aired', 'zone', 'twilight', 'television', 'hooked', 'clark']\n",
      "topic-4 ['war', 'film', 'movie', 'soldier', 'time', 'good', 'western', 'story', 'army', 'military', 'world', 'year', 'great', 'action', 'american']\n",
      "topic-5 ['film', 'horror', 'movie', 'good', 'story', 'make', 'character', 'time', 'great', 'bad', 'scene', 'house', 'plot', 'director', 'creepy']\n",
      "topic-6 ['season', 'episode', 'horror', 'haunted', 'daughter', 'family', 'creepy', 'ghost', 'castle', 'mother', 'mansion', 'aunt', 'father', 'stephen', 'son']\n",
      "topic-7 ['movie', 'film', 'sex', 'gay', 'scene', 'character', 'good', 'bad', 'story', 'time', 'make', 'porn', 'plot', 'woman', 'acting']\n",
      "topic-8 ['movie', 'film', 'father', 'son', 'story', 'good', 'time', 'life', 'character', 'child', 'family', 'mother', 'year', 'scene', 'love']\n",
      "topic-9 ['movie', 'funny', 'film', 'comedy', 'character', 'good', 'bad', 'laugh', 'joke', 'humor', 'scene', 'make', 'time', 'story', 'guy']\n",
      "topic-10 ['musical', 'song', 'daughter', 'music', 'father', 'mother', 'romantic', 'bollywood', 'broadway', 'soundtrack', 'romance', 'married', 'family', 'woman', 'marriage']\n",
      "topic-11 ['movie', 'action', 'gun', 'villain', 'cop', 'violence', 'film', 'prison', 'gangster', 'police', 'good', 'character', 'sean', 'criminal', 'crime']\n",
      "topic-12 ['movie', 'film', 'school', 'bad', 'student', 'good', 'make', 'time', 'high', 'character', 'acting', 'people', 'scene', 'actor', 'story']\n",
      "topic-13 ['movie', 'game', 'film', 'good', 'fight', 'kung', 'great', 'action', 'martial', 'scene', 'story', 'time', 'character', 'sport', 'fan']\n",
      "topic-14 ['movie', 'film', 'bad', 'good', 'music', 'time', 'scene', 'story', 'watch', 'acting', 'great', 'make', 'actor', 'thing', 'made']\n",
      "topic-15 ['movie', 'film', 'time', 'great', 'good', 'make', 'watch', 'character', 'story', 'people', 'love', 'song', 'comedy', 'scene', 'made']\n",
      "topic-16 ['worst', 'bad', 'zombie', 'awful', 'horror', 'terrible', 'crap', 'vampire', 'gore', 'sci', 'waste', 'slasher', 'asleep', 'insane', 'budget']\n",
      "topic-17 ['movie', 'worst', 'waste', 'bad', 'car', 'awful', 'money', 'funny', 'film', 'terrible', 'acting', 'worse', 'watch', 'laugh', 'crap']\n",
      "topic-18 ['film', 'movie', 'wife', 'good', 'mother', 'daughter', 'story', 'love', 'character', 'great', 'husband', 'time', 'woman', 'life', 'make']\n",
      "topic-19 ['movie', 'film', 'good', 'bad', 'character', 'make', 'time', 'story', 'car', 'scene', 'plot', 'thing', 'black', 'watch', 'people']\n",
      "topic-20 ['film', 'musical', 'movie', 'song', 'dance', 'time', 'number', 'great', 'make', 'woody', 'allen', 'show', 'good', 'story', 'dancing']\n",
      "topic-21 ['movie', 'film', 'good', 'character', 'time', 'great', 'cop', 'action', 'story', 'make', 'gangster', 'scene', 'lynch', 'bad', 'role']\n",
      "topic-22 ['movie', 'film', 'vampire', 'bad', 'good', 'scene', 'time', 'make', 'werewolf', 'character', 'story', 'horror', 'monster', 'plot', 'hunter']\n",
      "topic-23 ['kid', 'horror', 'vampire', 'slasher', 'school', 'porn', 'gore', 'worst', 'child', 'bad', 'scary', 'sex', 'killer', 'disney', 'student']\n",
      "topic-24 ['movie', 'bad', 'worst', 'film', 'acting', 'awful', 'funny', 'terrible', 'waste', 'actor', 'boring', 'character', 'vampire', 'watch', 'horrible']\n",
      "topic-25 ['movie', 'film', 'comedy', 'romantic', 'good', 'woman', 'character', 'love', 'time', 'story', 'great', 'funny', 'watch', 'make', 'actor']\n",
      "topic-26 ['film', 'movie', 'good', 'acting', 'character', 'director', 'actor', 'story', 'time', 'bad', 'make', 'great', 'script', 'performance', 'plot']\n",
      "topic-27 ['italy', 'italian', 'french', 'catholic', 'spain', 'france', 'paris', 'christ', 'emma', 'african', 'sidney', 'spanish', 'africa', 'adaptation', 'intensity']\n",
      "topic-28 ['movie', 'film', 'music', 'song', 'band', 'good', 'time', 'rock', 'great', 'musical', 'dance', 'story', 'scene', 'make', 'fan']\n",
      "topic-29 ['book', 'movie', 'film', 'christmas', 'read', 'good', 'story', 'santa', 'character', 'time', 'made', 'make', 'great', 'bad', 'version']\n",
      "topic-30 ['show', 'episode', 'season', 'series', 'character', 'good', 'time', 'film', 'movie', 'story', 'great', 'love', 'funny', 'watch', 'make']\n",
      "topic-31 ['war', 'political', 'french', 'show', 'battle', 'character', 'film', 'france', 'criminal', 'crime', 'action', 'japanese', 'kane', 'father', 'american']\n",
      "topic-32 ['zombie', 'horror', 'porn', 'gore', 'scary', 'porno', 'sex', 'scientist', 'sci', 'alien', 'monster', 'scare', 'nudity', 'vampire', 'slasher']\n",
      "topic-33 ['movie', 'film', 'scary', 'horror', 'make', 'story', 'good', 'time', 'people', 'ghost', 'christian', 'jesus', 'scared', 'scene', 'made']\n",
      "topic-34 ['movie', 'film', 'japanese', 'story', 'german', 'good', 'russian', 'american', 'time', 'people', 'made', 'character', 'hitler', 'make', 'chinese']\n",
      "topic-35 ['film', 'movie', 'character', 'time', 'story', 'love', 'life', 'make', 'woman', 'scene', 'good', 'show', 'people', 'thing', 'work']\n",
      "topic-36 ['movie', 'kid', 'child', 'film', 'show', 'time', 'year', 'good', 'great', 'story', 'watch', 'adult', 'make', 'parent', 'thing']\n",
      "topic-37 ['movie', 'disney', 'film', 'animation', 'story', 'character', 'cartoon', 'good', 'time', 'animated', 'great', 'make', 'king', 'lion', 'original']\n",
      "topic-38 ['song', 'band', 'music', 'musical', 'rock', 'soundtrack', 'bollywood', 'singer', 'dancing', 'punk', 'indian', 'musician', 'sing', 'concert', 'dance']\n",
      "topic-39 ['family', 'movie', 'show', 'film', 'brother', 'life', 'sister', 'story', 'character', 'time', 'good', 'love', 'series', 'people', 'watch']\n",
      "topic-40 ['film', 'movie', 'character', 'time', 'bad', 'good', 'scene', 'make', 'thing', 'story', 'people', 'man', 'made', 'life', 'end']\n",
      "topic-41 ['movie', 'zombie', 'film', 'time', 'trek', 'good', 'make', 'sci', 'story', 'people', 'bad', 'star', 'alien', 'dinosaur', 'cat']\n",
      "topic-42 ['movie', 'film', 'bad', 'horror', 'good', 'worst', 'acting', 'awful', 'time', 'watch', 'plot', 'make', 'actor', 'made', 'story']\n",
      "topic-43 ['film', 'movie', 'character', 'good', 'time', 'story', 'life', 'scene', 'woman', 'make', 'actor', 'love', 'wife', 'give', 'end']\n",
      "topic-44 ['film', 'murder', 'movie', 'character', 'good', 'killer', 'crime', 'scene', 'story', 'time', 'man', 'detective', 'make', 'great', 'role']\n",
      "topic-45 ['film', 'movie', 'forgotten', 'hilarity', 'imagined', 'countless', 'nowadays', 'bad', 'production', 'sense', 'awful', 'unintentional', 'lit', 'nightmare', 'celluloid']\n",
      "topic-46 ['movie', 'film', 'bad', 'time', 'good', 'scene', 'people', 'character', 'story', 'make', 'acting', 'plot', 'watch', 'end', 'worst']\n",
      "topic-47 ['bad', 'worst', 'vampire', 'awful', 'slasher', 'terrible', 'horror', 'gore', 'zombie', 'budget', 'film', 'acting', 'waste', 'review', 'crap']\n",
      "topic-48 ['kid', 'movie', 'funny', 'laugh', 'school', 'waste', 'disney', 'cry', 'money', 'inane', 'joke', 'decided', 'minimal', 'juvenile', 'watch']\n",
      "topic-49 ['movie', 'film', 'killer', 'good', 'bad', 'time', 'slasher', 'scene', 'character', 'end', 'death', 'plot', 'make', 'people', 'murder']\n",
      "./\n",
      "[0.4494, 0.31626, 0.30993, 0.34281, 0.3067, 0.32947, 0.34947, 0.40818, 0.33788, 0.36983, 0.34441, 0.34985, 0.40927, 0.31198, 0.44745, 0.38965, 0.49849, 0.47209, 0.32851, 0.31138, 0.39143, 0.31387, 0.36056, 0.40035, 0.49451, 0.34574, 0.41771, 0.33696, 0.36219, 0.36663, 0.33953, 0.29375, 0.49022, 0.39313, 0.36428, 0.31322, 0.29695, 0.40573, 0.52626, 0.31594, 0.36558, 0.31655, 0.48114, 0.32134, 0.33894, 0.45457, 0.43734, 0.4725, 0.41819, 0.32342]\n",
      "0.3768307999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 364.31it/s]\n",
      "196it [00:00, 558.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_N': 50, 'CV_wiki': 0.3768307999999999, 'sim_w2v': 0.19476375867643428, 'diversity': 0.30666666666666664, 'filename': 'results/240510_163303.txt', 'acc': 0.482, 'macro-F1': 0.4818057758301165, 'Purity': 0.58532, 'NMI': 0.00903887143750074}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_classification, evaluate_clustering\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i in range(args.stage_2_repeat):\n",
    "    model = ContBertTopicExtractorAE(N_topic=n_topic, N_word=args.n_word, bert=bert_name, bert_dim=768)\n",
    "    model.load_state_dict(torch.load(model_stage1_name), strict=True)\n",
    "    model.beta = nn.Parameter(torch.Tensor(model.N_topic, n_word))\n",
    "    nn.init.xavier_uniform_(model.beta)\n",
    "    model.beta_batchnorm = nn.Sequential()\n",
    "    model.cuda(gpu_ids[0])\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    dlosses = AverageMeter() \n",
    "    rlosses = AverageMeter()\n",
    "    closses = AverageMeter()\n",
    "    distlosses = AverageMeter()\n",
    "    trainloader = DataLoader(finetuneds, batch_size=bsz, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(testds2, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "    memoryloader = DataLoader(finetuneds, batch_size=bsz * 2, shuffle=False, num_workers=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.stage_2_lr)\n",
    "\n",
    "    memory_queue = F.softmax(torch.randn(512, n_topic).cuda(gpu_ids[0]), dim=1)\n",
    "    print(\"Coeff   / regul: {:.5f} - recon: {:.5f} - c: {:.5f} - dist: {:.5f} \".format(args.coeff_2_regul, \n",
    "                                                                                        args.coeff_2_recon,\n",
    "                                                                                        args.coeff_2_cons,\n",
    "                                                                                        args.coeff_2_dist))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        model.encoder.eval()\n",
    "        for batch_idx, batch in enumerate(trainloader):\n",
    "            org_input, pos_input, org_bow, pos_bow = batch\n",
    "            org_input = org_input.cuda(gpu_ids[0])\n",
    "            org_bow = org_bow.cuda(gpu_ids[0])\n",
    "            pos_input = pos_input.cuda(gpu_ids[0])\n",
    "            pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "            batch_size = org_input_ids.size(0)\n",
    "\n",
    "            org_dists, org_topic_logit = model.decode(org_input)\n",
    "            pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "            org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "            pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "\n",
    "            # reconstruction loss\n",
    "            # batchmean\n",
    "#             org_target = torch.matmul(org_topic.detach(), weight_cands)\n",
    "#             pos_target = torch.matmul(pos_topic.detach(), weight_cands)\n",
    "            \n",
    "#             _, org_target = torch.max(org_topic.detach(), 1)\n",
    "#             _, pos_target = torch.max(pos_topic.detach(), 1)\n",
    "            \n",
    "            recons_loss = torch.mean(-torch.sum(torch.log(org_dists + 1E-10) * (org_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-org_dists) + 1E-10) * ((1-org_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log(pos_dists + 1E-10) * (pos_bow * weight_cands), axis=1), axis=0)\n",
    "            recons_loss += torch.mean(-torch.sum(torch.log((1-pos_dists) + 1E-10) * ((1-pos_bow) * weight_cands), axis=1), axis=0)\n",
    "            recons_loss *= 0.5\n",
    "\n",
    "            # consistency loss\n",
    "            pos_sim = torch.sum(org_topic * pos_topic, dim=-1)\n",
    "            cons_loss = -pos_sim.mean()\n",
    "\n",
    "            # distribution loss\n",
    "            # batchmean\n",
    "            distmatch_loss = dist_match_loss(torch.cat((org_topic, pos_topic), dim=0), dirichlet_alpha_2)\n",
    "            \n",
    "\n",
    "            loss = args.coeff_2_recon * recons_loss + \\\n",
    "                   args.coeff_2_cons * cons_loss + \\\n",
    "                   args.coeff_2_dist * distmatch_loss \n",
    "            \n",
    "            losses.update(loss.item(), bsz)\n",
    "            closses.update(cons_loss.item(), bsz)\n",
    "            rlosses.update(recons_loss.item(), bsz)\n",
    "            distlosses.update(distmatch_loss.item(), bsz)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch-{} / recon: {:.5f} - dist: {:.5f} - cons: {:.5f}\".format(epoch, rlosses.avg, distlosses.avg, closses.avg))\n",
    "\n",
    "    print(\"------- Evaluation results -------\")\n",
    "    all_list = {}\n",
    "    for e, i in enumerate(model.beta.cpu().topk(15, dim=1).indices):\n",
    "        word_list = []\n",
    "        for j in i:\n",
    "            word_list.append(vocab_dict_reverse[j.item()])\n",
    "        all_list[e] = word_list\n",
    "        print(\"topic-{}\".format(e), word_list)\n",
    "\n",
    "    topic_words_list = list(all_list.values())\n",
    "    now = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "    results = get_topic_qualities(topic_words_list, palmetto_dir=args.palmetto_dir,\n",
    "                                  reference_corpus=[doc.split() for doc in trainds.preprocess_ctm(trainds.nonempty_text)],\n",
    "                                  filename=f'results/{now}.txt')\n",
    "    train_theta = []\n",
    "    test_theta = []\n",
    "    for batch_idx, batch in tqdm(enumerate(trainloader)):\n",
    "        org_input, _, org_bow, _ = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        train_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    train_theta = np.concatenate(train_theta, axis=0)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(testloader)): \n",
    "        org_input, org_bow = batch\n",
    "        org_input = org_input.cuda(gpu_ids[0])\n",
    "        org_bow = org_bow.cuda(gpu_ids[0])\n",
    "        # pos_input = pos_input.cuda(gpu_ids[0])\n",
    "        # pos_bow = pos_bow.cuda(gpu_ids[0])\n",
    "\n",
    "        batch_size = org_input_ids.size(0)\n",
    "\n",
    "        org_dists, org_topic_logit = model.decode(org_input)\n",
    "        # pos_dists, pos_topic_logit = model.decode(pos_input)\n",
    "\n",
    "        org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "        # pos_topic = F.softmax(pos_topic_logit, dim=1)\n",
    "        \n",
    "        test_theta.append(org_topic.detach().cpu())\n",
    "    \n",
    "    test_theta = np.concatenate(test_theta, axis=0)\n",
    "    \n",
    "    classification_res = evaluate_classification(train_theta, test_theta, textData.targets, textData.test_targets)\n",
    "    clustering_res = evaluate_clustering(test_theta, textData.test_targets)\n",
    "    \n",
    "    results.update(classification_res)\n",
    "    results.update(clustering_res)\n",
    "    \n",
    "    \n",
    "    if should_measure_hungarian:\n",
    "        topic_dist = torch.empty((0, n_topic))\n",
    "        model.eval()\n",
    "        evalloader = DataLoader(finetuneds, batch_size=bsz, shuffle=False, num_workers=0)\n",
    "        non_empty_text_index = [i for i, text in enumerate(textData.data) if len(text) != 0]\n",
    "        assert len(finetuneds) == len(non_empty_text_index)\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(evalloader):\n",
    "                org_input, _, org_bow, __ = batch\n",
    "                org_input = org_input.cuda(gpu_ids[0])\n",
    "                org_dists, org_topic_logit = model.decode(org_input)\n",
    "                org_topic = F.softmax(org_topic_logit, dim=1)\n",
    "                topic_dist = torch.cat((topic_dist, org_topic.detach().cpu()), 0)\n",
    "        label_accuracy = measure_hungarian_score(\n",
    "                             topic_dist,\n",
    "                             [target for i, target in enumerate(textData.targets)\n",
    "                              if i in non_empty_text_index]\n",
    "                         )\n",
    "        results['label_match'] = label_accuracy\n",
    "\n",
    "    print(results)\n",
    "    print()\n",
    "    results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic_N   CV_wiki   sim_w2v  diversity                   filename      acc  \\\n",
      "0       50  0.374004  0.190079   0.314667  results/240510_161129.txt  0.50280   \n",
      "1       50  0.383832  0.197498   0.266667  results/240510_161651.txt  0.49592   \n",
      "2       50  0.379954  0.192673   0.305333  results/240510_162219.txt  0.50188   \n",
      "3       50  0.376814  0.195370   0.280000  results/240510_162735.txt  0.49824   \n",
      "4       50  0.376831  0.194764   0.306667  results/240510_163303.txt  0.48200   \n",
      "\n",
      "   macro-F1   Purity       NMI  \n",
      "0  0.500289  0.58096  0.008207  \n",
      "1  0.491215  0.58280  0.008683  \n",
      "2  0.501527  0.57464  0.007711  \n",
      "3  0.495169  0.58500  0.009254  \n",
      "4  0.481806  0.58532  0.009039  \n",
      "mean\n",
      "topic_N      50.000000\n",
      "CV_wiki       0.378287\n",
      "sim_w2v       0.194077\n",
      "diversity     0.294667\n",
      "acc           0.496168\n",
      "macro-F1      0.494001\n",
      "Purity        0.581744\n",
      "NMI           0.008579\n",
      "dtype: float64\n",
      "std\n",
      "topic_N      0.000000\n",
      "CV_wiki      0.003747\n",
      "sim_w2v      0.002820\n",
      "diversity    0.020352\n",
      "acc          0.008392\n",
      "macro-F1     0.007969\n",
      "Purity       0.004347\n",
      "NMI          0.000626\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)\n",
    "print('mean')\n",
    "print(results_df.mean())\n",
    "print('std')\n",
    "print(results_df.std())\n",
    "\n",
    "if args.result_file is not None:\n",
    "    result_filename = f'results/{args.result_file}'\n",
    "else:\n",
    "    result_filename = f'results/{now}.tsv'\n",
    "\n",
    "results_df.to_csv(result_filename, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
